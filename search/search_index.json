{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"callingCardsTools \u00b6 Introduction \u00b6 CallingCardsTools Provides both an API and a number of cmd line tools for processing raw Calling Cards data. This is used in the nf-core/callingcards pipeline, which provides a workflow to process both yeast and mammals Calling Cards data. Documentation \u00b6 Served Documentation provides information on filetypes and the API. For help with the cmd line tools, simply install callingcardstools (see below) and do: callingcardstools --help Each of the cmd line tools also provides a --help message. Installation \u00b6 callingCardsTools is available through bioconda: conda install -c bioconda callingcardstools pypi: pip install callingcardstools or github (this will be the most current version): pip install git+https://github.com/cmatkhan/callingCardsTools.git After installing, you can get help with the cmd line tools by doing: callingcardstools --help Callingcardstools is containerized: \u00b6 A singularity container is hosted on Galaxyhub . If you go to this site, make sure the \u2018c\u2019s have loaded and then search for \u2018callingcardstools\u2019. There is a container for each version which is on bioconda. Make sure you get the correct version. A docker container is hosted on quay (and biocontainers) . Again, make sure you get the correct version. Development Installation \u00b6 install poetry I prefer to set the default location of the virtual environment to the project directory. You can set that as a global configuration for your poetry installation like so: poetry config virtualenvs.in-project true git clone the repo cd into the repo and issue the command poetry install shell into the virtual environment with poetry shell you can pip install -e . to install the package in editable mode. This is useful if you want to test the cmd line interface as you make changes to the source code.","title":"Overview"},{"location":"#callingcardstools","text":"","title":"callingCardsTools"},{"location":"#introduction","text":"CallingCardsTools Provides both an API and a number of cmd line tools for processing raw Calling Cards data. This is used in the nf-core/callingcards pipeline, which provides a workflow to process both yeast and mammals Calling Cards data.","title":"Introduction"},{"location":"#documentation","text":"Served Documentation provides information on filetypes and the API. For help with the cmd line tools, simply install callingcardstools (see below) and do: callingcardstools --help Each of the cmd line tools also provides a --help message.","title":"Documentation"},{"location":"#installation","text":"callingCardsTools is available through bioconda: conda install -c bioconda callingcardstools pypi: pip install callingcardstools or github (this will be the most current version): pip install git+https://github.com/cmatkhan/callingCardsTools.git After installing, you can get help with the cmd line tools by doing: callingcardstools --help","title":"Installation"},{"location":"#callingcardstools-is-containerized","text":"A singularity container is hosted on Galaxyhub . If you go to this site, make sure the \u2018c\u2019s have loaded and then search for \u2018callingcardstools\u2019. There is a container for each version which is on bioconda. Make sure you get the correct version. A docker container is hosted on quay (and biocontainers) . Again, make sure you get the correct version.","title":"Callingcardstools is containerized:"},{"location":"#development-installation","text":"install poetry I prefer to set the default location of the virtual environment to the project directory. You can set that as a global configuration for your poetry installation like so: poetry config virtualenvs.in-project true git clone the repo cd into the repo and issue the command poetry install shell into the virtual environment with poetry shell you can pip install -e . to install the package in editable mode. This is useful if you want to test the cmd line interface as you make changes to the source code.","title":"Development Installation"},{"location":"API/Alignment/AlignmentTagger/","text":"An object to facilitate adding tags to alignments in a bam file AlignmentTagger \u00b6 Bases: BarcodeParser Given an indexed fasta file (genome), id length and insertion length, this object can returned a read tagged with the RG, XS and XZ tags Source code in callingcardstools/Alignment/AlignmentTagger.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 class AlignmentTagger ( BarcodeParser ): \"\"\"Given an indexed fasta file (genome), id length and insertion length, this object can returned a read tagged with the RG, XS and XZ tags\"\"\" _fasta = \"\" _genome = \"\" def __init__ ( self , barcode_details_json : str , fasta_path : str ) -> None : \"\"\"Initializes the AlignmentTagger object with given barcode details and a fasta file. Args: barcode_details_json (str): The path to a JSON file containing barcode details. fasta_path (str): The path to the genome fasta file. A .fai index file created by samtools faidx must exist at the same location. Raises: FileNotFoundError: Raised if the path to the fasta file or its index file doesn't exist. \"\"\" super () . __init__ ( barcode_details_json ) self . fasta = fasta_path # open the fasta file as a pysam.FastaFile obj self . open () def __del__ ( self ): \"\"\"ensure that the genome file is closed when deleted\"\"\" del self . genome @property def fasta ( self ) -> str : \"\"\"path to the fasta file. The index .fai file MUST BE in the same directory\"\"\" return self . _fasta @fasta . setter def fasta ( self , new_fasta : str ) -> None : if not os . path . exists ( new_fasta ): raise FileNotFoundError ( f ' { new_fasta } does not exist -- check path' ) if not os . path . exists ( new_fasta + '.fai' ): raise FileNotFoundError ( f \"Genome index not found for { new_fasta } . \" f \"The index .fai file must exist in same \" f \"path. Use samtools faidx to create \" f \"an index if one DNE\" ) self . _fasta = new_fasta @property def genome ( self ): \"\"\"pysam FastaFile object\"\"\" return self . _genome @genome . setter def genome ( self , new_genome_obj : pysam . FastaFile ): # pylint:disable=E1101 self . _genome = new_genome_obj @genome . deleter def genome ( self ): try : self . _genome . close () except AttributeError : pass def open ( self ): \"\"\"open the genome file and set the self.genome attribute\"\"\" self . genome = pysam . FastaFile ( self . fasta , self . fasta + '.fai' ) # pylint:disable=E1101 # noqa def is_open ( self ): \"\"\"check if genome file is open\"\"\" return self . genome . is_open () def close ( self ): \"\"\"close the genome file\"\"\" del self . genome def extract_tag_dict ( self , id : str ) -> dict : \"\"\"given an id string created by ReadParser, parse into a dictionary of tags Args: id (str): id line from a given read in a bam produced from a fastq processed by (a script that uses) the ReadParser Raises: IndexError: Raised if parsing of the id doesn't work as expected Returns: dict: For example, the id line MN00200:647:000H533KW:1:11102:20080:1075_RT-AATTCACTACGTCAACA;RS-TaqAI;TF-ERT1 would be returned as {'RT': 'AATTCACTACGTCAACA', 'RS': 'TaqAI', 'TF': 'ERT1'} \"\"\" try : tag_str = id . split ( '_' )[ 1 ] except IndexError as exc : raise IndexError ( 'No read ID present -- ' 'expecting a string appended to the read ' 'ID with a _ in the bam' ) from exc try : tag_dict = { x . split ( '-' )[ 0 ]: x . split ( '-' )[ 1 ] for x in tag_str . split ( ';' )} except IndexError as exc : raise IndexError ( f ' { tag_str } not formed as expected -- ' f 'should have format similar to ' f 'RT-AATTCACTACGTCAACA;RS-TaqAI;TF-ERT1 where ' f 'different tags are delimited by ; and ' f 'tag-value pairs are delimited by - ' ) \\ from exc return tag_dict def tag_read ( self , read , decompose_barcode : bool = True ) -> dict : \"\"\"given a AlignedSegment object, add RG, XS and XZ tags Args: read (AlignedSegment): An aligned segment object -- eg returned in a for loop by interating over bam.fetch() object from pysam decompose_barcode (bool): if the barcode is appended as a read identifer on the bam id line, rather than an already decomposed tag string, then extract the barcode and evaluate it against expectations in the barcode_details json. Default to True. Raises: IndexError: Raised if no read ID is present. TypeError: Raised with the cigarstring is not parse-able in a given read ValueError: Raised when the insertion sequence indicies are out of bounds Returns: dict: A dictionary with key:value pairs {'read': tagged_read, 'barcode_details': dictionary of barcode detals} \"\"\" logger . debug ( read . query_name ) # instantiate some dict objects tag_dict = dict () barcode_dict = dict () # decompose_barcode determines whether the function expects to see # an undecomposed barcode string in the id location of the read.id # or an already parsed barcode string if decompose_barcode : try : tag_str = read . query_name . split ( '_' )[ 1 ] logger . debug ( tag_str ) except IndexError as exc : raise IndexError ( 'No read ID present -- ' 'expecting a string appended to the read ' 'ID with a _ in the bam' ) from exc barcode_dict = self . decompose_barcode ( tag_str ) for k , v in barcode_dict [ 'details' ] . items (): bam_tag = v . get ( 'bam_tag' , None ) if bam_tag : tag_dict [ bam_tag ] = v . get ( 'query' ) + '/' + str ( v . get ( 'dist' )) else : # add tags from the id line for tag , value in self . extract_tag_dict ( read . query_name ) . items (): tag_dict [ tag ] = value # (using the bitwise operator) check if the read is unmapped, # if so, set the region_dict start and end to *, indicating that # there is no alignment, and so there is no start and end region for # the alignment if read . flag & 0x4 : tag_dict [ 'XS' ] = \"*\" tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" # if the bit flag 0x10 is set, the read reverse strand. # Handle accordingly elif read . flag & 0x10 : # A cigartuple looks like [(0,4), (2,2), (1,6),..,(4,68)] if read # is reverse complement. If it is forward, it would have the # (4,68), in this case, in the first position. # The first entry in the tuple is the cigar operation and the # second is the length. Note that pysam does order the tuples in # the reverse order from the sam cigar specs, so cigar 30M would be # (0,30). 4 is cigar S or BAM_CSOFT_CLIP. The list operation below # extracts the length of cigar operation 4 and returns a integer. # if 4 DNE, then soft_clip_length is 0. try : soft_clip_length = read . cigartuples [ - 1 ][ 1 ] \\ if read . cigartuples [ - 1 ][ 0 ] == 4 \\ else 0 except TypeError as exc : raise TypeError ( f \"Read { read . query_name } , \" f \"cigar string { read . cigartuples } \" f \"is not parse-able\" ) \\ from exc # The insertion point is at the end of the alignment # note that this is -1 because per the docs # reference_end points to one past the last aligned residue. read_5_prime = ( read . reference_end - 1 ) + soft_clip_length # this is the `insert_length` number bases which precede the # read (after adjusting for soft clipping) try : # if the soft-clip adjustment put the 3 prime end beyond the # end of the chrom, set XS to * # TODO remove removeprefix removesuffix once ref genome # fixed for yeast if ( read_5_prime > self . genome . get_reference_length ( read . reference_name )): tag_dict [ 'XS' ] = \"*\" tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" # if the endpoint of the insertion sequence is off the end of # the chrom, set XZ to * elif ( read_5_prime + 1 + self . insert_length >= self . genome . get_reference_length ( read . reference_name )): tag_dict [ 'XS' ] = read_5_prime tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" else : # This is the first base -- adjusted for soft clipping -- # in the read which cover the genome tag_dict [ 'XS' ] = read_5_prime tag_dict [ 'XI' ] = read_5_prime + 1 tag_dict [ 'XE' ] = read_5_prime + 1 + self . insert_length # TODO remove removeprefix remove suffix once reference # genome is fixed for yeast tag_dict [ 'XZ' ] = self . genome . fetch ( read . reference_name , read_5_prime + 1 , read_5_prime + 1 + self . insert_length ) . upper () # noqa except ValueError as exc : raise ValueError ( f \"Read { read . query_name } , \" f \"insert region { read . reference_name } : { read_5_prime + 1 } -\" # noqa f \" { read_5_prime + 1 + self . insert_length } \" f \"is out of bounds\" ) \\ from exc # else, Read is in the forward orientation. Note that a single end # forward strand read with no other flags will have flag 0 else : # see if clause for lengthy explanation. This examines the first # operation in the cigar string. If it is a soft clip (code 4), # the length of the soft clipping is stored. Else there is 0 soft # clipping try : soft_clip_length = read . cigartuples [ 0 ][ 1 ] \\ if read . cigartuples [ 0 ][ 0 ] == 4 \\ else 0 except TypeError as exc : raise TypeError ( f \"Read { read . query_name } , \" f \"cigar string { read . cigartuples } is not \" f \"parse-able\" ) \\ from exc # extract insert position read_5_prime = read . reference_start - soft_clip_length # this is the `insert_length` number bases which precede the # read (after adjusting for soft clipping) try : # if the 5 prime end, after soft clipping, is less than 0, set # XS to * if ( read_5_prime < 0 ): tag_dict [ 'XS' ] = \"*\" tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" # if the insertion sequence extends beyond the beginning of the # chrom, set to * elif ( read_5_prime - self . insert_length < 0 ): tag_dict [ 'XS' ] = read_5_prime tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" else : # This is the first base -- adjusted for soft clipping -- # in the read which cover the genome tag_dict [ 'XS' ] = read_5_prime tag_dict [ 'XI' ] = read_5_prime - self . insert_length tag_dict [ 'XE' ] = read_5_prime # TODO remove the removeprefix removesuffix -- need to # standardize rob's genome names tag_dict [ 'XZ' ] = self . genome . fetch ( read . reference_name , read_5_prime - self . insert_length , # noqa read_5_prime ) . upper () except ValueError as exc : raise ValueError ( f \"Read { read . query_name } , \" f \"insert region \" f \" { read . reference_name } : { read_5_prime - self . insert_length } -\" # noqa f \" { read_5_prime } is out of bounds\" ) from exc # Set tags ------------------------------------------------------------ for tag , tag_str in tag_dict . items (): read . set_tag ( tag , tag_str ) return { 'read' : read , 'barcode_details' : barcode_dict } fasta : str property writable \u00b6 path to the fasta file. The index .fai file MUST BE in the same directory genome deletable property writable \u00b6 pysam FastaFile object __del__ () \u00b6 ensure that the genome file is closed when deleted Source code in callingcardstools/Alignment/AlignmentTagger.py 42 43 44 def __del__ ( self ): \"\"\"ensure that the genome file is closed when deleted\"\"\" del self . genome __init__ ( barcode_details_json , fasta_path ) \u00b6 Initializes the AlignmentTagger object with given barcode details and a fasta file. Parameters: Name Type Description Default barcode_details_json str The path to a JSON file containing barcode details. required fasta_path str The path to the genome fasta file. A .fai index file created by samtools faidx must exist at the same location. required Raises: Type Description FileNotFoundError Raised if the path to the fasta file or its index file doesn\u2019t exist. Source code in callingcardstools/Alignment/AlignmentTagger.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , barcode_details_json : str , fasta_path : str ) -> None : \"\"\"Initializes the AlignmentTagger object with given barcode details and a fasta file. Args: barcode_details_json (str): The path to a JSON file containing barcode details. fasta_path (str): The path to the genome fasta file. A .fai index file created by samtools faidx must exist at the same location. Raises: FileNotFoundError: Raised if the path to the fasta file or its index file doesn't exist. \"\"\" super () . __init__ ( barcode_details_json ) self . fasta = fasta_path # open the fasta file as a pysam.FastaFile obj self . open () close () \u00b6 close the genome file Source code in callingcardstools/Alignment/AlignmentTagger.py 88 89 90 def close ( self ): \"\"\"close the genome file\"\"\" del self . genome extract_tag_dict ( id ) \u00b6 given an id string created by ReadParser, parse into a dictionary of tags Parameters: Name Type Description Default id str id line from a given read in a bam produced from a fastq processed by (a script that uses) the ReadParser required Raises: Type Description IndexError Raised if parsing of the id doesn\u2019t work as expected Returns: Name Type Description dict dict For example, the id line MN00200:647:000H533KW:1:11102:20080:1075_RT-AATTCACTACGTCAACA;RS-TaqAI;TF-ERT1 would be returned as {\u2018RT\u2019: \u2018AATTCACTACGTCAACA\u2019, \u2018RS\u2019: \u2018TaqAI\u2019, \u2018TF\u2019: \u2018ERT1\u2019} Source code in callingcardstools/Alignment/AlignmentTagger.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def extract_tag_dict ( self , id : str ) -> dict : \"\"\"given an id string created by ReadParser, parse into a dictionary of tags Args: id (str): id line from a given read in a bam produced from a fastq processed by (a script that uses) the ReadParser Raises: IndexError: Raised if parsing of the id doesn't work as expected Returns: dict: For example, the id line MN00200:647:000H533KW:1:11102:20080:1075_RT-AATTCACTACGTCAACA;RS-TaqAI;TF-ERT1 would be returned as {'RT': 'AATTCACTACGTCAACA', 'RS': 'TaqAI', 'TF': 'ERT1'} \"\"\" try : tag_str = id . split ( '_' )[ 1 ] except IndexError as exc : raise IndexError ( 'No read ID present -- ' 'expecting a string appended to the read ' 'ID with a _ in the bam' ) from exc try : tag_dict = { x . split ( '-' )[ 0 ]: x . split ( '-' )[ 1 ] for x in tag_str . split ( ';' )} except IndexError as exc : raise IndexError ( f ' { tag_str } not formed as expected -- ' f 'should have format similar to ' f 'RT-AATTCACTACGTCAACA;RS-TaqAI;TF-ERT1 where ' f 'different tags are delimited by ; and ' f 'tag-value pairs are delimited by - ' ) \\ from exc return tag_dict is_open () \u00b6 check if genome file is open Source code in callingcardstools/Alignment/AlignmentTagger.py 84 85 86 def is_open ( self ): \"\"\"check if genome file is open\"\"\" return self . genome . is_open () open () \u00b6 open the genome file and set the self.genome attribute Source code in callingcardstools/Alignment/AlignmentTagger.py 80 81 82 def open ( self ): \"\"\"open the genome file and set the self.genome attribute\"\"\" self . genome = pysam . FastaFile ( self . fasta , self . fasta + '.fai' ) # pylint:disable=E1101 # noqa tag_read ( read , decompose_barcode = True ) \u00b6 given a AlignedSegment object, add RG, XS and XZ tags Parameters: Name Type Description Default read AlignedSegment An aligned segment object \u2013 eg returned in a for loop by interating over bam.fetch() object from pysam required decompose_barcode bool if the barcode is appended as a read identifer on the bam id line, rather than an already decomposed tag string, then extract the barcode and evaluate it against expectations in the barcode_details json. Default to True. True Raises: Type Description IndexError Raised if no read ID is present. TypeError Raised with the cigarstring is not parse-able in a given read ValueError Raised when the insertion sequence indicies are out of bounds Returns: Name Type Description dict dict A dictionary with key:value pairs {\u2018read\u2019: tagged_read, \u2018barcode_details\u2019: dictionary of barcode detals} Source code in callingcardstools/Alignment/AlignmentTagger.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 def tag_read ( self , read , decompose_barcode : bool = True ) -> dict : \"\"\"given a AlignedSegment object, add RG, XS and XZ tags Args: read (AlignedSegment): An aligned segment object -- eg returned in a for loop by interating over bam.fetch() object from pysam decompose_barcode (bool): if the barcode is appended as a read identifer on the bam id line, rather than an already decomposed tag string, then extract the barcode and evaluate it against expectations in the barcode_details json. Default to True. Raises: IndexError: Raised if no read ID is present. TypeError: Raised with the cigarstring is not parse-able in a given read ValueError: Raised when the insertion sequence indicies are out of bounds Returns: dict: A dictionary with key:value pairs {'read': tagged_read, 'barcode_details': dictionary of barcode detals} \"\"\" logger . debug ( read . query_name ) # instantiate some dict objects tag_dict = dict () barcode_dict = dict () # decompose_barcode determines whether the function expects to see # an undecomposed barcode string in the id location of the read.id # or an already parsed barcode string if decompose_barcode : try : tag_str = read . query_name . split ( '_' )[ 1 ] logger . debug ( tag_str ) except IndexError as exc : raise IndexError ( 'No read ID present -- ' 'expecting a string appended to the read ' 'ID with a _ in the bam' ) from exc barcode_dict = self . decompose_barcode ( tag_str ) for k , v in barcode_dict [ 'details' ] . items (): bam_tag = v . get ( 'bam_tag' , None ) if bam_tag : tag_dict [ bam_tag ] = v . get ( 'query' ) + '/' + str ( v . get ( 'dist' )) else : # add tags from the id line for tag , value in self . extract_tag_dict ( read . query_name ) . items (): tag_dict [ tag ] = value # (using the bitwise operator) check if the read is unmapped, # if so, set the region_dict start and end to *, indicating that # there is no alignment, and so there is no start and end region for # the alignment if read . flag & 0x4 : tag_dict [ 'XS' ] = \"*\" tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" # if the bit flag 0x10 is set, the read reverse strand. # Handle accordingly elif read . flag & 0x10 : # A cigartuple looks like [(0,4), (2,2), (1,6),..,(4,68)] if read # is reverse complement. If it is forward, it would have the # (4,68), in this case, in the first position. # The first entry in the tuple is the cigar operation and the # second is the length. Note that pysam does order the tuples in # the reverse order from the sam cigar specs, so cigar 30M would be # (0,30). 4 is cigar S or BAM_CSOFT_CLIP. The list operation below # extracts the length of cigar operation 4 and returns a integer. # if 4 DNE, then soft_clip_length is 0. try : soft_clip_length = read . cigartuples [ - 1 ][ 1 ] \\ if read . cigartuples [ - 1 ][ 0 ] == 4 \\ else 0 except TypeError as exc : raise TypeError ( f \"Read { read . query_name } , \" f \"cigar string { read . cigartuples } \" f \"is not parse-able\" ) \\ from exc # The insertion point is at the end of the alignment # note that this is -1 because per the docs # reference_end points to one past the last aligned residue. read_5_prime = ( read . reference_end - 1 ) + soft_clip_length # this is the `insert_length` number bases which precede the # read (after adjusting for soft clipping) try : # if the soft-clip adjustment put the 3 prime end beyond the # end of the chrom, set XS to * # TODO remove removeprefix removesuffix once ref genome # fixed for yeast if ( read_5_prime > self . genome . get_reference_length ( read . reference_name )): tag_dict [ 'XS' ] = \"*\" tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" # if the endpoint of the insertion sequence is off the end of # the chrom, set XZ to * elif ( read_5_prime + 1 + self . insert_length >= self . genome . get_reference_length ( read . reference_name )): tag_dict [ 'XS' ] = read_5_prime tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" else : # This is the first base -- adjusted for soft clipping -- # in the read which cover the genome tag_dict [ 'XS' ] = read_5_prime tag_dict [ 'XI' ] = read_5_prime + 1 tag_dict [ 'XE' ] = read_5_prime + 1 + self . insert_length # TODO remove removeprefix remove suffix once reference # genome is fixed for yeast tag_dict [ 'XZ' ] = self . genome . fetch ( read . reference_name , read_5_prime + 1 , read_5_prime + 1 + self . insert_length ) . upper () # noqa except ValueError as exc : raise ValueError ( f \"Read { read . query_name } , \" f \"insert region { read . reference_name } : { read_5_prime + 1 } -\" # noqa f \" { read_5_prime + 1 + self . insert_length } \" f \"is out of bounds\" ) \\ from exc # else, Read is in the forward orientation. Note that a single end # forward strand read with no other flags will have flag 0 else : # see if clause for lengthy explanation. This examines the first # operation in the cigar string. If it is a soft clip (code 4), # the length of the soft clipping is stored. Else there is 0 soft # clipping try : soft_clip_length = read . cigartuples [ 0 ][ 1 ] \\ if read . cigartuples [ 0 ][ 0 ] == 4 \\ else 0 except TypeError as exc : raise TypeError ( f \"Read { read . query_name } , \" f \"cigar string { read . cigartuples } is not \" f \"parse-able\" ) \\ from exc # extract insert position read_5_prime = read . reference_start - soft_clip_length # this is the `insert_length` number bases which precede the # read (after adjusting for soft clipping) try : # if the 5 prime end, after soft clipping, is less than 0, set # XS to * if ( read_5_prime < 0 ): tag_dict [ 'XS' ] = \"*\" tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" # if the insertion sequence extends beyond the beginning of the # chrom, set to * elif ( read_5_prime - self . insert_length < 0 ): tag_dict [ 'XS' ] = read_5_prime tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" else : # This is the first base -- adjusted for soft clipping -- # in the read which cover the genome tag_dict [ 'XS' ] = read_5_prime tag_dict [ 'XI' ] = read_5_prime - self . insert_length tag_dict [ 'XE' ] = read_5_prime # TODO remove the removeprefix removesuffix -- need to # standardize rob's genome names tag_dict [ 'XZ' ] = self . genome . fetch ( read . reference_name , read_5_prime - self . insert_length , # noqa read_5_prime ) . upper () except ValueError as exc : raise ValueError ( f \"Read { read . query_name } , \" f \"insert region \" f \" { read . reference_name } : { read_5_prime - self . insert_length } -\" # noqa f \" { read_5_prime } is out of bounds\" ) from exc # Set tags ------------------------------------------------------------ for tag , tag_str in tag_dict . items (): read . set_tag ( tag , tag_str ) return { 'read' : read , 'barcode_details' : barcode_dict }","title":"AlignmentTagger"},{"location":"API/Alignment/AlignmentTagger/#callingcardstools.Alignment.AlignmentTagger.AlignmentTagger","text":"Bases: BarcodeParser Given an indexed fasta file (genome), id length and insertion length, this object can returned a read tagged with the RG, XS and XZ tags Source code in callingcardstools/Alignment/AlignmentTagger.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 class AlignmentTagger ( BarcodeParser ): \"\"\"Given an indexed fasta file (genome), id length and insertion length, this object can returned a read tagged with the RG, XS and XZ tags\"\"\" _fasta = \"\" _genome = \"\" def __init__ ( self , barcode_details_json : str , fasta_path : str ) -> None : \"\"\"Initializes the AlignmentTagger object with given barcode details and a fasta file. Args: barcode_details_json (str): The path to a JSON file containing barcode details. fasta_path (str): The path to the genome fasta file. A .fai index file created by samtools faidx must exist at the same location. Raises: FileNotFoundError: Raised if the path to the fasta file or its index file doesn't exist. \"\"\" super () . __init__ ( barcode_details_json ) self . fasta = fasta_path # open the fasta file as a pysam.FastaFile obj self . open () def __del__ ( self ): \"\"\"ensure that the genome file is closed when deleted\"\"\" del self . genome @property def fasta ( self ) -> str : \"\"\"path to the fasta file. The index .fai file MUST BE in the same directory\"\"\" return self . _fasta @fasta . setter def fasta ( self , new_fasta : str ) -> None : if not os . path . exists ( new_fasta ): raise FileNotFoundError ( f ' { new_fasta } does not exist -- check path' ) if not os . path . exists ( new_fasta + '.fai' ): raise FileNotFoundError ( f \"Genome index not found for { new_fasta } . \" f \"The index .fai file must exist in same \" f \"path. Use samtools faidx to create \" f \"an index if one DNE\" ) self . _fasta = new_fasta @property def genome ( self ): \"\"\"pysam FastaFile object\"\"\" return self . _genome @genome . setter def genome ( self , new_genome_obj : pysam . FastaFile ): # pylint:disable=E1101 self . _genome = new_genome_obj @genome . deleter def genome ( self ): try : self . _genome . close () except AttributeError : pass def open ( self ): \"\"\"open the genome file and set the self.genome attribute\"\"\" self . genome = pysam . FastaFile ( self . fasta , self . fasta + '.fai' ) # pylint:disable=E1101 # noqa def is_open ( self ): \"\"\"check if genome file is open\"\"\" return self . genome . is_open () def close ( self ): \"\"\"close the genome file\"\"\" del self . genome def extract_tag_dict ( self , id : str ) -> dict : \"\"\"given an id string created by ReadParser, parse into a dictionary of tags Args: id (str): id line from a given read in a bam produced from a fastq processed by (a script that uses) the ReadParser Raises: IndexError: Raised if parsing of the id doesn't work as expected Returns: dict: For example, the id line MN00200:647:000H533KW:1:11102:20080:1075_RT-AATTCACTACGTCAACA;RS-TaqAI;TF-ERT1 would be returned as {'RT': 'AATTCACTACGTCAACA', 'RS': 'TaqAI', 'TF': 'ERT1'} \"\"\" try : tag_str = id . split ( '_' )[ 1 ] except IndexError as exc : raise IndexError ( 'No read ID present -- ' 'expecting a string appended to the read ' 'ID with a _ in the bam' ) from exc try : tag_dict = { x . split ( '-' )[ 0 ]: x . split ( '-' )[ 1 ] for x in tag_str . split ( ';' )} except IndexError as exc : raise IndexError ( f ' { tag_str } not formed as expected -- ' f 'should have format similar to ' f 'RT-AATTCACTACGTCAACA;RS-TaqAI;TF-ERT1 where ' f 'different tags are delimited by ; and ' f 'tag-value pairs are delimited by - ' ) \\ from exc return tag_dict def tag_read ( self , read , decompose_barcode : bool = True ) -> dict : \"\"\"given a AlignedSegment object, add RG, XS and XZ tags Args: read (AlignedSegment): An aligned segment object -- eg returned in a for loop by interating over bam.fetch() object from pysam decompose_barcode (bool): if the barcode is appended as a read identifer on the bam id line, rather than an already decomposed tag string, then extract the barcode and evaluate it against expectations in the barcode_details json. Default to True. Raises: IndexError: Raised if no read ID is present. TypeError: Raised with the cigarstring is not parse-able in a given read ValueError: Raised when the insertion sequence indicies are out of bounds Returns: dict: A dictionary with key:value pairs {'read': tagged_read, 'barcode_details': dictionary of barcode detals} \"\"\" logger . debug ( read . query_name ) # instantiate some dict objects tag_dict = dict () barcode_dict = dict () # decompose_barcode determines whether the function expects to see # an undecomposed barcode string in the id location of the read.id # or an already parsed barcode string if decompose_barcode : try : tag_str = read . query_name . split ( '_' )[ 1 ] logger . debug ( tag_str ) except IndexError as exc : raise IndexError ( 'No read ID present -- ' 'expecting a string appended to the read ' 'ID with a _ in the bam' ) from exc barcode_dict = self . decompose_barcode ( tag_str ) for k , v in barcode_dict [ 'details' ] . items (): bam_tag = v . get ( 'bam_tag' , None ) if bam_tag : tag_dict [ bam_tag ] = v . get ( 'query' ) + '/' + str ( v . get ( 'dist' )) else : # add tags from the id line for tag , value in self . extract_tag_dict ( read . query_name ) . items (): tag_dict [ tag ] = value # (using the bitwise operator) check if the read is unmapped, # if so, set the region_dict start and end to *, indicating that # there is no alignment, and so there is no start and end region for # the alignment if read . flag & 0x4 : tag_dict [ 'XS' ] = \"*\" tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" # if the bit flag 0x10 is set, the read reverse strand. # Handle accordingly elif read . flag & 0x10 : # A cigartuple looks like [(0,4), (2,2), (1,6),..,(4,68)] if read # is reverse complement. If it is forward, it would have the # (4,68), in this case, in the first position. # The first entry in the tuple is the cigar operation and the # second is the length. Note that pysam does order the tuples in # the reverse order from the sam cigar specs, so cigar 30M would be # (0,30). 4 is cigar S or BAM_CSOFT_CLIP. The list operation below # extracts the length of cigar operation 4 and returns a integer. # if 4 DNE, then soft_clip_length is 0. try : soft_clip_length = read . cigartuples [ - 1 ][ 1 ] \\ if read . cigartuples [ - 1 ][ 0 ] == 4 \\ else 0 except TypeError as exc : raise TypeError ( f \"Read { read . query_name } , \" f \"cigar string { read . cigartuples } \" f \"is not parse-able\" ) \\ from exc # The insertion point is at the end of the alignment # note that this is -1 because per the docs # reference_end points to one past the last aligned residue. read_5_prime = ( read . reference_end - 1 ) + soft_clip_length # this is the `insert_length` number bases which precede the # read (after adjusting for soft clipping) try : # if the soft-clip adjustment put the 3 prime end beyond the # end of the chrom, set XS to * # TODO remove removeprefix removesuffix once ref genome # fixed for yeast if ( read_5_prime > self . genome . get_reference_length ( read . reference_name )): tag_dict [ 'XS' ] = \"*\" tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" # if the endpoint of the insertion sequence is off the end of # the chrom, set XZ to * elif ( read_5_prime + 1 + self . insert_length >= self . genome . get_reference_length ( read . reference_name )): tag_dict [ 'XS' ] = read_5_prime tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" else : # This is the first base -- adjusted for soft clipping -- # in the read which cover the genome tag_dict [ 'XS' ] = read_5_prime tag_dict [ 'XI' ] = read_5_prime + 1 tag_dict [ 'XE' ] = read_5_prime + 1 + self . insert_length # TODO remove removeprefix remove suffix once reference # genome is fixed for yeast tag_dict [ 'XZ' ] = self . genome . fetch ( read . reference_name , read_5_prime + 1 , read_5_prime + 1 + self . insert_length ) . upper () # noqa except ValueError as exc : raise ValueError ( f \"Read { read . query_name } , \" f \"insert region { read . reference_name } : { read_5_prime + 1 } -\" # noqa f \" { read_5_prime + 1 + self . insert_length } \" f \"is out of bounds\" ) \\ from exc # else, Read is in the forward orientation. Note that a single end # forward strand read with no other flags will have flag 0 else : # see if clause for lengthy explanation. This examines the first # operation in the cigar string. If it is a soft clip (code 4), # the length of the soft clipping is stored. Else there is 0 soft # clipping try : soft_clip_length = read . cigartuples [ 0 ][ 1 ] \\ if read . cigartuples [ 0 ][ 0 ] == 4 \\ else 0 except TypeError as exc : raise TypeError ( f \"Read { read . query_name } , \" f \"cigar string { read . cigartuples } is not \" f \"parse-able\" ) \\ from exc # extract insert position read_5_prime = read . reference_start - soft_clip_length # this is the `insert_length` number bases which precede the # read (after adjusting for soft clipping) try : # if the 5 prime end, after soft clipping, is less than 0, set # XS to * if ( read_5_prime < 0 ): tag_dict [ 'XS' ] = \"*\" tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" # if the insertion sequence extends beyond the beginning of the # chrom, set to * elif ( read_5_prime - self . insert_length < 0 ): tag_dict [ 'XS' ] = read_5_prime tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" else : # This is the first base -- adjusted for soft clipping -- # in the read which cover the genome tag_dict [ 'XS' ] = read_5_prime tag_dict [ 'XI' ] = read_5_prime - self . insert_length tag_dict [ 'XE' ] = read_5_prime # TODO remove the removeprefix removesuffix -- need to # standardize rob's genome names tag_dict [ 'XZ' ] = self . genome . fetch ( read . reference_name , read_5_prime - self . insert_length , # noqa read_5_prime ) . upper () except ValueError as exc : raise ValueError ( f \"Read { read . query_name } , \" f \"insert region \" f \" { read . reference_name } : { read_5_prime - self . insert_length } -\" # noqa f \" { read_5_prime } is out of bounds\" ) from exc # Set tags ------------------------------------------------------------ for tag , tag_str in tag_dict . items (): read . set_tag ( tag , tag_str ) return { 'read' : read , 'barcode_details' : barcode_dict }","title":"AlignmentTagger"},{"location":"API/Alignment/AlignmentTagger/#callingcardstools.Alignment.AlignmentTagger.AlignmentTagger.fasta","text":"path to the fasta file. The index .fai file MUST BE in the same directory","title":"fasta"},{"location":"API/Alignment/AlignmentTagger/#callingcardstools.Alignment.AlignmentTagger.AlignmentTagger.genome","text":"pysam FastaFile object","title":"genome"},{"location":"API/Alignment/AlignmentTagger/#callingcardstools.Alignment.AlignmentTagger.AlignmentTagger.__del__","text":"ensure that the genome file is closed when deleted Source code in callingcardstools/Alignment/AlignmentTagger.py 42 43 44 def __del__ ( self ): \"\"\"ensure that the genome file is closed when deleted\"\"\" del self . genome","title":"__del__()"},{"location":"API/Alignment/AlignmentTagger/#callingcardstools.Alignment.AlignmentTagger.AlignmentTagger.__init__","text":"Initializes the AlignmentTagger object with given barcode details and a fasta file. Parameters: Name Type Description Default barcode_details_json str The path to a JSON file containing barcode details. required fasta_path str The path to the genome fasta file. A .fai index file created by samtools faidx must exist at the same location. required Raises: Type Description FileNotFoundError Raised if the path to the fasta file or its index file doesn\u2019t exist. Source code in callingcardstools/Alignment/AlignmentTagger.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def __init__ ( self , barcode_details_json : str , fasta_path : str ) -> None : \"\"\"Initializes the AlignmentTagger object with given barcode details and a fasta file. Args: barcode_details_json (str): The path to a JSON file containing barcode details. fasta_path (str): The path to the genome fasta file. A .fai index file created by samtools faidx must exist at the same location. Raises: FileNotFoundError: Raised if the path to the fasta file or its index file doesn't exist. \"\"\" super () . __init__ ( barcode_details_json ) self . fasta = fasta_path # open the fasta file as a pysam.FastaFile obj self . open ()","title":"__init__()"},{"location":"API/Alignment/AlignmentTagger/#callingcardstools.Alignment.AlignmentTagger.AlignmentTagger.close","text":"close the genome file Source code in callingcardstools/Alignment/AlignmentTagger.py 88 89 90 def close ( self ): \"\"\"close the genome file\"\"\" del self . genome","title":"close()"},{"location":"API/Alignment/AlignmentTagger/#callingcardstools.Alignment.AlignmentTagger.AlignmentTagger.extract_tag_dict","text":"given an id string created by ReadParser, parse into a dictionary of tags Parameters: Name Type Description Default id str id line from a given read in a bam produced from a fastq processed by (a script that uses) the ReadParser required Raises: Type Description IndexError Raised if parsing of the id doesn\u2019t work as expected Returns: Name Type Description dict dict For example, the id line MN00200:647:000H533KW:1:11102:20080:1075_RT-AATTCACTACGTCAACA;RS-TaqAI;TF-ERT1 would be returned as {\u2018RT\u2019: \u2018AATTCACTACGTCAACA\u2019, \u2018RS\u2019: \u2018TaqAI\u2019, \u2018TF\u2019: \u2018ERT1\u2019} Source code in callingcardstools/Alignment/AlignmentTagger.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def extract_tag_dict ( self , id : str ) -> dict : \"\"\"given an id string created by ReadParser, parse into a dictionary of tags Args: id (str): id line from a given read in a bam produced from a fastq processed by (a script that uses) the ReadParser Raises: IndexError: Raised if parsing of the id doesn't work as expected Returns: dict: For example, the id line MN00200:647:000H533KW:1:11102:20080:1075_RT-AATTCACTACGTCAACA;RS-TaqAI;TF-ERT1 would be returned as {'RT': 'AATTCACTACGTCAACA', 'RS': 'TaqAI', 'TF': 'ERT1'} \"\"\" try : tag_str = id . split ( '_' )[ 1 ] except IndexError as exc : raise IndexError ( 'No read ID present -- ' 'expecting a string appended to the read ' 'ID with a _ in the bam' ) from exc try : tag_dict = { x . split ( '-' )[ 0 ]: x . split ( '-' )[ 1 ] for x in tag_str . split ( ';' )} except IndexError as exc : raise IndexError ( f ' { tag_str } not formed as expected -- ' f 'should have format similar to ' f 'RT-AATTCACTACGTCAACA;RS-TaqAI;TF-ERT1 where ' f 'different tags are delimited by ; and ' f 'tag-value pairs are delimited by - ' ) \\ from exc return tag_dict","title":"extract_tag_dict()"},{"location":"API/Alignment/AlignmentTagger/#callingcardstools.Alignment.AlignmentTagger.AlignmentTagger.is_open","text":"check if genome file is open Source code in callingcardstools/Alignment/AlignmentTagger.py 84 85 86 def is_open ( self ): \"\"\"check if genome file is open\"\"\" return self . genome . is_open ()","title":"is_open()"},{"location":"API/Alignment/AlignmentTagger/#callingcardstools.Alignment.AlignmentTagger.AlignmentTagger.open","text":"open the genome file and set the self.genome attribute Source code in callingcardstools/Alignment/AlignmentTagger.py 80 81 82 def open ( self ): \"\"\"open the genome file and set the self.genome attribute\"\"\" self . genome = pysam . FastaFile ( self . fasta , self . fasta + '.fai' ) # pylint:disable=E1101 # noqa","title":"open()"},{"location":"API/Alignment/AlignmentTagger/#callingcardstools.Alignment.AlignmentTagger.AlignmentTagger.tag_read","text":"given a AlignedSegment object, add RG, XS and XZ tags Parameters: Name Type Description Default read AlignedSegment An aligned segment object \u2013 eg returned in a for loop by interating over bam.fetch() object from pysam required decompose_barcode bool if the barcode is appended as a read identifer on the bam id line, rather than an already decomposed tag string, then extract the barcode and evaluate it against expectations in the barcode_details json. Default to True. True Raises: Type Description IndexError Raised if no read ID is present. TypeError Raised with the cigarstring is not parse-able in a given read ValueError Raised when the insertion sequence indicies are out of bounds Returns: Name Type Description dict dict A dictionary with key:value pairs {\u2018read\u2019: tagged_read, \u2018barcode_details\u2019: dictionary of barcode detals} Source code in callingcardstools/Alignment/AlignmentTagger.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 def tag_read ( self , read , decompose_barcode : bool = True ) -> dict : \"\"\"given a AlignedSegment object, add RG, XS and XZ tags Args: read (AlignedSegment): An aligned segment object -- eg returned in a for loop by interating over bam.fetch() object from pysam decompose_barcode (bool): if the barcode is appended as a read identifer on the bam id line, rather than an already decomposed tag string, then extract the barcode and evaluate it against expectations in the barcode_details json. Default to True. Raises: IndexError: Raised if no read ID is present. TypeError: Raised with the cigarstring is not parse-able in a given read ValueError: Raised when the insertion sequence indicies are out of bounds Returns: dict: A dictionary with key:value pairs {'read': tagged_read, 'barcode_details': dictionary of barcode detals} \"\"\" logger . debug ( read . query_name ) # instantiate some dict objects tag_dict = dict () barcode_dict = dict () # decompose_barcode determines whether the function expects to see # an undecomposed barcode string in the id location of the read.id # or an already parsed barcode string if decompose_barcode : try : tag_str = read . query_name . split ( '_' )[ 1 ] logger . debug ( tag_str ) except IndexError as exc : raise IndexError ( 'No read ID present -- ' 'expecting a string appended to the read ' 'ID with a _ in the bam' ) from exc barcode_dict = self . decompose_barcode ( tag_str ) for k , v in barcode_dict [ 'details' ] . items (): bam_tag = v . get ( 'bam_tag' , None ) if bam_tag : tag_dict [ bam_tag ] = v . get ( 'query' ) + '/' + str ( v . get ( 'dist' )) else : # add tags from the id line for tag , value in self . extract_tag_dict ( read . query_name ) . items (): tag_dict [ tag ] = value # (using the bitwise operator) check if the read is unmapped, # if so, set the region_dict start and end to *, indicating that # there is no alignment, and so there is no start and end region for # the alignment if read . flag & 0x4 : tag_dict [ 'XS' ] = \"*\" tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" # if the bit flag 0x10 is set, the read reverse strand. # Handle accordingly elif read . flag & 0x10 : # A cigartuple looks like [(0,4), (2,2), (1,6),..,(4,68)] if read # is reverse complement. If it is forward, it would have the # (4,68), in this case, in the first position. # The first entry in the tuple is the cigar operation and the # second is the length. Note that pysam does order the tuples in # the reverse order from the sam cigar specs, so cigar 30M would be # (0,30). 4 is cigar S or BAM_CSOFT_CLIP. The list operation below # extracts the length of cigar operation 4 and returns a integer. # if 4 DNE, then soft_clip_length is 0. try : soft_clip_length = read . cigartuples [ - 1 ][ 1 ] \\ if read . cigartuples [ - 1 ][ 0 ] == 4 \\ else 0 except TypeError as exc : raise TypeError ( f \"Read { read . query_name } , \" f \"cigar string { read . cigartuples } \" f \"is not parse-able\" ) \\ from exc # The insertion point is at the end of the alignment # note that this is -1 because per the docs # reference_end points to one past the last aligned residue. read_5_prime = ( read . reference_end - 1 ) + soft_clip_length # this is the `insert_length` number bases which precede the # read (after adjusting for soft clipping) try : # if the soft-clip adjustment put the 3 prime end beyond the # end of the chrom, set XS to * # TODO remove removeprefix removesuffix once ref genome # fixed for yeast if ( read_5_prime > self . genome . get_reference_length ( read . reference_name )): tag_dict [ 'XS' ] = \"*\" tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" # if the endpoint of the insertion sequence is off the end of # the chrom, set XZ to * elif ( read_5_prime + 1 + self . insert_length >= self . genome . get_reference_length ( read . reference_name )): tag_dict [ 'XS' ] = read_5_prime tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" else : # This is the first base -- adjusted for soft clipping -- # in the read which cover the genome tag_dict [ 'XS' ] = read_5_prime tag_dict [ 'XI' ] = read_5_prime + 1 tag_dict [ 'XE' ] = read_5_prime + 1 + self . insert_length # TODO remove removeprefix remove suffix once reference # genome is fixed for yeast tag_dict [ 'XZ' ] = self . genome . fetch ( read . reference_name , read_5_prime + 1 , read_5_prime + 1 + self . insert_length ) . upper () # noqa except ValueError as exc : raise ValueError ( f \"Read { read . query_name } , \" f \"insert region { read . reference_name } : { read_5_prime + 1 } -\" # noqa f \" { read_5_prime + 1 + self . insert_length } \" f \"is out of bounds\" ) \\ from exc # else, Read is in the forward orientation. Note that a single end # forward strand read with no other flags will have flag 0 else : # see if clause for lengthy explanation. This examines the first # operation in the cigar string. If it is a soft clip (code 4), # the length of the soft clipping is stored. Else there is 0 soft # clipping try : soft_clip_length = read . cigartuples [ 0 ][ 1 ] \\ if read . cigartuples [ 0 ][ 0 ] == 4 \\ else 0 except TypeError as exc : raise TypeError ( f \"Read { read . query_name } , \" f \"cigar string { read . cigartuples } is not \" f \"parse-able\" ) \\ from exc # extract insert position read_5_prime = read . reference_start - soft_clip_length # this is the `insert_length` number bases which precede the # read (after adjusting for soft clipping) try : # if the 5 prime end, after soft clipping, is less than 0, set # XS to * if ( read_5_prime < 0 ): tag_dict [ 'XS' ] = \"*\" tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" # if the insertion sequence extends beyond the beginning of the # chrom, set to * elif ( read_5_prime - self . insert_length < 0 ): tag_dict [ 'XS' ] = read_5_prime tag_dict [ 'XI' ] = \"*\" tag_dict [ 'XE' ] = \"*\" tag_dict [ 'XZ' ] = \"*\" else : # This is the first base -- adjusted for soft clipping -- # in the read which cover the genome tag_dict [ 'XS' ] = read_5_prime tag_dict [ 'XI' ] = read_5_prime - self . insert_length tag_dict [ 'XE' ] = read_5_prime # TODO remove the removeprefix removesuffix -- need to # standardize rob's genome names tag_dict [ 'XZ' ] = self . genome . fetch ( read . reference_name , read_5_prime - self . insert_length , # noqa read_5_prime ) . upper () except ValueError as exc : raise ValueError ( f \"Read { read . query_name } , \" f \"insert region \" f \" { read . reference_name } : { read_5_prime - self . insert_length } -\" # noqa f \" { read_5_prime } is out of bounds\" ) from exc # Set tags ------------------------------------------------------------ for tag , tag_str in tag_dict . items (): read . set_tag ( tag , tag_str ) return { 'read' : read , 'barcode_details' : barcode_dict }","title":"tag_read()"},{"location":"API/Alignment/SummaryParser/","text":"SummaryParser \u00b6 Class to parse summary data with provided grouping and ordering parameters. Able to convert this data into qBED format, a variant of the BED format. Source code in callingcardstools/Alignment/SummaryParser.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class SummaryParser (): \"\"\" Class to parse summary data with provided grouping and ordering parameters. Able to convert this data into qBED format, a variant of the BED format. \"\"\" _query_string = \"status == 0\" _summary_columns = { 'id' : str , 'status' : int , 'mapq' : int , 'flag' : int , 'chr' : str , 'strand' : str , 'five_prime' : str , 'insert_start' : str , 'insert_stop' : str , 'insert_seq' : str , 'depth' : int } _grouping_fields = { 'chr' , 'insert_start' , 'insert_stop' , 'strand' } _qbed_col_order = \\ [ 'chr' , 'start' , 'end' , 'depth' , 'strand' ] _summary = None def __init__ ( self , summary : Union [ str , pd . DataFrame ]) -> None : \"\"\" Initialize SummaryParser with given summary data. Args: summary (Union[str, pd.DataFrame]): Either a path to a CSV file or an existing pandas DataFrame. \"\"\" self . summary = summary @property def query_string ( self ): \"\"\" Query string for filtering summary data. Default is \"status == 0\". \"\"\" return self . _query_string @query_string . setter def query_string ( self , query_string : str ): self . _query_string = query_string @property def summary ( self ): \"\"\" The summary data in DataFrame format. \"\"\" return self . _summary @summary . setter def summary ( self , summary : Union [ str , pd . DataFrame ]): # check input if isinstance ( summary , str ): # check genome and index paths if not os . path . exists ( summary ): raise FileNotFoundError ( f \"Input file DNE: { summary } \" ) summary = pd . read_csv ( summary , dtype = self . summary_columns ) elif isinstance ( summary , pd . DataFrame ): logger . info ( f 'passed a dataframe to SummaryParser' ) else : raise IOError ( f ' { summary } is not a data type recognized ' + 'as a summary by SummaryParser' ) if 'depth' not in summary . columns : summary [ 'depth' ] = 1 self . _verify ( summary ) self . _summary = summary @property def summary_columns ( self ): \"\"\" The expected structure (column names and data types) of the summary data. \"\"\" return self . _summary_columns @summary_columns . setter def summary_columns ( self , col_list : list ): self . _summary_columns = col_list @property def grouping_fields ( self ): \"\"\" The set of fields to be used for grouping data in summary. \"\"\" return self . _grouping_fields @grouping_fields . setter def grouping_fields ( self , new_grouping_fields : dict ): self . grouping_fields = new_grouping_fields @property def qbed_col_order ( self ): \"\"\" Order of columns to be used when generating a DataFrame in qBED format. \"\"\" return self . _qbed_col_order @qbed_col_order . setter def qbed_col_order ( self , new_col_order : list ): self . _qbed_col_order = new_col_order def _verify ( self , summary : pd . DataFrame ) -> None : \"\"\" Verifies that the provided summary DataFrame matches the expected structure. Args: summary (pd.DataFrame): Summary data as a DataFrame. Raises: ValueError: Raised when the structure of the summary data does not match the expected structure. \"\"\" if not len ( set ( self . summary_columns . keys ()) - set ( summary . columns )) == 0 : raise ValueError ( f \"The expected summary columns are \" f \" { ',' . join ( self . summary_columns ) } in that order\" ) def to_qbed ( self ) -> pd . DataFrame : \"\"\" Converts the summary data into a DataFrame in qBED format. It uses the query string to filter data, groups by the defined grouping fields, and orders columns as defined in qbed_col_order. Returns: pd.DataFrame: A DataFrame in qBED format. \"\"\" local_grouping_fields = self . grouping_fields return self . summary \\ . query ( self . query_string )[[ 'chr' , 'insert_start' , 'insert_stop' , 'depth' , 'strand' ]] \\ . groupby ( list ( local_grouping_fields ))[ 'depth' ] \\ . agg ([ 'sum' ]) \\ . reset_index () \\ . rename ( columns = { 'sum' : 'depth' , 'insert_start' : 'start' , 'insert_stop' : 'end' })[ self . qbed_col_order ] def write_qbed ( self , output_path : str ) -> None : \"\"\" Writes the qBED-formatted DataFrame to a text file at the given path. Args: output_path (str): The path to the file where the output should be written. \"\"\" if not output_path [ - 4 :] in [ '.tsv' , 'txt' ]: logger . warning ( f \"output path { output_path } does not end with tsv or txt\" ) self . to_qbed () . to_csv ( output_path , sep = \" \\t \" , header = None , index = False ) grouping_fields property writable \u00b6 The set of fields to be used for grouping data in summary. qbed_col_order property writable \u00b6 Order of columns to be used when generating a DataFrame in qBED format. query_string property writable \u00b6 Query string for filtering summary data. Default is \u201cstatus == 0\u201d. summary property writable \u00b6 The summary data in DataFrame format. summary_columns property writable \u00b6 The expected structure (column names and data types) of the summary data. __init__ ( summary ) \u00b6 Initialize SummaryParser with given summary data. Parameters: Name Type Description Default summary Union [ str , DataFrame ] Either a path to a CSV file or an existing pandas DataFrame. required Source code in callingcardstools/Alignment/SummaryParser.py 35 36 37 38 39 40 41 42 43 def __init__ ( self , summary : Union [ str , pd . DataFrame ]) -> None : \"\"\" Initialize SummaryParser with given summary data. Args: summary (Union[str, pd.DataFrame]): Either a path to a CSV file or an existing pandas DataFrame. \"\"\" self . summary = summary to_qbed () \u00b6 Converts the summary data into a DataFrame in qBED format. It uses the query string to filter data, groups by the defined grouping fields, and orders columns as defined in qbed_col_order. Returns: Type Description DataFrame pd.DataFrame: A DataFrame in qBED format. Source code in callingcardstools/Alignment/SummaryParser.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def to_qbed ( self ) -> pd . DataFrame : \"\"\" Converts the summary data into a DataFrame in qBED format. It uses the query string to filter data, groups by the defined grouping fields, and orders columns as defined in qbed_col_order. Returns: pd.DataFrame: A DataFrame in qBED format. \"\"\" local_grouping_fields = self . grouping_fields return self . summary \\ . query ( self . query_string )[[ 'chr' , 'insert_start' , 'insert_stop' , 'depth' , 'strand' ]] \\ . groupby ( list ( local_grouping_fields ))[ 'depth' ] \\ . agg ([ 'sum' ]) \\ . reset_index () \\ . rename ( columns = { 'sum' : 'depth' , 'insert_start' : 'start' , 'insert_stop' : 'end' })[ self . qbed_col_order ] write_qbed ( output_path ) \u00b6 Writes the qBED-formatted DataFrame to a text file at the given path. Parameters: Name Type Description Default output_path str The path to the file where the output should be written. required Source code in callingcardstools/Alignment/SummaryParser.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def write_qbed ( self , output_path : str ) -> None : \"\"\" Writes the qBED-formatted DataFrame to a text file at the given path. Args: output_path (str): The path to the file where the output should be written. \"\"\" if not output_path [ - 4 :] in [ '.tsv' , 'txt' ]: logger . warning ( f \"output path { output_path } does not end with tsv or txt\" ) self . to_qbed () . to_csv ( output_path , sep = \" \\t \" , header = None , index = False )","title":"SummaryParser"},{"location":"API/Alignment/SummaryParser/#callingcardstools.Alignment.SummaryParser.SummaryParser","text":"Class to parse summary data with provided grouping and ordering parameters. Able to convert this data into qBED format, a variant of the BED format. Source code in callingcardstools/Alignment/SummaryParser.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class SummaryParser (): \"\"\" Class to parse summary data with provided grouping and ordering parameters. Able to convert this data into qBED format, a variant of the BED format. \"\"\" _query_string = \"status == 0\" _summary_columns = { 'id' : str , 'status' : int , 'mapq' : int , 'flag' : int , 'chr' : str , 'strand' : str , 'five_prime' : str , 'insert_start' : str , 'insert_stop' : str , 'insert_seq' : str , 'depth' : int } _grouping_fields = { 'chr' , 'insert_start' , 'insert_stop' , 'strand' } _qbed_col_order = \\ [ 'chr' , 'start' , 'end' , 'depth' , 'strand' ] _summary = None def __init__ ( self , summary : Union [ str , pd . DataFrame ]) -> None : \"\"\" Initialize SummaryParser with given summary data. Args: summary (Union[str, pd.DataFrame]): Either a path to a CSV file or an existing pandas DataFrame. \"\"\" self . summary = summary @property def query_string ( self ): \"\"\" Query string for filtering summary data. Default is \"status == 0\". \"\"\" return self . _query_string @query_string . setter def query_string ( self , query_string : str ): self . _query_string = query_string @property def summary ( self ): \"\"\" The summary data in DataFrame format. \"\"\" return self . _summary @summary . setter def summary ( self , summary : Union [ str , pd . DataFrame ]): # check input if isinstance ( summary , str ): # check genome and index paths if not os . path . exists ( summary ): raise FileNotFoundError ( f \"Input file DNE: { summary } \" ) summary = pd . read_csv ( summary , dtype = self . summary_columns ) elif isinstance ( summary , pd . DataFrame ): logger . info ( f 'passed a dataframe to SummaryParser' ) else : raise IOError ( f ' { summary } is not a data type recognized ' + 'as a summary by SummaryParser' ) if 'depth' not in summary . columns : summary [ 'depth' ] = 1 self . _verify ( summary ) self . _summary = summary @property def summary_columns ( self ): \"\"\" The expected structure (column names and data types) of the summary data. \"\"\" return self . _summary_columns @summary_columns . setter def summary_columns ( self , col_list : list ): self . _summary_columns = col_list @property def grouping_fields ( self ): \"\"\" The set of fields to be used for grouping data in summary. \"\"\" return self . _grouping_fields @grouping_fields . setter def grouping_fields ( self , new_grouping_fields : dict ): self . grouping_fields = new_grouping_fields @property def qbed_col_order ( self ): \"\"\" Order of columns to be used when generating a DataFrame in qBED format. \"\"\" return self . _qbed_col_order @qbed_col_order . setter def qbed_col_order ( self , new_col_order : list ): self . _qbed_col_order = new_col_order def _verify ( self , summary : pd . DataFrame ) -> None : \"\"\" Verifies that the provided summary DataFrame matches the expected structure. Args: summary (pd.DataFrame): Summary data as a DataFrame. Raises: ValueError: Raised when the structure of the summary data does not match the expected structure. \"\"\" if not len ( set ( self . summary_columns . keys ()) - set ( summary . columns )) == 0 : raise ValueError ( f \"The expected summary columns are \" f \" { ',' . join ( self . summary_columns ) } in that order\" ) def to_qbed ( self ) -> pd . DataFrame : \"\"\" Converts the summary data into a DataFrame in qBED format. It uses the query string to filter data, groups by the defined grouping fields, and orders columns as defined in qbed_col_order. Returns: pd.DataFrame: A DataFrame in qBED format. \"\"\" local_grouping_fields = self . grouping_fields return self . summary \\ . query ( self . query_string )[[ 'chr' , 'insert_start' , 'insert_stop' , 'depth' , 'strand' ]] \\ . groupby ( list ( local_grouping_fields ))[ 'depth' ] \\ . agg ([ 'sum' ]) \\ . reset_index () \\ . rename ( columns = { 'sum' : 'depth' , 'insert_start' : 'start' , 'insert_stop' : 'end' })[ self . qbed_col_order ] def write_qbed ( self , output_path : str ) -> None : \"\"\" Writes the qBED-formatted DataFrame to a text file at the given path. Args: output_path (str): The path to the file where the output should be written. \"\"\" if not output_path [ - 4 :] in [ '.tsv' , 'txt' ]: logger . warning ( f \"output path { output_path } does not end with tsv or txt\" ) self . to_qbed () . to_csv ( output_path , sep = \" \\t \" , header = None , index = False )","title":"SummaryParser"},{"location":"API/Alignment/SummaryParser/#callingcardstools.Alignment.SummaryParser.SummaryParser.grouping_fields","text":"The set of fields to be used for grouping data in summary.","title":"grouping_fields"},{"location":"API/Alignment/SummaryParser/#callingcardstools.Alignment.SummaryParser.SummaryParser.qbed_col_order","text":"Order of columns to be used when generating a DataFrame in qBED format.","title":"qbed_col_order"},{"location":"API/Alignment/SummaryParser/#callingcardstools.Alignment.SummaryParser.SummaryParser.query_string","text":"Query string for filtering summary data. Default is \u201cstatus == 0\u201d.","title":"query_string"},{"location":"API/Alignment/SummaryParser/#callingcardstools.Alignment.SummaryParser.SummaryParser.summary","text":"The summary data in DataFrame format.","title":"summary"},{"location":"API/Alignment/SummaryParser/#callingcardstools.Alignment.SummaryParser.SummaryParser.summary_columns","text":"The expected structure (column names and data types) of the summary data.","title":"summary_columns"},{"location":"API/Alignment/SummaryParser/#callingcardstools.Alignment.SummaryParser.SummaryParser.__init__","text":"Initialize SummaryParser with given summary data. Parameters: Name Type Description Default summary Union [ str , DataFrame ] Either a path to a CSV file or an existing pandas DataFrame. required Source code in callingcardstools/Alignment/SummaryParser.py 35 36 37 38 39 40 41 42 43 def __init__ ( self , summary : Union [ str , pd . DataFrame ]) -> None : \"\"\" Initialize SummaryParser with given summary data. Args: summary (Union[str, pd.DataFrame]): Either a path to a CSV file or an existing pandas DataFrame. \"\"\" self . summary = summary","title":"__init__()"},{"location":"API/Alignment/SummaryParser/#callingcardstools.Alignment.SummaryParser.SummaryParser.to_qbed","text":"Converts the summary data into a DataFrame in qBED format. It uses the query string to filter data, groups by the defined grouping fields, and orders columns as defined in qbed_col_order. Returns: Type Description DataFrame pd.DataFrame: A DataFrame in qBED format. Source code in callingcardstools/Alignment/SummaryParser.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def to_qbed ( self ) -> pd . DataFrame : \"\"\" Converts the summary data into a DataFrame in qBED format. It uses the query string to filter data, groups by the defined grouping fields, and orders columns as defined in qbed_col_order. Returns: pd.DataFrame: A DataFrame in qBED format. \"\"\" local_grouping_fields = self . grouping_fields return self . summary \\ . query ( self . query_string )[[ 'chr' , 'insert_start' , 'insert_stop' , 'depth' , 'strand' ]] \\ . groupby ( list ( local_grouping_fields ))[ 'depth' ] \\ . agg ([ 'sum' ]) \\ . reset_index () \\ . rename ( columns = { 'sum' : 'depth' , 'insert_start' : 'start' , 'insert_stop' : 'end' })[ self . qbed_col_order ]","title":"to_qbed()"},{"location":"API/Alignment/SummaryParser/#callingcardstools.Alignment.SummaryParser.SummaryParser.write_qbed","text":"Writes the qBED-formatted DataFrame to a text file at the given path. Parameters: Name Type Description Default output_path str The path to the file where the output should be written. required Source code in callingcardstools/Alignment/SummaryParser.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def write_qbed ( self , output_path : str ) -> None : \"\"\" Writes the qBED-formatted DataFrame to a text file at the given path. Args: output_path (str): The path to the file where the output should be written. \"\"\" if not output_path [ - 4 :] in [ '.tsv' , 'txt' ]: logger . warning ( f \"output path { output_path } does not end with tsv or txt\" ) self . to_qbed () . to_csv ( output_path , sep = \" \\t \" , header = None , index = False )","title":"write_qbed()"},{"location":"API/Alignment/mammals/Qbed/","text":"InnerDefaultDict \u00b6 Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/Alignment/mammals/Qbed.py 21 22 23 24 25 26 27 28 29 class InnerDefaultDict ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( data_type ) MiddleDefaultDict1 \u00b6 Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/Alignment/mammals/Qbed.py 32 33 34 35 36 37 38 39 40 class MiddleDefaultDict1 ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( partial ( InnerDefaultDict , data_type )) MiddleDefaultDict2 \u00b6 Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/Alignment/mammals/Qbed.py 43 44 45 46 47 48 49 50 51 class MiddleDefaultDict2 ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( partial ( MiddleDefaultDict1 , data_type )) MiddleDefaultDict3 \u00b6 Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/Alignment/mammals/Qbed.py 54 55 56 57 58 59 60 61 62 class MiddleDefaultDict3 ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( partial ( MiddleDefaultDict2 , data_type )) OuterDefaultDict \u00b6 Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/Alignment/mammals/Qbed.py 65 66 67 68 69 70 71 72 73 class OuterDefaultDict ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( partial ( MiddleDefaultDict3 , data_type )) Qbed \u00b6 An object to write records from a tagged_read_dict to qbed file and qc files. See https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8150125/ for more details. Attributes: Name Type Description qbed_fields list List of strings. Values in list are column names for qbed file. qbed OuterDefaultDict A nested defaultdict object. The keys are the qbed_fields. The values are the counts of each record. status_dict DefaultDict A defaultdict object. The keys are the status flags. The values are the counts of each status flag. Source code in callingcardstools/Alignment/mammals/Qbed.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 class Qbed (): \"\"\" An object to write records from a tagged_read_dict to qbed file and qc files. See https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8150125/ for more details. Attributes: qbed_fields (list): List of strings. Values in list are column names for qbed file. qbed (OuterDefaultDict): A nested defaultdict object. The keys are the qbed_fields. The values are the counts of each record. status_dict (DefaultDict): A defaultdict object. The keys are the status flags. The values are the counts of each status flag. \"\"\" _qbed_fields : list _qbed : DefaultDict _status_dict : DefaultDict def __init__ ( self , pickle_path : str = None ) -> None : \"\"\" Create a ReadRecords object. This object will write records to a qbed file and a qc file. Args: pickle_path: Path to a pickle file to load. If None, then initialize a new ReadRecords object. \"\"\" if pickle_path : if not os . path . exists ( pickle_path ): msg = f \"Path to pickle file { pickle_path } does not exist\" raise FileNotFoundError ( msg ) self . load ( pickle_path ) else : # set qbed fields self . qbed_fields = [ 'chr' , 'start' , 'end' , 'depth' , 'strand' , 'annotation' ] self . qbed = OuterDefaultDict ( int ) self . status_dict = DefaultDict ( int ) @property def qbed_fields ( self ): \"\"\"Get the qbed fields\"\"\" return self . _qbed_fields @qbed_fields . setter def qbed_fields ( self , value : list ): \"\"\"Set the qbed fields\"\"\" if not len ( value ) == 6 : raise ValueError ( 'qbed_fields must have 6 values' ) self . _qbed_fields = value @property def qbed ( self ): \"\"\"Get the qbed\"\"\" return self . _qbed @qbed . setter def qbed ( self , value : DefaultDict ): \"\"\"Set the qbed\"\"\" self . _qbed = value @property def status_dict ( self ): \"\"\"Get the status_dict\"\"\" return self . _status_dict @status_dict . setter def status_dict ( self , value : DefaultDict ): \"\"\"Set the status_dict\"\"\" self . _status_dict = value # private methods --------------------------------------------------------- def _combine ( self , other ): \"\"\"Combines the qbed property of two Qbed objects. Args: other (Qbed): Another Qbed object. Raises: ValueError: If the other object is not a Qbed object. \"\"\" if not isinstance ( other , Qbed ): raise ValueError ( 'The other object must be a Qbed object.' ) # Combine qbed property for chr , value1 in other . qbed . items (): for start , value2 in value1 . items (): for end , value3 in value2 . items (): for strand , value4 in value3 . items (): for srt_seq , count in value4 . items (): ( self . qbed [ chr ] [ start ] [ end ] [ strand ] [ srt_seq ]) += count # Combine status_dict property for status , count in other . status_dict . items (): self . status_dict [ status ] += count def _srt_writer ( self , output_path : str , single_srt_count : int , multi_srt_count : int ) -> None : # Open a TSV file for writing. with open ( output_path , \"w\" , newline = \"\" , encoding = 'utf-8' ) as tsvfile : fieldnames = [ 'srt_type' , 'count' ] # Create a DictWriter instance with a tab delimiter. writer = csv . DictWriter ( tsvfile , fieldnames = fieldnames , delimiter = ' \\t ' ) # Write writer . writeheader () writer . writerow ({ 'srt_type' : 'single_srt' , 'count' : single_srt_count }) writer . writerow ({ 'srt_type' : 'multi_srt' , 'count' : multi_srt_count }) # public methods ---------------------------------------------------------- def load ( self , file_path : str ) -> None : \"\"\"Load a BarcodeQcCounter object from a file using Pickle. Args: file_path (str): The file path where the object is stored. \"\"\" logger . info ( \"loading Qbed object from %s \" , file_path ) with open ( file_path , \"rb\" ) as file : file_data = pickle . load ( file ) if not isinstance ( file_data , Qbed ): raise TypeError ( f \" { file_path } is not a Qbed object\" ) # copy the data from the loaded object to the current instance self . qbed_fields = file_data . qbed_fields self . qbed = file_data . qbed self . status_dict = file_data . status_dict @classmethod def combine ( cls , counters : Iterable [ \"Qbed\" ]) -> \"Qbed\" : \"\"\"Combine multiple Qbed objects into a single object. Args: counters (Iterable[Qbed]): An iterable of Qbed objects. Returns: Qbed: A new Qbed object formed from the list of input Qbeds. \"\"\" result = Qbed () for counter in counters : result . _combine ( counter ) return result def __add__ ( self , other : \"Qbed\" ) -> \"Qbed\" : \"\"\"Add two Qbed objects together with the + operator.\"\"\" if not isinstance ( other , Qbed ): raise TypeError ( \"Both objects must be of type 'Qbed'\" ) result = Qbed () return result . combine ([ self , other ]) def update ( self , tagged_read : dict , status : int , insert_offset : int = 1 , srt_tag : str = 'ST' ) -> None : \"\"\"write records to both the raw qbed tmpfile and raw qc tmpfile. Note that these tempfiles will be destroyed when the object is destroyed. Args: tagged_read (dict): A pysam.AlignedSegment object which has been tagged with the appropriate calling cards tags based on the BarcodeParser object used to create the object. status (int): A value which reflects how the read performs based on pre-defined quality metrics. A status of 0 is considered a pass. A status of greater than 0 is a read which fails at least 1 quality metric insert_offset (int): number to add to tag XI value to calculate the end coordinate. For instance, if the start coord is the first T in TTAA, then the offset would be 4. srt_tag (str): The tag which corresponds to the SRT sequence of a given read. This will be included in the annotation column of the mammals qbed file. \"\"\" if len ({ 'read' , 'barcode_details' } - tagged_read . keys ()) > 0 : raise KeyError ( 'tagged_read must have keys ' '{\"reads\",\"barcode_details\"}' ) if status == 0 : # for mammals, the SRT tag is expected. This will raise a KeyError # if the SRT tag is not present try : srt_with_edit_dist = tagged_read [ 'read' ] . get_tag ( srt_tag ) srt = re . sub ( r '\\/\\d+' , '' , srt_with_edit_dist ) except KeyError as exc : raise f \"tagged_read must have SRT key { srt_tag } \" from exc chr = tagged_read [ 'read' ] . reference_name start = tagged_read [ 'read' ] . get_tag ( 'XI' ) end = tagged_read [ 'read' ] . get_tag ( 'XI' ) + insert_offset strand = '+' if tagged_read [ 'read' ] . is_forward else '-' self . qbed [ chr ][ start ][ end ][ strand ][ srt ] += 1 self . status_dict [ status ] += 1 def write ( self , filename : str , suffix : str = \"\" , raw : bool = False ) -> pd . DataFrame : \"\"\"Translate the qbed object and status_dict to DataFrames and write to either a pickle or tsv file. Args: filename (str): The name of the file to write to. raw (bool): If True, write to a raw file. Otherwise, write to a tsv file. Returns: [pd.DataFrame, pd.DataFrame]: The qbed and status DataFrames \"\"\" # create qbed DataFrame qbed_df = pd . DataFrame ( columns = self . qbed_fields ) concat_list = [] # List to hold the Series before concatenating single_srt_counter = 0 multi_srt_counter = 0 for chr , value1 in self . qbed . items (): for start , value2 in value1 . items (): for end , value3 in value2 . items (): for strand , value4 in value3 . items (): locus_srt_set = set () for srt_seq , count in value4 . items (): locus_srt_set . add ( srt_seq ) # Prepare a new series to add to qbed DataFrame new_row = pd . Series ([ chr , start , end , count , strand , srt_seq ], index = self . qbed_fields ) concat_list . append ( new_row ) # add a hop record to the qbed DataFrame # count single/multi srt as appropriate if len ( locus_srt_set ) > 1 : multi_srt_counter += 1 else : single_srt_counter += 1 qbed_df = pd . concat ([ qbed_df , pd . DataFrame ( concat_list , columns = self . qbed_fields )], ignore_index = True ) # create status DataFrame status_df = pd . DataFrame ( columns = [ 'status' , 'count' ]) concat_list = [] # List to hold the Series before concatenating for status , count in self . status_dict . items (): status_str = \",\" . join ( StatusFlags . decompose ( status )) # Prepare a new series to add to status DataFrame new_row = pd . Series ([ status_str , count ], index = [ 'status' , 'count' ]) concat_list . append ( new_row ) status_df = pd . concat ([ status_df , pd . DataFrame ( concat_list , columns = [ 'status' , 'count' ])], ignore_index = True ) # write to file if raw : pickle_output_file = filename + '_' + suffix + '_qbed.pkl' \\ if suffix else filename + '_qbed.pkl' logger . info ( \"writing Qbed object to %s \" , pickle_output_file ) with open ( pickle_output_file , 'wb' ) as file : pickle . dump ( self , file ) else : qbed_output_file = filename + '_' + suffix + '.qbed' \\ if suffix else filename + '.qbed' logger . info ( \"writing qbed to %s \" , qbed_output_file ) qbed_df . to_csv ( qbed_output_file , sep = ' \\t ' , index = False , header = False ) qc_output_file = filename + '_' + suffix + '_aln_summary.tsv' \\ if suffix else filename + '_aln_summary.tsv' logger . info ( \"writing qc summary to %s \" , qc_output_file ) status_df . to_csv ( qc_output_file , sep = ' \\t ' , index = False , header = False ) srt_output_file = filename + '_' + suffix + '_srt_count.tsv' \\ if suffix else filename + '_srt_count.tsv' logger . info ( \"writing srt summary to %s \" , srt_output_file ) self . _srt_writer ( srt_output_file , single_srt_counter , multi_srt_counter ) qbed property writable \u00b6 Get the qbed qbed_fields property writable \u00b6 Get the qbed fields status_dict property writable \u00b6 Get the status_dict __add__ ( other ) \u00b6 Add two Qbed objects together with the + operator. Source code in callingcardstools/Alignment/mammals/Qbed.py 233 234 235 236 237 238 239 def __add__ ( self , other : \"Qbed\" ) -> \"Qbed\" : \"\"\"Add two Qbed objects together with the + operator.\"\"\" if not isinstance ( other , Qbed ): raise TypeError ( \"Both objects must be of type 'Qbed'\" ) result = Qbed () return result . combine ([ self , other ]) __init__ ( pickle_path = None ) \u00b6 Create a ReadRecords object. This object will write records to a qbed file and a qc file. Parameters: Name Type Description Default pickle_path str Path to a pickle file to load. If None, then initialize a new ReadRecords object. None Source code in callingcardstools/Alignment/mammals/Qbed.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def __init__ ( self , pickle_path : str = None ) -> None : \"\"\" Create a ReadRecords object. This object will write records to a qbed file and a qc file. Args: pickle_path: Path to a pickle file to load. If None, then initialize a new ReadRecords object. \"\"\" if pickle_path : if not os . path . exists ( pickle_path ): msg = f \"Path to pickle file { pickle_path } does not exist\" raise FileNotFoundError ( msg ) self . load ( pickle_path ) else : # set qbed fields self . qbed_fields = [ 'chr' , 'start' , 'end' , 'depth' , 'strand' , 'annotation' ] self . qbed = OuterDefaultDict ( int ) self . status_dict = DefaultDict ( int ) combine ( counters ) classmethod \u00b6 Combine multiple Qbed objects into a single object. Parameters: Name Type Description Default counters Iterable [ Qbed ] An iterable of Qbed objects. required Returns: Name Type Description Qbed Qbed A new Qbed object formed from the list of input Qbeds. Source code in callingcardstools/Alignment/mammals/Qbed.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 @classmethod def combine ( cls , counters : Iterable [ \"Qbed\" ]) -> \"Qbed\" : \"\"\"Combine multiple Qbed objects into a single object. Args: counters (Iterable[Qbed]): An iterable of Qbed objects. Returns: Qbed: A new Qbed object formed from the list of input Qbeds. \"\"\" result = Qbed () for counter in counters : result . _combine ( counter ) return result load ( file_path ) \u00b6 Load a BarcodeQcCounter object from a file using Pickle. Parameters: Name Type Description Default file_path str The file path where the object is stored. required Source code in callingcardstools/Alignment/mammals/Qbed.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def load ( self , file_path : str ) -> None : \"\"\"Load a BarcodeQcCounter object from a file using Pickle. Args: file_path (str): The file path where the object is stored. \"\"\" logger . info ( \"loading Qbed object from %s \" , file_path ) with open ( file_path , \"rb\" ) as file : file_data = pickle . load ( file ) if not isinstance ( file_data , Qbed ): raise TypeError ( f \" { file_path } is not a Qbed object\" ) # copy the data from the loaded object to the current instance self . qbed_fields = file_data . qbed_fields self . qbed = file_data . qbed self . status_dict = file_data . status_dict update ( tagged_read , status , insert_offset = 1 , srt_tag = 'ST' ) \u00b6 write records to both the raw qbed tmpfile and raw qc tmpfile. Note that these tempfiles will be destroyed when the object is destroyed. Parameters: Name Type Description Default tagged_read dict A pysam.AlignedSegment object which has been tagged with the appropriate calling cards tags based on the BarcodeParser object used to create the object. required status int A value which reflects how the read performs based on pre-defined quality metrics. A status of 0 is considered a pass. A status of greater than 0 is a read which fails at least 1 quality metric required insert_offset int number to add to tag XI value to calculate the end coordinate. For instance, if the start coord is the first T in TTAA, then the offset would be 4. 1 srt_tag str The tag which corresponds to the SRT sequence of a given read. This will be included in the annotation column of the mammals qbed file. 'ST' Source code in callingcardstools/Alignment/mammals/Qbed.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def update ( self , tagged_read : dict , status : int , insert_offset : int = 1 , srt_tag : str = 'ST' ) -> None : \"\"\"write records to both the raw qbed tmpfile and raw qc tmpfile. Note that these tempfiles will be destroyed when the object is destroyed. Args: tagged_read (dict): A pysam.AlignedSegment object which has been tagged with the appropriate calling cards tags based on the BarcodeParser object used to create the object. status (int): A value which reflects how the read performs based on pre-defined quality metrics. A status of 0 is considered a pass. A status of greater than 0 is a read which fails at least 1 quality metric insert_offset (int): number to add to tag XI value to calculate the end coordinate. For instance, if the start coord is the first T in TTAA, then the offset would be 4. srt_tag (str): The tag which corresponds to the SRT sequence of a given read. This will be included in the annotation column of the mammals qbed file. \"\"\" if len ({ 'read' , 'barcode_details' } - tagged_read . keys ()) > 0 : raise KeyError ( 'tagged_read must have keys ' '{\"reads\",\"barcode_details\"}' ) if status == 0 : # for mammals, the SRT tag is expected. This will raise a KeyError # if the SRT tag is not present try : srt_with_edit_dist = tagged_read [ 'read' ] . get_tag ( srt_tag ) srt = re . sub ( r '\\/\\d+' , '' , srt_with_edit_dist ) except KeyError as exc : raise f \"tagged_read must have SRT key { srt_tag } \" from exc chr = tagged_read [ 'read' ] . reference_name start = tagged_read [ 'read' ] . get_tag ( 'XI' ) end = tagged_read [ 'read' ] . get_tag ( 'XI' ) + insert_offset strand = '+' if tagged_read [ 'read' ] . is_forward else '-' self . qbed [ chr ][ start ][ end ][ strand ][ srt ] += 1 self . status_dict [ status ] += 1 write ( filename , suffix = '' , raw = False ) \u00b6 Translate the qbed object and status_dict to DataFrames and write to either a pickle or tsv file. Parameters: Name Type Description Default filename str The name of the file to write to. required raw bool If True, write to a raw file. Otherwise, write to a tsv file. False Returns: Type Description DataFrame [pd.DataFrame, pd.DataFrame]: The qbed and status DataFrames Source code in callingcardstools/Alignment/mammals/Qbed.py 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 def write ( self , filename : str , suffix : str = \"\" , raw : bool = False ) -> pd . DataFrame : \"\"\"Translate the qbed object and status_dict to DataFrames and write to either a pickle or tsv file. Args: filename (str): The name of the file to write to. raw (bool): If True, write to a raw file. Otherwise, write to a tsv file. Returns: [pd.DataFrame, pd.DataFrame]: The qbed and status DataFrames \"\"\" # create qbed DataFrame qbed_df = pd . DataFrame ( columns = self . qbed_fields ) concat_list = [] # List to hold the Series before concatenating single_srt_counter = 0 multi_srt_counter = 0 for chr , value1 in self . qbed . items (): for start , value2 in value1 . items (): for end , value3 in value2 . items (): for strand , value4 in value3 . items (): locus_srt_set = set () for srt_seq , count in value4 . items (): locus_srt_set . add ( srt_seq ) # Prepare a new series to add to qbed DataFrame new_row = pd . Series ([ chr , start , end , count , strand , srt_seq ], index = self . qbed_fields ) concat_list . append ( new_row ) # add a hop record to the qbed DataFrame # count single/multi srt as appropriate if len ( locus_srt_set ) > 1 : multi_srt_counter += 1 else : single_srt_counter += 1 qbed_df = pd . concat ([ qbed_df , pd . DataFrame ( concat_list , columns = self . qbed_fields )], ignore_index = True ) # create status DataFrame status_df = pd . DataFrame ( columns = [ 'status' , 'count' ]) concat_list = [] # List to hold the Series before concatenating for status , count in self . status_dict . items (): status_str = \",\" . join ( StatusFlags . decompose ( status )) # Prepare a new series to add to status DataFrame new_row = pd . Series ([ status_str , count ], index = [ 'status' , 'count' ]) concat_list . append ( new_row ) status_df = pd . concat ([ status_df , pd . DataFrame ( concat_list , columns = [ 'status' , 'count' ])], ignore_index = True ) # write to file if raw : pickle_output_file = filename + '_' + suffix + '_qbed.pkl' \\ if suffix else filename + '_qbed.pkl' logger . info ( \"writing Qbed object to %s \" , pickle_output_file ) with open ( pickle_output_file , 'wb' ) as file : pickle . dump ( self , file ) else : qbed_output_file = filename + '_' + suffix + '.qbed' \\ if suffix else filename + '.qbed' logger . info ( \"writing qbed to %s \" , qbed_output_file ) qbed_df . to_csv ( qbed_output_file , sep = ' \\t ' , index = False , header = False ) qc_output_file = filename + '_' + suffix + '_aln_summary.tsv' \\ if suffix else filename + '_aln_summary.tsv' logger . info ( \"writing qc summary to %s \" , qc_output_file ) status_df . to_csv ( qc_output_file , sep = ' \\t ' , index = False , header = False ) srt_output_file = filename + '_' + suffix + '_srt_count.tsv' \\ if suffix else filename + '_srt_count.tsv' logger . info ( \"writing srt summary to %s \" , srt_output_file ) self . _srt_writer ( srt_output_file , single_srt_counter , multi_srt_counter )","title":"Qbed"},{"location":"API/Alignment/mammals/Qbed/#callingcardstools.Alignment.mammals.Qbed.InnerDefaultDict","text":"Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/Alignment/mammals/Qbed.py 21 22 23 24 25 26 27 28 29 class InnerDefaultDict ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( data_type )","title":"InnerDefaultDict"},{"location":"API/Alignment/mammals/Qbed/#callingcardstools.Alignment.mammals.Qbed.MiddleDefaultDict1","text":"Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/Alignment/mammals/Qbed.py 32 33 34 35 36 37 38 39 40 class MiddleDefaultDict1 ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( partial ( InnerDefaultDict , data_type ))","title":"MiddleDefaultDict1"},{"location":"API/Alignment/mammals/Qbed/#callingcardstools.Alignment.mammals.Qbed.MiddleDefaultDict2","text":"Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/Alignment/mammals/Qbed.py 43 44 45 46 47 48 49 50 51 class MiddleDefaultDict2 ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( partial ( MiddleDefaultDict1 , data_type ))","title":"MiddleDefaultDict2"},{"location":"API/Alignment/mammals/Qbed/#callingcardstools.Alignment.mammals.Qbed.MiddleDefaultDict3","text":"Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/Alignment/mammals/Qbed.py 54 55 56 57 58 59 60 61 62 class MiddleDefaultDict3 ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( partial ( MiddleDefaultDict2 , data_type ))","title":"MiddleDefaultDict3"},{"location":"API/Alignment/mammals/Qbed/#callingcardstools.Alignment.mammals.Qbed.OuterDefaultDict","text":"Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/Alignment/mammals/Qbed.py 65 66 67 68 69 70 71 72 73 class OuterDefaultDict ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( partial ( MiddleDefaultDict3 , data_type ))","title":"OuterDefaultDict"},{"location":"API/Alignment/mammals/Qbed/#callingcardstools.Alignment.mammals.Qbed.Qbed","text":"An object to write records from a tagged_read_dict to qbed file and qc files. See https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8150125/ for more details. Attributes: Name Type Description qbed_fields list List of strings. Values in list are column names for qbed file. qbed OuterDefaultDict A nested defaultdict object. The keys are the qbed_fields. The values are the counts of each record. status_dict DefaultDict A defaultdict object. The keys are the status flags. The values are the counts of each status flag. Source code in callingcardstools/Alignment/mammals/Qbed.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 class Qbed (): \"\"\" An object to write records from a tagged_read_dict to qbed file and qc files. See https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8150125/ for more details. Attributes: qbed_fields (list): List of strings. Values in list are column names for qbed file. qbed (OuterDefaultDict): A nested defaultdict object. The keys are the qbed_fields. The values are the counts of each record. status_dict (DefaultDict): A defaultdict object. The keys are the status flags. The values are the counts of each status flag. \"\"\" _qbed_fields : list _qbed : DefaultDict _status_dict : DefaultDict def __init__ ( self , pickle_path : str = None ) -> None : \"\"\" Create a ReadRecords object. This object will write records to a qbed file and a qc file. Args: pickle_path: Path to a pickle file to load. If None, then initialize a new ReadRecords object. \"\"\" if pickle_path : if not os . path . exists ( pickle_path ): msg = f \"Path to pickle file { pickle_path } does not exist\" raise FileNotFoundError ( msg ) self . load ( pickle_path ) else : # set qbed fields self . qbed_fields = [ 'chr' , 'start' , 'end' , 'depth' , 'strand' , 'annotation' ] self . qbed = OuterDefaultDict ( int ) self . status_dict = DefaultDict ( int ) @property def qbed_fields ( self ): \"\"\"Get the qbed fields\"\"\" return self . _qbed_fields @qbed_fields . setter def qbed_fields ( self , value : list ): \"\"\"Set the qbed fields\"\"\" if not len ( value ) == 6 : raise ValueError ( 'qbed_fields must have 6 values' ) self . _qbed_fields = value @property def qbed ( self ): \"\"\"Get the qbed\"\"\" return self . _qbed @qbed . setter def qbed ( self , value : DefaultDict ): \"\"\"Set the qbed\"\"\" self . _qbed = value @property def status_dict ( self ): \"\"\"Get the status_dict\"\"\" return self . _status_dict @status_dict . setter def status_dict ( self , value : DefaultDict ): \"\"\"Set the status_dict\"\"\" self . _status_dict = value # private methods --------------------------------------------------------- def _combine ( self , other ): \"\"\"Combines the qbed property of two Qbed objects. Args: other (Qbed): Another Qbed object. Raises: ValueError: If the other object is not a Qbed object. \"\"\" if not isinstance ( other , Qbed ): raise ValueError ( 'The other object must be a Qbed object.' ) # Combine qbed property for chr , value1 in other . qbed . items (): for start , value2 in value1 . items (): for end , value3 in value2 . items (): for strand , value4 in value3 . items (): for srt_seq , count in value4 . items (): ( self . qbed [ chr ] [ start ] [ end ] [ strand ] [ srt_seq ]) += count # Combine status_dict property for status , count in other . status_dict . items (): self . status_dict [ status ] += count def _srt_writer ( self , output_path : str , single_srt_count : int , multi_srt_count : int ) -> None : # Open a TSV file for writing. with open ( output_path , \"w\" , newline = \"\" , encoding = 'utf-8' ) as tsvfile : fieldnames = [ 'srt_type' , 'count' ] # Create a DictWriter instance with a tab delimiter. writer = csv . DictWriter ( tsvfile , fieldnames = fieldnames , delimiter = ' \\t ' ) # Write writer . writeheader () writer . writerow ({ 'srt_type' : 'single_srt' , 'count' : single_srt_count }) writer . writerow ({ 'srt_type' : 'multi_srt' , 'count' : multi_srt_count }) # public methods ---------------------------------------------------------- def load ( self , file_path : str ) -> None : \"\"\"Load a BarcodeQcCounter object from a file using Pickle. Args: file_path (str): The file path where the object is stored. \"\"\" logger . info ( \"loading Qbed object from %s \" , file_path ) with open ( file_path , \"rb\" ) as file : file_data = pickle . load ( file ) if not isinstance ( file_data , Qbed ): raise TypeError ( f \" { file_path } is not a Qbed object\" ) # copy the data from the loaded object to the current instance self . qbed_fields = file_data . qbed_fields self . qbed = file_data . qbed self . status_dict = file_data . status_dict @classmethod def combine ( cls , counters : Iterable [ \"Qbed\" ]) -> \"Qbed\" : \"\"\"Combine multiple Qbed objects into a single object. Args: counters (Iterable[Qbed]): An iterable of Qbed objects. Returns: Qbed: A new Qbed object formed from the list of input Qbeds. \"\"\" result = Qbed () for counter in counters : result . _combine ( counter ) return result def __add__ ( self , other : \"Qbed\" ) -> \"Qbed\" : \"\"\"Add two Qbed objects together with the + operator.\"\"\" if not isinstance ( other , Qbed ): raise TypeError ( \"Both objects must be of type 'Qbed'\" ) result = Qbed () return result . combine ([ self , other ]) def update ( self , tagged_read : dict , status : int , insert_offset : int = 1 , srt_tag : str = 'ST' ) -> None : \"\"\"write records to both the raw qbed tmpfile and raw qc tmpfile. Note that these tempfiles will be destroyed when the object is destroyed. Args: tagged_read (dict): A pysam.AlignedSegment object which has been tagged with the appropriate calling cards tags based on the BarcodeParser object used to create the object. status (int): A value which reflects how the read performs based on pre-defined quality metrics. A status of 0 is considered a pass. A status of greater than 0 is a read which fails at least 1 quality metric insert_offset (int): number to add to tag XI value to calculate the end coordinate. For instance, if the start coord is the first T in TTAA, then the offset would be 4. srt_tag (str): The tag which corresponds to the SRT sequence of a given read. This will be included in the annotation column of the mammals qbed file. \"\"\" if len ({ 'read' , 'barcode_details' } - tagged_read . keys ()) > 0 : raise KeyError ( 'tagged_read must have keys ' '{\"reads\",\"barcode_details\"}' ) if status == 0 : # for mammals, the SRT tag is expected. This will raise a KeyError # if the SRT tag is not present try : srt_with_edit_dist = tagged_read [ 'read' ] . get_tag ( srt_tag ) srt = re . sub ( r '\\/\\d+' , '' , srt_with_edit_dist ) except KeyError as exc : raise f \"tagged_read must have SRT key { srt_tag } \" from exc chr = tagged_read [ 'read' ] . reference_name start = tagged_read [ 'read' ] . get_tag ( 'XI' ) end = tagged_read [ 'read' ] . get_tag ( 'XI' ) + insert_offset strand = '+' if tagged_read [ 'read' ] . is_forward else '-' self . qbed [ chr ][ start ][ end ][ strand ][ srt ] += 1 self . status_dict [ status ] += 1 def write ( self , filename : str , suffix : str = \"\" , raw : bool = False ) -> pd . DataFrame : \"\"\"Translate the qbed object and status_dict to DataFrames and write to either a pickle or tsv file. Args: filename (str): The name of the file to write to. raw (bool): If True, write to a raw file. Otherwise, write to a tsv file. Returns: [pd.DataFrame, pd.DataFrame]: The qbed and status DataFrames \"\"\" # create qbed DataFrame qbed_df = pd . DataFrame ( columns = self . qbed_fields ) concat_list = [] # List to hold the Series before concatenating single_srt_counter = 0 multi_srt_counter = 0 for chr , value1 in self . qbed . items (): for start , value2 in value1 . items (): for end , value3 in value2 . items (): for strand , value4 in value3 . items (): locus_srt_set = set () for srt_seq , count in value4 . items (): locus_srt_set . add ( srt_seq ) # Prepare a new series to add to qbed DataFrame new_row = pd . Series ([ chr , start , end , count , strand , srt_seq ], index = self . qbed_fields ) concat_list . append ( new_row ) # add a hop record to the qbed DataFrame # count single/multi srt as appropriate if len ( locus_srt_set ) > 1 : multi_srt_counter += 1 else : single_srt_counter += 1 qbed_df = pd . concat ([ qbed_df , pd . DataFrame ( concat_list , columns = self . qbed_fields )], ignore_index = True ) # create status DataFrame status_df = pd . DataFrame ( columns = [ 'status' , 'count' ]) concat_list = [] # List to hold the Series before concatenating for status , count in self . status_dict . items (): status_str = \",\" . join ( StatusFlags . decompose ( status )) # Prepare a new series to add to status DataFrame new_row = pd . Series ([ status_str , count ], index = [ 'status' , 'count' ]) concat_list . append ( new_row ) status_df = pd . concat ([ status_df , pd . DataFrame ( concat_list , columns = [ 'status' , 'count' ])], ignore_index = True ) # write to file if raw : pickle_output_file = filename + '_' + suffix + '_qbed.pkl' \\ if suffix else filename + '_qbed.pkl' logger . info ( \"writing Qbed object to %s \" , pickle_output_file ) with open ( pickle_output_file , 'wb' ) as file : pickle . dump ( self , file ) else : qbed_output_file = filename + '_' + suffix + '.qbed' \\ if suffix else filename + '.qbed' logger . info ( \"writing qbed to %s \" , qbed_output_file ) qbed_df . to_csv ( qbed_output_file , sep = ' \\t ' , index = False , header = False ) qc_output_file = filename + '_' + suffix + '_aln_summary.tsv' \\ if suffix else filename + '_aln_summary.tsv' logger . info ( \"writing qc summary to %s \" , qc_output_file ) status_df . to_csv ( qc_output_file , sep = ' \\t ' , index = False , header = False ) srt_output_file = filename + '_' + suffix + '_srt_count.tsv' \\ if suffix else filename + '_srt_count.tsv' logger . info ( \"writing srt summary to %s \" , srt_output_file ) self . _srt_writer ( srt_output_file , single_srt_counter , multi_srt_counter )","title":"Qbed"},{"location":"API/Alignment/mammals/Qbed/#callingcardstools.Alignment.mammals.Qbed.Qbed.qbed","text":"Get the qbed","title":"qbed"},{"location":"API/Alignment/mammals/Qbed/#callingcardstools.Alignment.mammals.Qbed.Qbed.qbed_fields","text":"Get the qbed fields","title":"qbed_fields"},{"location":"API/Alignment/mammals/Qbed/#callingcardstools.Alignment.mammals.Qbed.Qbed.status_dict","text":"Get the status_dict","title":"status_dict"},{"location":"API/Alignment/mammals/Qbed/#callingcardstools.Alignment.mammals.Qbed.Qbed.__add__","text":"Add two Qbed objects together with the + operator. Source code in callingcardstools/Alignment/mammals/Qbed.py 233 234 235 236 237 238 239 def __add__ ( self , other : \"Qbed\" ) -> \"Qbed\" : \"\"\"Add two Qbed objects together with the + operator.\"\"\" if not isinstance ( other , Qbed ): raise TypeError ( \"Both objects must be of type 'Qbed'\" ) result = Qbed () return result . combine ([ self , other ])","title":"__add__()"},{"location":"API/Alignment/mammals/Qbed/#callingcardstools.Alignment.mammals.Qbed.Qbed.__init__","text":"Create a ReadRecords object. This object will write records to a qbed file and a qc file. Parameters: Name Type Description Default pickle_path str Path to a pickle file to load. If None, then initialize a new ReadRecords object. None Source code in callingcardstools/Alignment/mammals/Qbed.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def __init__ ( self , pickle_path : str = None ) -> None : \"\"\" Create a ReadRecords object. This object will write records to a qbed file and a qc file. Args: pickle_path: Path to a pickle file to load. If None, then initialize a new ReadRecords object. \"\"\" if pickle_path : if not os . path . exists ( pickle_path ): msg = f \"Path to pickle file { pickle_path } does not exist\" raise FileNotFoundError ( msg ) self . load ( pickle_path ) else : # set qbed fields self . qbed_fields = [ 'chr' , 'start' , 'end' , 'depth' , 'strand' , 'annotation' ] self . qbed = OuterDefaultDict ( int ) self . status_dict = DefaultDict ( int )","title":"__init__()"},{"location":"API/Alignment/mammals/Qbed/#callingcardstools.Alignment.mammals.Qbed.Qbed.combine","text":"Combine multiple Qbed objects into a single object. Parameters: Name Type Description Default counters Iterable [ Qbed ] An iterable of Qbed objects. required Returns: Name Type Description Qbed Qbed A new Qbed object formed from the list of input Qbeds. Source code in callingcardstools/Alignment/mammals/Qbed.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 @classmethod def combine ( cls , counters : Iterable [ \"Qbed\" ]) -> \"Qbed\" : \"\"\"Combine multiple Qbed objects into a single object. Args: counters (Iterable[Qbed]): An iterable of Qbed objects. Returns: Qbed: A new Qbed object formed from the list of input Qbeds. \"\"\" result = Qbed () for counter in counters : result . _combine ( counter ) return result","title":"combine()"},{"location":"API/Alignment/mammals/Qbed/#callingcardstools.Alignment.mammals.Qbed.Qbed.load","text":"Load a BarcodeQcCounter object from a file using Pickle. Parameters: Name Type Description Default file_path str The file path where the object is stored. required Source code in callingcardstools/Alignment/mammals/Qbed.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def load ( self , file_path : str ) -> None : \"\"\"Load a BarcodeQcCounter object from a file using Pickle. Args: file_path (str): The file path where the object is stored. \"\"\" logger . info ( \"loading Qbed object from %s \" , file_path ) with open ( file_path , \"rb\" ) as file : file_data = pickle . load ( file ) if not isinstance ( file_data , Qbed ): raise TypeError ( f \" { file_path } is not a Qbed object\" ) # copy the data from the loaded object to the current instance self . qbed_fields = file_data . qbed_fields self . qbed = file_data . qbed self . status_dict = file_data . status_dict","title":"load()"},{"location":"API/Alignment/mammals/Qbed/#callingcardstools.Alignment.mammals.Qbed.Qbed.update","text":"write records to both the raw qbed tmpfile and raw qc tmpfile. Note that these tempfiles will be destroyed when the object is destroyed. Parameters: Name Type Description Default tagged_read dict A pysam.AlignedSegment object which has been tagged with the appropriate calling cards tags based on the BarcodeParser object used to create the object. required status int A value which reflects how the read performs based on pre-defined quality metrics. A status of 0 is considered a pass. A status of greater than 0 is a read which fails at least 1 quality metric required insert_offset int number to add to tag XI value to calculate the end coordinate. For instance, if the start coord is the first T in TTAA, then the offset would be 4. 1 srt_tag str The tag which corresponds to the SRT sequence of a given read. This will be included in the annotation column of the mammals qbed file. 'ST' Source code in callingcardstools/Alignment/mammals/Qbed.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def update ( self , tagged_read : dict , status : int , insert_offset : int = 1 , srt_tag : str = 'ST' ) -> None : \"\"\"write records to both the raw qbed tmpfile and raw qc tmpfile. Note that these tempfiles will be destroyed when the object is destroyed. Args: tagged_read (dict): A pysam.AlignedSegment object which has been tagged with the appropriate calling cards tags based on the BarcodeParser object used to create the object. status (int): A value which reflects how the read performs based on pre-defined quality metrics. A status of 0 is considered a pass. A status of greater than 0 is a read which fails at least 1 quality metric insert_offset (int): number to add to tag XI value to calculate the end coordinate. For instance, if the start coord is the first T in TTAA, then the offset would be 4. srt_tag (str): The tag which corresponds to the SRT sequence of a given read. This will be included in the annotation column of the mammals qbed file. \"\"\" if len ({ 'read' , 'barcode_details' } - tagged_read . keys ()) > 0 : raise KeyError ( 'tagged_read must have keys ' '{\"reads\",\"barcode_details\"}' ) if status == 0 : # for mammals, the SRT tag is expected. This will raise a KeyError # if the SRT tag is not present try : srt_with_edit_dist = tagged_read [ 'read' ] . get_tag ( srt_tag ) srt = re . sub ( r '\\/\\d+' , '' , srt_with_edit_dist ) except KeyError as exc : raise f \"tagged_read must have SRT key { srt_tag } \" from exc chr = tagged_read [ 'read' ] . reference_name start = tagged_read [ 'read' ] . get_tag ( 'XI' ) end = tagged_read [ 'read' ] . get_tag ( 'XI' ) + insert_offset strand = '+' if tagged_read [ 'read' ] . is_forward else '-' self . qbed [ chr ][ start ][ end ][ strand ][ srt ] += 1 self . status_dict [ status ] += 1","title":"update()"},{"location":"API/Alignment/mammals/Qbed/#callingcardstools.Alignment.mammals.Qbed.Qbed.write","text":"Translate the qbed object and status_dict to DataFrames and write to either a pickle or tsv file. Parameters: Name Type Description Default filename str The name of the file to write to. required raw bool If True, write to a raw file. Otherwise, write to a tsv file. False Returns: Type Description DataFrame [pd.DataFrame, pd.DataFrame]: The qbed and status DataFrames Source code in callingcardstools/Alignment/mammals/Qbed.py 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 def write ( self , filename : str , suffix : str = \"\" , raw : bool = False ) -> pd . DataFrame : \"\"\"Translate the qbed object and status_dict to DataFrames and write to either a pickle or tsv file. Args: filename (str): The name of the file to write to. raw (bool): If True, write to a raw file. Otherwise, write to a tsv file. Returns: [pd.DataFrame, pd.DataFrame]: The qbed and status DataFrames \"\"\" # create qbed DataFrame qbed_df = pd . DataFrame ( columns = self . qbed_fields ) concat_list = [] # List to hold the Series before concatenating single_srt_counter = 0 multi_srt_counter = 0 for chr , value1 in self . qbed . items (): for start , value2 in value1 . items (): for end , value3 in value2 . items (): for strand , value4 in value3 . items (): locus_srt_set = set () for srt_seq , count in value4 . items (): locus_srt_set . add ( srt_seq ) # Prepare a new series to add to qbed DataFrame new_row = pd . Series ([ chr , start , end , count , strand , srt_seq ], index = self . qbed_fields ) concat_list . append ( new_row ) # add a hop record to the qbed DataFrame # count single/multi srt as appropriate if len ( locus_srt_set ) > 1 : multi_srt_counter += 1 else : single_srt_counter += 1 qbed_df = pd . concat ([ qbed_df , pd . DataFrame ( concat_list , columns = self . qbed_fields )], ignore_index = True ) # create status DataFrame status_df = pd . DataFrame ( columns = [ 'status' , 'count' ]) concat_list = [] # List to hold the Series before concatenating for status , count in self . status_dict . items (): status_str = \",\" . join ( StatusFlags . decompose ( status )) # Prepare a new series to add to status DataFrame new_row = pd . Series ([ status_str , count ], index = [ 'status' , 'count' ]) concat_list . append ( new_row ) status_df = pd . concat ([ status_df , pd . DataFrame ( concat_list , columns = [ 'status' , 'count' ])], ignore_index = True ) # write to file if raw : pickle_output_file = filename + '_' + suffix + '_qbed.pkl' \\ if suffix else filename + '_qbed.pkl' logger . info ( \"writing Qbed object to %s \" , pickle_output_file ) with open ( pickle_output_file , 'wb' ) as file : pickle . dump ( self , file ) else : qbed_output_file = filename + '_' + suffix + '.qbed' \\ if suffix else filename + '.qbed' logger . info ( \"writing qbed to %s \" , qbed_output_file ) qbed_df . to_csv ( qbed_output_file , sep = ' \\t ' , index = False , header = False ) qc_output_file = filename + '_' + suffix + '_aln_summary.tsv' \\ if suffix else filename + '_aln_summary.tsv' logger . info ( \"writing qc summary to %s \" , qc_output_file ) status_df . to_csv ( qc_output_file , sep = ' \\t ' , index = False , header = False ) srt_output_file = filename + '_' + suffix + '_srt_count.tsv' \\ if suffix else filename + '_srt_count.tsv' logger . info ( \"writing srt summary to %s \" , srt_output_file ) self . _srt_writer ( srt_output_file , single_srt_counter , multi_srt_counter )","title":"write()"},{"location":"API/Analysis/yeast/chipexo_promoter_sig/chipexo_promoter_sig/","text":"Find the promoter signature of the chipexo data. This is calculated as the most significant peak in each promoter region. Parameters: Name Type Description Default chipexo_data_path str path to the chipexo allevents file. required chipexo_orig_chr_convention str chromosome convention of the chipexo allevents file. required promoter_data_path str path to the promoter data file. required promoter_orig_chr_convention str chromosome convention of the promoter data file. required chrmap_data_path str path to the chromosome map file. required unified_chr_convention str chromosome convention to convert to. required Returns: Type Description DataFrame pandas.DataFrame: A pandas DataFrame containing the promoter signature of the chipexo data. Example import pandas as pd import tempfile Create temporary chipexo data file \u00b6 with tempfile.NamedTemporaryFile(mode=\u2019w+\u2019, \u2026 suffix=\u2019.tsv\u2019) as chipexo_file: \u2026 _ = chipexo_file.write(\u2018chr\\tcoord\\tYPD_log2Fold\\t\u2019 \u2026 \u2018 YPD_log2P\\nchr1\\t150\\t2.0\\t0.05\\n\u2019) Create temporary promoter data file \u00b6 with tempfile.NamedTemporaryFile(mode=\u2019w+\u2019, \u2026 suffix=\u2019.tsv\u2019) as promoter_file: \u2026 _ = promoter_file.write(\u2018chr\\tstart\\tend\\t\u2019 \u2026 \u2018associated_feature\\nchr1\\t100\\t\u2019 \u2026 \u2018200\\tpromoter1\\n\u2019) Create temporary chromosome map file \u00b6 with tempfile.NamedTemporaryFile(mode=\u2019w+\u2019, \u2026 suffix=\u2019.tsv\u2019) as chrmap_file: \u2026 - = chrmap_file.write(\u2018chr\\tucsc\\nchr1\\tchr1\\n\u2019) Call the function \u00b6 result = chipexo_promoter_sig(chipexo_file.name, \u2018chr\u2019, \u2026 promoter_file.name, \u2018chr\u2019, \u2026 chrmap_file.name, \u2018ucsc\u2019) isinstance(result, pd.DataFrame) True Source code in callingcardstools/Analysis/yeast/chipexo_promoter_sig.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def chipexo_promoter_sig ( chipexo_data_path : str , chipexo_orig_chr_convention : str , promoter_data_path : str , promoter_orig_chr_convention : str , chrmap_data_path : str , unified_chr_convention : str ) -> pd . DataFrame : \"\"\" Find the promoter signature of the chipexo data. This is calculated as the most significant peak in each promoter region. Args: chipexo_data_path (str): path to the chipexo allevents file. chipexo_orig_chr_convention (str): chromosome convention of the chipexo allevents file. promoter_data_path (str): path to the promoter data file. promoter_orig_chr_convention (str): chromosome convention of the promoter data file. chrmap_data_path (str): path to the chromosome map file. unified_chr_convention (str): chromosome convention to convert to. Returns: pandas.DataFrame: A pandas DataFrame containing the promoter signature of the chipexo data. Example: >>> import pandas as pd >>> import tempfile >>> # Create temporary chipexo data file >>> with tempfile.NamedTemporaryFile(mode='w+', ... suffix='.tsv') as chipexo_file: ... _ = chipexo_file.write('chr\\\\tcoord\\\\tYPD_log2Fold\\\\t' ... ' YPD_log2P\\\\nchr1\\\\t150\\\\t2.0\\\\t0.05\\\\n') >>> # Create temporary promoter data file >>> with tempfile.NamedTemporaryFile(mode='w+', ... suffix='.tsv') as promoter_file: ... _ = promoter_file.write('chr\\\\tstart\\\\tend\\\\t' ... 'associated_feature\\\\nchr1\\\\t100\\\\t' ... '200\\\\tpromoter1\\\\n') >>> # Create temporary chromosome map file >>> with tempfile.NamedTemporaryFile(mode='w+', ... suffix='.tsv') as chrmap_file: ... - = chrmap_file.write('chr\\\\tucsc\\\\nchr1\\\\tchr1\\\\n') >>> # Call the function >>> result = chipexo_promoter_sig(chipexo_file.name, 'chr', ... promoter_file.name, 'chr', ... chrmap_file.name, 'ucsc') >>> isinstance(result, pd.DataFrame) True \"\"\" # read in chrmap data chrmap_df = read_in_chrmap ( chrmap_data_path , { chipexo_orig_chr_convention , promoter_orig_chr_convention , unified_chr_convention }) # read in promoter data promoter_df = read_in_promoter_data ( promoter_data_path , promoter_orig_chr_convention , unified_chr_convention , chrmap_df ) # read in chipexo data chipexo_df = read_in_chipexo_data ( chipexo_data_path , chipexo_orig_chr_convention , unified_chr_convention , chrmap_df ) # Step 1: Inner Join return pd . merge ( promoter_df , chipexo_df , on = 'chr' , how = 'inner' ) \\ . query ( 'start <= chipexo_start <= end' ) \\ . groupby ([ 'chr' , 'start' , 'end' , 'name' , 'strand' ]) \\ . agg ( n_sig_peaks = pd . NamedAgg ( column = 'chr' , aggfunc = 'count' ), max_fc = pd . NamedAgg ( column = 'YPD_log2Fold' , aggfunc = 'max' ), min_pval = pd . NamedAgg ( column = 'YPD_log2P' , aggfunc = 'min' )) \\ . reset_index ()","title":"chipexo_promoter_sig"},{"location":"API/Analysis/yeast/chipexo_promoter_sig/chipexo_promoter_sig/#callingcardstools.Analysis.yeast.chipexo_promoter_sig.chipexo_promoter_sig--create-temporary-chipexo-data-file","text":"with tempfile.NamedTemporaryFile(mode=\u2019w+\u2019, \u2026 suffix=\u2019.tsv\u2019) as chipexo_file: \u2026 _ = chipexo_file.write(\u2018chr\\tcoord\\tYPD_log2Fold\\t\u2019 \u2026 \u2018 YPD_log2P\\nchr1\\t150\\t2.0\\t0.05\\n\u2019)","title":"Create temporary chipexo data file"},{"location":"API/Analysis/yeast/chipexo_promoter_sig/chipexo_promoter_sig/#callingcardstools.Analysis.yeast.chipexo_promoter_sig.chipexo_promoter_sig--create-temporary-promoter-data-file","text":"with tempfile.NamedTemporaryFile(mode=\u2019w+\u2019, \u2026 suffix=\u2019.tsv\u2019) as promoter_file: \u2026 _ = promoter_file.write(\u2018chr\\tstart\\tend\\t\u2019 \u2026 \u2018associated_feature\\nchr1\\t100\\t\u2019 \u2026 \u2018200\\tpromoter1\\n\u2019)","title":"Create temporary promoter data file"},{"location":"API/Analysis/yeast/chipexo_promoter_sig/chipexo_promoter_sig/#callingcardstools.Analysis.yeast.chipexo_promoter_sig.chipexo_promoter_sig--create-temporary-chromosome-map-file","text":"with tempfile.NamedTemporaryFile(mode=\u2019w+\u2019, \u2026 suffix=\u2019.tsv\u2019) as chrmap_file: \u2026 - = chrmap_file.write(\u2018chr\\tucsc\\nchr1\\tchr1\\n\u2019)","title":"Create temporary chromosome map file"},{"location":"API/Analysis/yeast/chipexo_promoter_sig/chipexo_promoter_sig/#callingcardstools.Analysis.yeast.chipexo_promoter_sig.chipexo_promoter_sig--call-the-function","text":"result = chipexo_promoter_sig(chipexo_file.name, \u2018chr\u2019, \u2026 promoter_file.name, \u2018chr\u2019, \u2026 chrmap_file.name, \u2018ucsc\u2019) isinstance(result, pd.DataFrame) True Source code in callingcardstools/Analysis/yeast/chipexo_promoter_sig.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def chipexo_promoter_sig ( chipexo_data_path : str , chipexo_orig_chr_convention : str , promoter_data_path : str , promoter_orig_chr_convention : str , chrmap_data_path : str , unified_chr_convention : str ) -> pd . DataFrame : \"\"\" Find the promoter signature of the chipexo data. This is calculated as the most significant peak in each promoter region. Args: chipexo_data_path (str): path to the chipexo allevents file. chipexo_orig_chr_convention (str): chromosome convention of the chipexo allevents file. promoter_data_path (str): path to the promoter data file. promoter_orig_chr_convention (str): chromosome convention of the promoter data file. chrmap_data_path (str): path to the chromosome map file. unified_chr_convention (str): chromosome convention to convert to. Returns: pandas.DataFrame: A pandas DataFrame containing the promoter signature of the chipexo data. Example: >>> import pandas as pd >>> import tempfile >>> # Create temporary chipexo data file >>> with tempfile.NamedTemporaryFile(mode='w+', ... suffix='.tsv') as chipexo_file: ... _ = chipexo_file.write('chr\\\\tcoord\\\\tYPD_log2Fold\\\\t' ... ' YPD_log2P\\\\nchr1\\\\t150\\\\t2.0\\\\t0.05\\\\n') >>> # Create temporary promoter data file >>> with tempfile.NamedTemporaryFile(mode='w+', ... suffix='.tsv') as promoter_file: ... _ = promoter_file.write('chr\\\\tstart\\\\tend\\\\t' ... 'associated_feature\\\\nchr1\\\\t100\\\\t' ... '200\\\\tpromoter1\\\\n') >>> # Create temporary chromosome map file >>> with tempfile.NamedTemporaryFile(mode='w+', ... suffix='.tsv') as chrmap_file: ... - = chrmap_file.write('chr\\\\tucsc\\\\nchr1\\\\tchr1\\\\n') >>> # Call the function >>> result = chipexo_promoter_sig(chipexo_file.name, 'chr', ... promoter_file.name, 'chr', ... chrmap_file.name, 'ucsc') >>> isinstance(result, pd.DataFrame) True \"\"\" # read in chrmap data chrmap_df = read_in_chrmap ( chrmap_data_path , { chipexo_orig_chr_convention , promoter_orig_chr_convention , unified_chr_convention }) # read in promoter data promoter_df = read_in_promoter_data ( promoter_data_path , promoter_orig_chr_convention , unified_chr_convention , chrmap_df ) # read in chipexo data chipexo_df = read_in_chipexo_data ( chipexo_data_path , chipexo_orig_chr_convention , unified_chr_convention , chrmap_df ) # Step 1: Inner Join return pd . merge ( promoter_df , chipexo_df , on = 'chr' , how = 'inner' ) \\ . query ( 'start <= chipexo_start <= end' ) \\ . groupby ([ 'chr' , 'start' , 'end' , 'name' , 'strand' ]) \\ . agg ( n_sig_peaks = pd . NamedAgg ( column = 'chr' , aggfunc = 'count' ), max_fc = pd . NamedAgg ( column = 'YPD_log2Fold' , aggfunc = 'max' ), min_pval = pd . NamedAgg ( column = 'YPD_log2P' , aggfunc = 'min' )) \\ . reset_index ()","title":"Call the function"},{"location":"API/Analysis/yeast/chipexo_promoter_sig/read_in_chipexo_data/","text":"Read in the data from the chipexo file. This is data parsed from yeastepigenome.org. see yeastepigenome.org and https://github.com/cmatKhan/parsing_yeast_database_data Parameters: Name Type Description Default chipexo_allevents_data_path str path to the chipexo data required chipexo_orig_chr_convention str chromosome convention of the chipexo allevents file required unified_chr_convention str chromosome convention to convert to required Returns: Type Description DataFrame pandas.DataFrame: A pandas DataFrame containing the chipexo allevents data Raises: Type Description AttributeError If the chipexo table does not contain at least the following columns: chr , start , end , YPD_log2Fold , YPD_log2P . Note that the start column is the original coord column from the yeastepigenome.org data and end is simply coord + 1. It is in this format to make it somewhat easier to input to other processes that accept bed-like files. Source code in callingcardstools/Analysis/yeast/chipexo_promoter_sig.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def read_in_chipexo_data ( chipexo_data_path : str , curr_chr_convention : str , new_chr_convention : str , chrmap_df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Read in the data from the chipexo file. This is data parsed from yeastepigenome.org. see yeastepigenome.org and https://github.com/cmatKhan/parsing_yeast_database_data Args: chipexo_allevents_data_path (str): path to the chipexo data chipexo_orig_chr_convention (str): chromosome convention of the chipexo allevents file unified_chr_convention (str): chromosome convention to convert to Returns: pandas.DataFrame: A pandas DataFrame containing the chipexo allevents data Raises: AttributeError: If the chipexo table does not contain at least the following columns: `chr`, `start`, `end`, `YPD_log2Fold`, `YPD_log2P`. Note that the `start` column is the original `coord` column from the yeastepigenome.org data and `end` is simply `coord` + 1. It is in this format to make it somewhat easier to input to other processes that accept bed-like files. \"\"\" df = pd . read_csv ( chipexo_data_path , header = 0 , index_col = False ) if not { 'chr' , 'start' , 'end' , 'YPD_log2Fold' , 'YPD_log2P' } . issubset ( df . columns ): raise AttributeError ( 'The chipexo table must contain at least the ' 'following columns: `chr`, `start`, `end`, ' '`YPD_log2Fold`, `YPD_log2P`. Note that the ' '`start` column is the original `coord` column ' 'from the yeastepigenome.org data and `end` ' 'is simply `coord` + 1. It is in this format ' 'to make it somewhat easier to input to other ' 'processes that accept bed-like files.' ) df . rename ( columns = { 'start' : 'chipexo_start' , 'end' : 'chipexo_end' }, inplace = True ) return relabel_chr_column ( df , chrmap_df , curr_chr_convention , new_chr_convention )","title":"read_in_chipexo_data"},{"location":"API/Analysis/yeast/find_min_responsive/find_min_responsive/","text":"Finds the minimum number of responsive genes in a list of DataFrames. This function takes a list of DataFrames and finds the minimum number of responsive genes in any of the DataFrames. This is used to normalize the rank response across expression data sets. Parameters: Name Type Description Default data_path_list list A list of paths to expression dataframes required identifier_col_list list A list of column names for the feature identifier in each DataFrame required effect_col_list list A list of column names for the effect in each DataFrame. If there is no effect column in the dataframe at the same index, enter None required effect_thres_list list A list of effect thresholds in each DataFrame required pval_col_list list A list of column names for the p-value in each DataFrame. If no threshold is to be applied to the dataframe at the same index, enter None required pval_thres_list list A list of p-value thresholds in each DataFrame. If no threshold is to be applied to the dataframe at the same index, enter None required Returns: Name Type Description int int The minimum number of responsive genes in any of the DataFrames. Raises: Type Description TypeError if data_path_list, identifier_col_list, effect_col_list, or pval_col_list is not a list. Also raised if there is an error in the min() statement after tallying number of responsive genes in each dataframe. ValueError if the length of data_path_list, identifier_col_list, effect_col_list, or pval_col_list is not equal. Source code in callingcardstools/Analysis/yeast/find_min_responsive.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def find_min_responsive ( data_path_list : list , identifier_col_list : list , effect_col_list : list , effect_thres_list : list , pval_col_list : list , pval_thres_list : list , ) -> int : \"\"\" Finds the minimum number of responsive genes in a list of DataFrames. This function takes a list of DataFrames and finds the minimum number of responsive genes in any of the DataFrames. This is used to normalize the rank response across expression data sets. Args: data_path_list (list): A list of paths to expression dataframes identifier_col_list (list): A list of column names for the feature identifier in each DataFrame effect_col_list (list): A list of column names for the effect in each DataFrame. If there is no effect column in the dataframe at the same index, enter `None` effect_thres_list (list): A list of effect thresholds in each DataFrame pval_col_list (list): A list of column names for the p-value in each DataFrame. If no threshold is to be applied to the dataframe at the same index, enter `None` pval_thres_list (list): A list of p-value thresholds in each DataFrame. If no threshold is to be applied to the dataframe at the same index, enter `None` Returns: int: The minimum number of responsive genes in any of the DataFrames. Raises: TypeError: if data_path_list, identifier_col_list, effect_col_list, or pval_col_list is not a list. Also raised if there is an error in the `min()` statement after tallying number of responsive genes in each dataframe. ValueError: if the length of data_path_list, identifier_col_list, effect_col_list, or pval_col_list is not equal. \"\"\" if not isinstance ( data_path_list , list ): raise TypeError ( \"data_path_list must be a list\" ) if not isinstance ( identifier_col_list , list ): raise TypeError ( \"identifier_col_list must be a list\" ) if not isinstance ( effect_col_list , list ): raise TypeError ( \"effect_col_list must be a list\" ) if not isinstance ( pval_col_list , list ): raise TypeError ( \"pval_col_list must be a list\" ) if ( len ( data_path_list ) != len ( identifier_col_list ) != len ( data_path_list ) != len ( effect_col_list ) != len ( data_path_list ) != len ( pval_col_list ) != len ( effect_thres_list ) != len ( pval_thres_list ) ): raise ValueError ( \"Length of data_path_list, identifier_col_list, \" \"effect_col_list, and pval_col_list must be equal\" ) for data_path in data_path_list : if not os . path . exists ( data_path ): raise FileNotFoundError ( f \" { data_path } does not exist\" ) for identifier_col in identifier_col_list : if not isinstance ( identifier_col , str ): raise TypeError ( \"identifier_col_list must contain only strings\" ) for effect_col in effect_col_list : if not isinstance ( effect_col , ( str , type ( None ))): raise TypeError ( \"effect_col_list must contain only strings or \" \"`None`\" ) for pval_col in pval_col_list : if not isinstance ( pval_col , ( str , type ( None ))): raise TypeError ( \"pval_col_list must contain only strings or \" \"`None`\" ) for effect_thres in effect_thres_list : if not isinstance ( effect_thres , ( int , float , type ( None ))): raise TypeError ( \"effect_thres_list must contain only numbers or \" \"`None`\" ) for pval_thres in pval_thres_list : if not isinstance ( pval_thres , ( int , float , type ( None ))): raise TypeError ( \"pval_thres_list must contain only numbers or \" \"`None`\" ) df_list = [ label_responsive_genes ( read_in_data ( data_path , identifier_col , effect_col , pval_col , \"source\" , \"expression\" ), effect_thres , pval_thres , ) for data_path , identifier_col , effect_col , effect_thres , pval_col , pval_thres in zip ( data_path_list , identifier_col_list , effect_col_list , effect_thres_list , pval_col_list , pval_thres_list , ) ] try : min_responsive = min ([ df [ \"responsive\" ] . sum () for df in df_list ]) except TypeError as exc : logger . error ( \"Error in `find_min_responsive()`: %s \" , exc ) raise return min_responsive","title":"find_min_responsive"},{"location":"API/Analysis/yeast/rank_response/bin_by_binding_rank/","text":"Assigns a rank bin to each row in a DataFrame based on binding signal. This function divides the DataFrame into partitions based on the specified bin size, assigns a rank to each row within these partitions, and then sorts the DataFrame based on the \u2018effect\u2019 and \u2018binding_pvalue\u2019 columns. The ranking is assigned such that rows within each bin get the same rank, and the rank value is determined by the bin size. Parameters: Name Type Description Default df DataFrame The DataFrame to be ranked and sorted. It must contain \u2018effect\u2019 and \u2018binding_pvalue\u2019 columns. required bin_size int The size of each bin for partitioning the DataFrame for ranking. required rank_by_binding_effect bool If True, the DataFrame is sorted by abs(\u2018effect\u2019) in descending order first with ties broken by pvalue. If False, sort by pvalue first with ties broken by effect size. Defaults to False False Returns: Type Description pd.DataFrame: The input DataFrame with an added \u2018rank\u2019 column, sorted by \u2018effect\u2019 in descending order or \u2018binding_pvalue\u2019 in ascending order depending on rank_by_binding_effect . Example df = pd.DataFrame({\u2018effect\u2019: [1.2, 0.5, 0.8], \u2026 \u2018binding_pvalue\u2019: [5, 3, 4]}) bin_by_binding_rank(df, 2) Returns a DataFrame with added \u2018rank\u2019 column and sorted as per \u00b6 the specified criteria. \u00b6 Source code in callingcardstools/Analysis/yeast/rank_response.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def bin_by_binding_rank ( df : pd . DataFrame , bin_size : int , rank_by_binding_effect : bool = False ): \"\"\" Assigns a rank bin to each row in a DataFrame based on binding signal. This function divides the DataFrame into partitions based on the specified bin size, assigns a rank to each row within these partitions, and then sorts the DataFrame based on the 'effect' and 'binding_pvalue' columns. The ranking is assigned such that rows within each bin get the same rank, and the rank value is determined by the bin size. Args: df (pd.DataFrame): The DataFrame to be ranked and sorted. It must contain 'effect' and 'binding_pvalue' columns. bin_size (int): The size of each bin for partitioning the DataFrame for ranking. rank_by_binding_effect (bool, optional): If True, the DataFrame is sorted by abs('effect') in descending order first with ties broken by pvalue. If False, sort by pvalue first with ties broken by effect size. Defaults to False Returns: pd.DataFrame: The input DataFrame with an added 'rank' column, sorted by 'effect' in descending order or 'binding_pvalue' in ascending order depending on `rank_by_binding_effect`. Example: >>> df = pd.DataFrame({'effect': [1.2, 0.5, 0.8], ... 'binding_pvalue': [5, 3, 4]}) >>> bin_by_binding_rank(df, 2) # Returns a DataFrame with added 'rank' column and sorted as per # the specified criteria. \"\"\" if \"binding_pvalue\" not in df . columns : raise KeyError ( \"Column 'binding_pvalue' is not in the data\" ) if \"binding_effect\" not in df . columns : raise KeyError ( \"Column 'binding_effect' is not in the data\" ) parts = min ( len ( df ), bin_size ) df_abs = df . assign ( abs_binding_effect = df [ \"binding_effect\" ] . abs ()) df_sorted = df_abs . sort_values ( by = ( [ \"abs_binding_effect\" , \"binding_pvalue\" ] if rank_by_binding_effect else [ \"binding_pvalue\" , \"abs_binding_effect\" ] ), ascending = [ False , True ] if rank_by_binding_effect else [ True , False ], ) return ( df_sorted . drop ( columns = [ \"abs_binding_effect\" ]) . reset_index ( drop = True ) . assign ( rank_bin = create_partitions ( len ( df_sorted ), parts ) * parts ) )","title":"bin_by_binding_rank"},{"location":"API/Analysis/yeast/rank_response/bin_by_binding_rank/#callingcardstools.Analysis.yeast.rank_response.bin_by_binding_rank--returns-a-dataframe-with-added-rank-column-and-sorted-as-per","text":"","title":"Returns a DataFrame with added \n\n\n\n\n\n\n\n  \n  \n      Assigns a rank bin to each row in a DataFrame based on binding signal.\nThis function divides the DataFrame into partitions based on the specified\nbin size, assigns a rank to each row within these partitions, and then\nsorts the DataFrame based on the &lsquo;effect&rsquo; and &lsquo;binding_pvalue&rsquo; columns. The\nranking is assigned such that rows within each bin get the same rank, and\nthe rank value is determined by the bin size.\n\n\n\n  Parameters:\n  \n    \n      \n        Name\n        Type\n        Description\n        Default\n      \n    \n    \n        \n          df\n          \n                DataFrame\n          \n          \n            \n              The DataFrame to be ranked and sorted.\nIt must contain &lsquo;effect&rsquo; and &lsquo;binding_pvalue&rsquo; columns.\n            \n          \n          \n              required\n          \n        \n        \n          bin_size\n          \n                int\n          \n          \n            \n              The size of each bin for partitioning the DataFrame\nfor ranking.\n            \n          \n          \n              required\n          \n        \n        \n          rank_by_binding_effect\n          \n                bool\n          \n          \n            \n              If True, the DataFrame is sorted by\nabs(&lsquo;effect&rsquo;) in descending order first with ties broken by pvalue.\nIf False, sort by pvalue first with ties broken by effect size.\nDefaults to False\n            \n          \n          \n                False\n          \n        \n    \n  \n\n\n\n  Returns:\n  \n    \n      \n        Type\n        Description\n      \n    \n    \n        \n          \n          \n          \n            \n              pd.DataFrame: The input DataFrame with an added &lsquo;rank&rsquo; column, sorted\nby &lsquo;effect&rsquo; in descending order or &lsquo;binding_pvalue&rsquo; in\nascending order depending on rank_by_binding_effect.\n            \n          \n        \n    \n  \n\n\n  Example\n  \n\n\ndf = pd.DataFrame({&lsquo;effect&rsquo;: [1.2, 0.5, 0.8],\n&hellip;                    &lsquo;binding_pvalue&rsquo;: [5, 3, 4]})\nbin_by_binding_rank(df, 2)\n\n\n\nReturns a DataFrame with added &lsquo;rank&rsquo; column and sorted as per&para;\nthe specified criteria.&para;\n\n          \n            Source code in callingcardstools/Analysis/yeast/rank_response.py\n             70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125def bin_by_binding_rank(\n    df: pd.DataFrame, bin_size: int, rank_by_binding_effect: bool = False\n):\n    &quot;&quot;&quot;\n    Assigns a rank bin to each row in a DataFrame based on binding signal.\n\n    This function divides the DataFrame into partitions based on the specified\n    bin size, assigns a rank to each row within these partitions, and then\n    sorts the DataFrame based on the &#39;effect&#39; and &#39;binding_pvalue&#39; columns. The\n    ranking is assigned such that rows within each bin get the same rank, and\n    the rank value is determined by the bin size.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to be ranked and sorted.\n            It must contain &#39;effect&#39; and &#39;binding_pvalue&#39; columns.\n        bin_size (int): The size of each bin for partitioning the DataFrame\n            for ranking.\n        rank_by_binding_effect (bool, optional): If True, the DataFrame is sorted by\n            abs(&#39;effect&#39;) in descending order first with ties broken by pvalue.\n            If False, sort by pvalue first with ties broken by effect size.\n            Defaults to False\n\n    Returns:\n        pd.DataFrame: The input DataFrame with an added &#39;rank&#39; column, sorted\n            by &#39;effect&#39; in descending order or &#39;binding_pvalue&#39; in\n            ascending order depending on `rank_by_binding_effect`.\n\n    Example:\n        &gt;&gt;&gt; df = pd.DataFrame({&#39;effect&#39;: [1.2, 0.5, 0.8],\n        ...                    &#39;binding_pvalue&#39;: [5, 3, 4]})\n        &gt;&gt;&gt; bin_by_binding_rank(df, 2)\n        # Returns a DataFrame with added &#39;rank&#39; column and sorted as per\n        # the specified criteria.\n    &quot;&quot;&quot;\n    if &quot;binding_pvalue&quot; not in df.columns:\n        raise KeyError(&quot;Column &#39;binding_pvalue&#39; is not in the data&quot;)\n    if &quot;binding_effect&quot; not in df.columns:\n        raise KeyError(&quot;Column &#39;binding_effect&#39; is not in the data&quot;)\n\n    parts = min(len(df), bin_size)\n    df_abs = df.assign(abs_binding_effect=df[&quot;binding_effect&quot;].abs())\n\n    df_sorted = df_abs.sort_values(\n        by=(\n            [&quot;abs_binding_effect&quot;, &quot;binding_pvalue&quot;]\n            if rank_by_binding_effect\n            else [&quot;binding_pvalue&quot;, &quot;abs_binding_effect&quot;]\n        ),\n        ascending=[False, True] if rank_by_binding_effect else [True, False],\n    )\n\n    return (\n        df_sorted.drop(columns=[&quot;abs_binding_effect&quot;])\n        .reset_index(drop=True)\n        .assign(rank_bin=create_partitions(len(df_sorted), parts) * parts)\n    )\n\n          \n  \n\nrank\u0002wzxhzdk:1\u0003 column and sorted as per"},{"location":"API/Analysis/yeast/rank_response/bin_by_binding_rank/#callingcardstools.Analysis.yeast.rank_response.bin_by_binding_rank--the-specified-criteria","text":"Source code in callingcardstools/Analysis/yeast/rank_response.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def bin_by_binding_rank ( df : pd . DataFrame , bin_size : int , rank_by_binding_effect : bool = False ): \"\"\" Assigns a rank bin to each row in a DataFrame based on binding signal. This function divides the DataFrame into partitions based on the specified bin size, assigns a rank to each row within these partitions, and then sorts the DataFrame based on the 'effect' and 'binding_pvalue' columns. The ranking is assigned such that rows within each bin get the same rank, and the rank value is determined by the bin size. Args: df (pd.DataFrame): The DataFrame to be ranked and sorted. It must contain 'effect' and 'binding_pvalue' columns. bin_size (int): The size of each bin for partitioning the DataFrame for ranking. rank_by_binding_effect (bool, optional): If True, the DataFrame is sorted by abs('effect') in descending order first with ties broken by pvalue. If False, sort by pvalue first with ties broken by effect size. Defaults to False Returns: pd.DataFrame: The input DataFrame with an added 'rank' column, sorted by 'effect' in descending order or 'binding_pvalue' in ascending order depending on `rank_by_binding_effect`. Example: >>> df = pd.DataFrame({'effect': [1.2, 0.5, 0.8], ... 'binding_pvalue': [5, 3, 4]}) >>> bin_by_binding_rank(df, 2) # Returns a DataFrame with added 'rank' column and sorted as per # the specified criteria. \"\"\" if \"binding_pvalue\" not in df . columns : raise KeyError ( \"Column 'binding_pvalue' is not in the data\" ) if \"binding_effect\" not in df . columns : raise KeyError ( \"Column 'binding_effect' is not in the data\" ) parts = min ( len ( df ), bin_size ) df_abs = df . assign ( abs_binding_effect = df [ \"binding_effect\" ] . abs ()) df_sorted = df_abs . sort_values ( by = ( [ \"abs_binding_effect\" , \"binding_pvalue\" ] if rank_by_binding_effect else [ \"binding_pvalue\" , \"abs_binding_effect\" ] ), ascending = [ False , True ] if rank_by_binding_effect else [ True , False ], ) return ( df_sorted . drop ( columns = [ \"abs_binding_effect\" ]) . reset_index ( drop = True ) . assign ( rank_bin = create_partitions ( len ( df_sorted ), parts ) * parts ) )","title":"the specified criteria."},{"location":"API/Analysis/yeast/rank_response/compute_rank_response/","text":"Computes rank-based statistics and binomial test results for a DataFrame. This function groups the DataFrame by \u2018rank_bin\u2019 and aggregates it to calculate the number of responsive items in each rank bin, as well as various statistics related to a binomial test. It calculates the cumulative number of successes, response ratio, p-value, and confidence intervals for each rank bin. Parameters: Name Type Description Default df DataFrame DataFrame containing the columns \u2018rank_bin\u2019, \u2018responsive\u2019, and \u2018random\u2019. \u2018rank_bin\u2019 is an integer representing the rank bin, \u2018responsive\u2019 is a boolean indicating responsiveness, and \u2018random\u2019 is a float representing the random expectation. required Additional keyword arguments Additional keyword arguments are passed to the binomtest function, including arguments to the proportional_ci method of the BinomTestResults object (see scipy documentation for details) required Returns: Type Description DataFrame pd.DataFrame: A DataFrame indexed by \u2018rank_bin\u2019 with columns for the number of responsive items in each bin (\u2018n_responsive_in_rank\u2019), cumulative number of successes (\u2018n_successes\u2019), response ratio (\u2018response_ratio\u2019), p-value (\u2018p_value\u2019), and confidence interval bounds (\u2018ci_lower\u2019 and \u2018ci_upper\u2019). Example df = pd.DataFrame({\u2018rank_bin\u2019: [1, 1, 2], \u2026 \u2018responsive\u2019: [True, False, True], \u2026 \u2018random\u2019: [0.5, 0.5, 0.5]}) compute_rank_response(df) Returns a DataFrame with rank-based statistics and binomial \u00b6 test results. \u00b6 Source code in callingcardstools/Analysis/yeast/rank_response.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def compute_rank_response ( df : pd . DataFrame , ** kwargs ) -> pd . DataFrame : \"\"\" Computes rank-based statistics and binomial test results for a DataFrame. This function groups the DataFrame by 'rank_bin' and aggregates it to calculate the number of responsive items in each rank bin, as well as various statistics related to a binomial test. It calculates the cumulative number of successes, response ratio, p-value, and confidence intervals for each rank bin. Args: df (pd.DataFrame): DataFrame containing the columns 'rank_bin', 'responsive', and 'random'. 'rank_bin' is an integer representing the rank bin, 'responsive' is a boolean indicating responsiveness, and 'random' is a float representing the random expectation. Additional keyword arguments: Additional keyword arguments are passed to the binomtest function, including arguments to the proportional_ci method of the BinomTestResults object (see scipy documentation for details) Returns: pd.DataFrame: A DataFrame indexed by 'rank_bin' with columns for the number of responsive items in each bin ('n_responsive_in_rank'), cumulative number of successes ('n_successes'), response ratio ('response_ratio'), p-value ('p_value'), and confidence interval bounds ('ci_lower' and 'ci_upper'). Example: >>> df = pd.DataFrame({'rank_bin': [1, 1, 2], ... 'responsive': [True, False, True], ... 'random': [0.5, 0.5, 0.5]}) >>> compute_rank_response(df) # Returns a DataFrame with rank-based statistics and binomial # test results. \"\"\" rank_response_df = ( df . groupby ( \"rank_bin\" ) . agg ( n_responsive_in_rank = pd . NamedAgg ( column = \"responsive\" , aggfunc = \"sum\" ), random = pd . NamedAgg ( column = \"random\" , aggfunc = \"first\" ), ) . reset_index () ) rank_response_df [ \"n_successes\" ] = rank_response_df [ \"n_responsive_in_rank\" ] . cumsum () # Binomial Test and Confidence Interval rank_response_df [[ \"response_ratio\" , \"pvalue\" , \"ci_lower\" , \"ci_upper\" ]] = ( rank_response_df . apply ( lambda row : parse_binomtest_results ( binomtest ( int ( row [ \"n_successes\" ]), int ( row . rank_bin ), float ( row [ \"random\" ]), alternative = kwargs . get ( \"alternative\" , \"two-sided\" ), ), ** kwargs , ), axis = 1 , result_type = \"expand\" , ) ) return rank_response_df","title":"compute_rank_response"},{"location":"API/Analysis/yeast/rank_response/compute_rank_response/#callingcardstools.Analysis.yeast.rank_response.compute_rank_response--returns-a-dataframe-with-rank-based-statistics-and-binomial","text":"","title":"Returns a DataFrame with rank-based statistics and binomial"},{"location":"API/Analysis/yeast/rank_response/compute_rank_response/#callingcardstools.Analysis.yeast.rank_response.compute_rank_response--test-results","text":"Source code in callingcardstools/Analysis/yeast/rank_response.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def compute_rank_response ( df : pd . DataFrame , ** kwargs ) -> pd . DataFrame : \"\"\" Computes rank-based statistics and binomial test results for a DataFrame. This function groups the DataFrame by 'rank_bin' and aggregates it to calculate the number of responsive items in each rank bin, as well as various statistics related to a binomial test. It calculates the cumulative number of successes, response ratio, p-value, and confidence intervals for each rank bin. Args: df (pd.DataFrame): DataFrame containing the columns 'rank_bin', 'responsive', and 'random'. 'rank_bin' is an integer representing the rank bin, 'responsive' is a boolean indicating responsiveness, and 'random' is a float representing the random expectation. Additional keyword arguments: Additional keyword arguments are passed to the binomtest function, including arguments to the proportional_ci method of the BinomTestResults object (see scipy documentation for details) Returns: pd.DataFrame: A DataFrame indexed by 'rank_bin' with columns for the number of responsive items in each bin ('n_responsive_in_rank'), cumulative number of successes ('n_successes'), response ratio ('response_ratio'), p-value ('p_value'), and confidence interval bounds ('ci_lower' and 'ci_upper'). Example: >>> df = pd.DataFrame({'rank_bin': [1, 1, 2], ... 'responsive': [True, False, True], ... 'random': [0.5, 0.5, 0.5]}) >>> compute_rank_response(df) # Returns a DataFrame with rank-based statistics and binomial # test results. \"\"\" rank_response_df = ( df . groupby ( \"rank_bin\" ) . agg ( n_responsive_in_rank = pd . NamedAgg ( column = \"responsive\" , aggfunc = \"sum\" ), random = pd . NamedAgg ( column = \"random\" , aggfunc = \"first\" ), ) . reset_index () ) rank_response_df [ \"n_successes\" ] = rank_response_df [ \"n_responsive_in_rank\" ] . cumsum () # Binomial Test and Confidence Interval rank_response_df [[ \"response_ratio\" , \"pvalue\" , \"ci_lower\" , \"ci_upper\" ]] = ( rank_response_df . apply ( lambda row : parse_binomtest_results ( binomtest ( int ( row [ \"n_successes\" ]), int ( row . rank_bin ), float ( row [ \"random\" ]), alternative = kwargs . get ( \"alternative\" , \"two-sided\" ), ), ** kwargs , ), axis = 1 , result_type = \"expand\" , ) ) return rank_response_df","title":"test results."},{"location":"API/Analysis/yeast/rank_response/create_partitions/","text":"Splits a vector of a specified length into nearly equal partitions. This function creates a partition vector where each partition is of equal size, except the last partition which may be smaller depending on the vector length and the number of equal parts specified. Each element in the partition vector represents the partition number. Parameters: Name Type Description Default vector_length int The total length of the vector to be partitioned. required equal_parts int The number of equal parts to divide the vector. Defaults to 100. 100 Returns: Type Description numpy.ndarray: An array where each element represents the partition number for each element in the original vector. Examples: >>> create_partitions ( 10 , 3 ) array([1, 1, 1, 2, 2, 2, 3, 3, 3, 3]) Source code in callingcardstools/Analysis/yeast/rank_response.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def create_partitions ( vector_length , equal_parts = 100 ): \"\"\" Splits a vector of a specified length into nearly equal partitions. This function creates a partition vector where each partition is of equal size, except the last partition which may be smaller depending on the vector length and the number of equal parts specified. Each element in the partition vector represents the partition number. Args: vector_length (int): The total length of the vector to be partitioned. equal_parts (int, optional): The number of equal parts to divide the vector. Defaults to 100. Returns: numpy.ndarray: An array where each element represents the partition number for each element in the original vector. Examples: >>> create_partitions(10, 3) array([1, 1, 1, 2, 2, 2, 3, 3, 3, 3]) \"\"\" quotient , remainder = divmod ( vector_length , equal_parts ) return np . concatenate ( [ np . repeat ( np . arange ( 1 , quotient + 1 ), equal_parts ), np . repeat ( quotient + 1 , remainder ), ] )","title":"create_partitions"},{"location":"API/Analysis/yeast/rank_response/create_rank_response_table/","text":"Create a rank repsonse table from a dictionary which contains the configuration parameters. See docs at https://cmatkhan.github.io/callingCardsTools/file_format_specs/yeast_rank_response/ # noqa for details Parameters: Name Type Description Default config_dict dict A dictionary containing the configuration parameters required Returns: Name Type Description tuple tuple [ DataFrame , DataFrame , DataFrame ] A tuple containing three DataFrames (see rank_response_summarize): 1. A dataframe summarized where hte responsive_ratio is summarized by rank_bin 2. The merged, labelled and sorted dataframe with both binding and expression data 3. The random expectation dataframe Raises: Type Description KeyError if the configuration dictionary is missing any of the required keys FileExistsError if the data files do not exist AttributeError if there are NA values in the effect or pvalue columns ValueError if there are incomplete cases in the data Source code in callingcardstools/Analysis/yeast/rank_response.py 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 def create_rank_response_table ( config_dict : dict , ) -> tuple [ pd . DataFrame , pd . DataFrame , pd . DataFrame ]: \"\"\" Create a rank repsonse table from a dictionary which contains the configuration parameters. See docs at https://cmatkhan.github.io/callingCardsTools/file_format_specs/yeast_rank_response/ # noqa for details Args: config_dict (dict): A dictionary containing the configuration parameters Returns: tuple: A tuple containing three DataFrames (see rank_response_summarize): 1. A dataframe summarized where hte responsive_ratio is summarized by rank_bin 2. The merged, labelled and sorted dataframe with both binding and expression data 3. The random expectation dataframe Raises: KeyError: if the configuration dictionary is missing any of the required keys FileExistsError: if the data files do not exist AttributeError: if there are NA values in the effect or pvalue columns ValueError: if there are incomplete cases in the data \"\"\" # validate the configuration key/value pairs args = validate_config ( config_dict ) try : if len ( args [ \"binding_data_path\" ]) > 1 : binding_data = combine_data ( data_paths = args [ \"binding_data_path\" ], identifier_col = args [ \"binding_identifier_col\" ], effect_col = args [ \"binding_effect_col\" ], pval_col = args [ \"binding_pvalue_col\" ], source = args [ \"binding_source\" ], data_type = \"binding\" , ) else : binding_data = read_in_data ( data_path = args [ \"binding_data_path\" ][ 0 ], identifier_col = args [ \"binding_identifier_col\" ], effect_col = args [ \"binding_effect_col\" ], pval_col = args [ \"binding_pvalue_col\" ], source = args [ \"binding_source\" ], data_type = \"binding\" , ) except ( KeyError , FileExistsError , AttributeError ) as exc : logger . error ( \"Error reading in binding data: %s \" , exc ) raise try : if len ( args [ \"expression_data_path\" ]) > 1 : expression_data = combine_data ( data_paths = args [ \"expression_data_path\" ], identifier_col = args [ \"expression_identifier_col\" ], effect_col = args [ \"expression_effect_col\" ], pval_col = args [ \"expression_pvalue_col\" ], source = args [ \"expression_source\" ], data_type = \"expression\" , ) else : expression_data = read_in_data ( data_path = args [ \"expression_data_path\" ][ 0 ], identifier_col = args [ \"expression_identifier_col\" ], effect_col = args [ \"expression_effect_col\" ], pval_col = args [ \"expression_pvalue_col\" ], source = args [ \"expression_source\" ], data_type = \"expression\" , ) except ( KeyError , FileExistsError , AttributeError ) as exc : logger . error ( \"Error reading in expression data: %s \" , exc ) raise labeled_expression_data = label_responsive_genes ( expression_data , args [ \"expression_effect_thres\" ], args [ \"expression_pvalue_thres\" ], args [ \"normalization_cutoff\" ], ) # Calculate counts for responsive and unresponsive responsive_unresponsive_counts = labeled_expression_data [ \"responsive\" ] . value_counts () # Create the DataFrame random_expectation_df = pd . DataFrame ( { \"unresponsive\" : [ responsive_unresponsive_counts . get ( False , 0 )], \"responsive\" : [ responsive_unresponsive_counts . get ( True , 0 )], } ) # Calculate the 'random' column total_expression_genes = random_expectation_df . sum ( axis = 1 ) random_expectation_df [ \"random\" ] = ( random_expectation_df [ \"responsive\" ] / total_expression_genes ) df = labeled_expression_data . merge ( binding_data [[ \"binding_effect\" , \"binding_pvalue\" , \"binding_source\" , \"feature\" ]], how = \"inner\" , on = \"feature\" , ) # test that there no incomplete cases. raise an error if there are if df . isnull () . values . any (): raise ValueError ( \"There are incomplete cases in the data\" ) logger . debug ( \"There are %s genes in the data after merging \" \"the %s binding data and \" \" %s expression data\" , str ( df . shape [ 0 ]), args [ \"binding_source\" ], args [ \"expression_source\" ], ) df_expression_labeled_binding_ranked = bin_by_binding_rank ( df , args [ \"rank_bin_size\" ], args [ \"rank_by_binding_effect\" ] ) df_expression_labeled_binding_ranked [ \"random\" ] = random_expectation_df [ \"random\" ] . iloc [ 0 ] rank_response_df = compute_rank_response ( df_expression_labeled_binding_ranked ) return rank_response_df , df_expression_labeled_binding_ranked , random_expectation_df","title":"create_rank_response_table"},{"location":"API/Analysis/yeast/rank_response/label_responsive_genes/","text":"Labels genes in a DataFrame as responsive or not based on thresholds for expression effect and p-value. Note that the comparisons on the thresholds are strictly greater than for the abs_expression_effect_threshold and strictly less than for the expression_pvalue_threshold. The function adds a new boolean column \u2018responsive\u2019 to the DataFrame, where each gene is labeled as responsive if its absolute effect expression is strictly greater than a threshold and its p-value is strictly less than a specified threshold. If normalization is enabled, only the top genes meeting the criteria up to the minimum number found in the normalized subset are labeled as responsive. Parameters: Name Type Description Default df DataFrame DataFrame containing gene data. Must include \u2018expression_effect\u2019 and \u2018expression_pvalue\u2019 columns. required abs_expression_effect_threshold float Absolute value threshold for the absolute value of the expression effect. Values strictly greater than this threshold are considered responsive if the pvalue threshold passes. required expression_pvalue_threshold float Threshold for the expression p-value. Values strictly less than this threshold are considered responsive if the effect threshold passes. required normalization_cutoff int The maximum number of responsive genes to consider prior to labelling. This serves to normalize rank response across expression data sets. Defaults to -1, which disables normalization. -1 Returns: Type Description pd.DataFrame: The input DataFrame with an added \u2018responsive\u2019 column. Raises: Type Description KeyError If \u2018expression_effect\u2019 or \u2018expression_pvalue\u2019 are not in Examples: >>> df = pd . DataFrame ({ 'effect_expression' : [ 0.5 , 0.7 , 1.2 ], 'p_expression': [0.01, 0.05, 0.2]}) >>> label_responsive_genes ( df , 0.6 , 0.05 ) . responsive [False, True, False] Source code in callingcardstools/Analysis/yeast/rank_response.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 def label_responsive_genes ( df , abs_expression_effect_threshold , expression_pvalue_threshold , normalization_cutoff : int = - 1 , ): \"\"\" Labels genes in a DataFrame as responsive or not based on thresholds for expression effect and p-value. Note that the comparisons on the thresholds are strictly greater than for the abs_expression_effect_threshold and strictly less than for the expression_pvalue_threshold. The function adds a new boolean column 'responsive' to the DataFrame, where each gene is labeled as responsive if its absolute effect expression is strictly greater than a threshold and its p-value is strictly less than a specified threshold. If normalization is enabled, only the top genes meeting the criteria up to the minimum number found in the normalized subset are labeled as responsive. Args: df (pd.DataFrame): DataFrame containing gene data. Must include 'expression_effect' and 'expression_pvalue' columns. abs_expression_effect_threshold (float): Absolute value threshold for the absolute value of the expression effect. Values strictly greater than this threshold are considered responsive if the pvalue threshold passes. expression_pvalue_threshold (float): Threshold for the expression p-value. Values strictly less than this threshold are considered responsive if the effect threshold passes. normalization_cutoff (int, optional): The maximum number of responsive genes to consider prior to labelling. This serves to normalize rank response across expression data sets. Defaults to -1, which disables normalization. Returns: pd.DataFrame: The input DataFrame with an added 'responsive' column. Raises: KeyError: If 'expression_effect' or 'expression_pvalue' are not in the DataFrame. Examples: >>> df = pd.DataFrame({'effect_expression': [0.5, 0.7, 1.2], 'p_expression': [0.01, 0.05, 0.2]}) >>> label_responsive_genes(df, 0.6, 0.05).responsive [False, True, False] \"\"\" if \"expression_effect\" not in df . columns : raise KeyError ( \"Column 'effect_expression' is not in the data\" ) if \"expression_pvalue\" not in df . columns : raise KeyError ( \"Column 'effect_pvalue' is not in the data\" ) expression_effect_rank_cutoff = ( normalization_cutoff if normalization_cutoff > 0 else len ( df ) + 1 ) df_abs = df . assign ( abs_expression_effect = df [ \"expression_effect\" ] . abs ()) # if either the effect or p-value threshold is `None`, then set # the threshold to the appropiate boundary to prevent filtering on that # column abs_expression_effect_threshold = ( abs_expression_effect_threshold if abs_expression_effect_threshold is not None else min ( df_abs [ \"abs_expression_effect\" ]) - 1 ) expression_pvalue_threshold = ( expression_pvalue_threshold if expression_pvalue_threshold is not None else max ( df_abs [ \"expression_pvalue\" ]) + 1 ) df_ranked = ( df_abs . sort_values ( by = [ \"abs_expression_effect\" , \"expression_pvalue\" ], ascending = [ False , True ] ) . reset_index ( drop = True ) # Add 1 to start ranking from 1 instead of 0 . assign ( rank = lambda x : x . index + 1 ) ) df_ranked [ \"responsive\" ] = ( ( df_ranked [ \"abs_expression_effect\" ] > abs_expression_effect_threshold ) # noqa & ( df_ranked [ \"expression_pvalue\" ] < expression_pvalue_threshold ) & ( df_ranked [ \"rank\" ] <= expression_effect_rank_cutoff ) ) return df_ranked . drop ( columns = [ \"rank\" , \"abs_expression_effect\" ])","title":"label_responsive_genes"},{"location":"API/Analysis/yeast/rank_response/parse_binomtest_results/","text":"Parses the results of a binomtest into a tuple of floats. This function takes the results of a binomtest and returns a tuple of floats containing the response ratio, p-value, and confidence interval bounds. Parameters: Name Type Description Default binomtest_obj BinomTestResult The results of a binomtest for a single rank bin. required Additional keyword arguments Additional keyword arguments are passed to the proportional_ci method of the binomtest object. required Returns: Name Type Description tuple A tuple of floats containing the response ratio, p-value, and confidence interval bounds. Example parse_binomtest_results(binomtest(1, 2, 0.5, alternative=\u2019greater\u2019) (0.5, 0.75, 0.2, 0.8) Source code in callingcardstools/Analysis/yeast/rank_response.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def parse_binomtest_results ( binomtest_obj : BinomTestResult , ** kwargs ): \"\"\" Parses the results of a binomtest into a tuple of floats. This function takes the results of a binomtest and returns a tuple of floats containing the response ratio, p-value, and confidence interval bounds. Args: binomtest_obj (scipy.stats.BinomTestResult): The results of a binomtest for a single rank bin. Additional keyword arguments: Additional keyword arguments are passed to the proportional_ci method of the binomtest object. Returns: tuple: A tuple of floats containing the response ratio, p-value, and confidence interval bounds. Example: >>> parse_binomtest_results(binomtest(1, 2, 0.5, alternative='greater') (0.5, 0.75, 0.2, 0.8) \"\"\" return ( binomtest_obj . statistic , binomtest_obj . pvalue , binomtest_obj . proportion_ci ( confidence_level = kwargs . get ( \"confidence_level\" , 0.95 ), method = kwargs . get ( \"method\" , \"exact\" ), ) . low , binomtest_obj . proportion_ci ( confidence_level = kwargs . get ( \"confidence_level\" , 0.95 ), method = kwargs . get ( \"method\" , \"exact\" ), ) . high , )","title":"parse_binomtest_results"},{"location":"API/Analysis/yeast/rank_response/validate_config/","text":"Validate the yeast rank_response input configuration file. Parameters: Name Type Description Default config dict the configuration dictionary. required Returns: Name Type Description dict dict the validated configuration dictionary. Raises: Type Description KeyError if the configuration is invalid due to either a missing key or an invalid value. TypeError if the configuration is invalid due to an invalid type. FileNotFoundError if the configuration is invalid due to a missing Source code in callingcardstools/Analysis/yeast/rank_response.py 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 def validate_config ( config : dict ) -> dict : \"\"\" Validate the yeast rank_response input configuration file. Args: config (dict): the configuration dictionary. Returns: dict: the validated configuration dictionary. Raises: KeyError: if the configuration is invalid due to either a missing key or an invalid value. TypeError: if the configuration is invalid due to an invalid type. FileNotFoundError: if the configuration is invalid due to a missing \"\"\" # set default values if they are not in the config file config . setdefault ( \"rank_by_binding_effect\" , False ) config . setdefault ( \"rank_bin_size\" , 5 ) config . setdefault ( \"normalization_cutoff\" , - 1 ) # this is used to check if a column is set to 'none' and replace it # with None. currently set for the expression_{effect/pvalue}_{col/thres} try : if not isinstance ( config [ \"binding_data_path\" ], list ): raise TypeError ( \"binding_data_path must be a list of strings\" ) for path in config [ \"binding_data_path\" ]: if not os . path . exists ( path ): raise FileNotFoundError ( f \"Binding data file { path } does not exist\" ) except KeyError as exc : raise KeyError ( \"Missing key 'binding_data_path' in config\" ) from exc try : config [ \"binding_source\" ] = str ( config [ \"binding_source\" ]) except KeyError as exc : raise KeyError ( \"Missing key 'binding_source' in config\" ) from exc try : if not isinstance ( config [ \"binding_identifier_col\" ], str ): raise TypeError ( \"binding_identifier_col must be a string\" ) except KeyError as exc : raise KeyError ( \"Missing key 'binding_identifier_col' in config\" ) from exc try : if not isinstance ( config [ \"binding_effect_col\" ], str ): raise TypeError ( \"binding_effect_col must be a string\" ) except KeyError as exc : raise KeyError ( \"Missing key 'binding_effect_col' in config\" ) from exc try : if not isinstance ( config [ \"binding_pvalue_col\" ], str ): raise TypeError ( \"binding_pvalue_col must be a string\" ) except KeyError as exc : raise KeyError ( \"Missing key 'binding_pvalue_col' in config\" ) from exc try : if not isinstance ( config [ \"rank_by_binding_effect\" ], bool ): raise TypeError ( \"rank_by_binding_effect must be a boolean\" ) except KeyError as exc : raise KeyError ( \"Missing key 'rank_by_binding_effect' in config\" ) from exc try : if not isinstance ( config [ \"expression_data_path\" ], list ): raise TypeError ( \"expression_data_path must be a list of strings\" ) for path in config [ \"expression_data_path\" ]: if not os . path . exists ( path ): raise FileNotFoundError ( f \"Expression data file { path } does not exist\" ) except KeyError as exc : raise KeyError ( \"Missing key 'expression_data_path' in config\" ) from exc try : config [ \"expression_source\" ] = str ( config [ \"expression_source\" ]) except KeyError as exc : raise KeyError ( \"Missing key 'expression_source' in config\" ) from exc try : if not isinstance ( config [ \"expression_identifier_col\" ], str ): raise TypeError ( \"expression_identifier_col must be a string\" ) except KeyError as exc : raise KeyError ( \"Missing key 'expression_identifier_col' in config\" ) from exc try : if not isinstance ( config [ \"expression_effect_col\" ], ( str , type ( None ))): raise TypeError ( \"expression_effect_col must be a string or None\" ) except KeyError as exc : raise KeyError ( \"Missing key 'expression_effect_col' in config\" ) from exc try : if not isinstance ( config [ \"expression_effect_thres\" ], ( int , float , type ( None ))): raise TypeError ( \"expression_effect_thres must be numeric or None\" ) except KeyError as exc : raise KeyError ( \"Missing key 'expression_effect_thres' in config\" ) from exc for key in [ \"expression_effect_col\" , \"expression_effect_thres\" ]: config [ key ] = set_none_str_to_none ( config [ key ]) if ( config [ \"expression_effect_col\" ] is None and config [ \"expression_effect_thres\" ] is not None ) or ( config [ \"expression_effect_col\" ] is not None and config [ \"expression_effect_thres\" ] is None ): raise KeyError ( \"expression_effect_thres must be None if \" \"expression_effect_col is None\" ) try : if not isinstance ( config [ \"expression_pvalue_col\" ], ( str , type ( None ))): raise TypeError ( \"expression_pvalue_col must be a string or None\" ) except KeyError as exc : raise KeyError ( \"Missing key 'expression_pvalue_col' in config\" ) from exc try : if not isinstance ( config [ \"expression_pvalue_thres\" ], ( int , float , type ( None ))): raise TypeError ( \"expression_pvalue_thres must be numeric or None\" ) except KeyError as exc : raise KeyError ( \"Missing key 'expression_pvalue_thres' in config\" ) from exc for key in [ \"expression_pvalue_col\" , \"expression_pvalue_thres\" ]: config [ key ] = set_none_str_to_none ( config [ key ]) if ( config [ \"expression_pvalue_col\" ] is None and config [ \"expression_pvalue_thres\" ] is not None ) or ( config [ \"expression_pvalue_col\" ] is not None and config [ \"expression_pvalue_thres\" ] is None ): raise KeyError ( \"expression_pvalue_thres must be None if \" \"expression_pvalue_col is None\" ) if ( config [ \"expression_pvalue_col\" ] is None and config [ \"expression_effect_col\" ] is None ): raise KeyError ( \"expression_pvalue_col and expression_effect_col \" \"cannot both be None\" ) try : if not isinstance ( config [ \"rank_bin_size\" ], int ): raise TypeError ( \"rank_bin_size must be an integer\" ) except KeyError as exc : raise KeyError ( \"Missing key 'rank_bin_size' in config\" ) from exc try : if not isinstance ( config [ \"normalization_cutoff\" ], int ): raise TypeError ( \"`normalization_cutoff` must be an integer >= -1. \" + \"Default is -1, which disables normalization\" ) except KeyError as exc : raise KeyError ( \"Missing key 'normalization_cutoff' in config\" ) from exc return config","title":"validate_config"},{"location":"API/Analysis/yeast/read_in_data/combine_data/","text":"Read in multiple data files and combine the effect and pvalue columns using specified functions (defaults are additive mean for effect and log mean for pvalue). Parameters: Name Type Description Default data_paths List [ str ] List of data file paths required identifier_col str Name of the feature identifier column required effect_col str Name of the effect column required pval_col str Name of the pvalue column required source str Source of the data required data_type str Type of data, either \u2018binding\u2019 or \u2018expression\u2019 required combine_effect_func Callable Function to combine effect columns mean combine_pval_func Callable Function to combine pvalue columns combine_pvals_detect_logged Returns: Type Description DataFrame pd.DataFrame: Combined dataframe with averaged effect and pvalue Source code in callingcardstools/Analysis/yeast/read_in_data.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def combine_data ( data_paths : List [ str ], identifier_col : str , effect_col : str , pval_col : str , source : str , data_type : Literal [ \"binding\" , \"expression\" ], combine_effect_func : Callable [[ pd . Series ], float ] = np . mean , combine_pval_func : Callable [[ pd . Series ], float ] = combine_pvals_detect_logged , ) -> pd . DataFrame : \"\"\" Read in multiple data files and combine the effect and pvalue columns using specified functions (defaults are additive mean for effect and log mean for pvalue). Args: data_paths (List[str]): List of data file paths identifier_col (str): Name of the feature identifier column effect_col (str): Name of the effect column pval_col (str): Name of the pvalue column source (str): Source of the data data_type (str): Type of data, either 'binding' or 'expression' combine_effect_func (Callable): Function to combine effect columns combine_pval_func (Callable): Function to combine pvalue columns Returns: pd.DataFrame: Combined dataframe with averaged effect and pvalue \"\"\" logger . info ( \"combining data for data type {data_type} from {data_paths} \" ) all_dfs = [] for data_path in data_paths : df = read_in_data ( data_path = data_path , identifier_col = identifier_col , effect_col = effect_col , pval_col = pval_col , source = source , data_type = data_type , ) all_dfs . append ( df ) combined_df = ( pd . concat ( all_dfs ) . groupby ([ \"feature\" , f \" { data_type } _source\" ]) . agg ( { f \" { data_type } _effect\" : combine_effect_func , f \" { data_type } _pvalue\" : combine_pval_func , } ) . reset_index () ) return combined_df","title":"combine_data"},{"location":"API/Analysis/yeast/read_in_data/read_in_data/","text":"Read in data from a file and return a dataframe with the following columns: gene_id, {binding/expression}_effect, {binding/expression}_pvalue, source Parameters: Name Type Description Default data_path str path to the data file required identifier_col str name of the feature identifier column in the data required effect_col str name of the effect column in the data required pval_col str name of the pvalue column in the data required source str source of the data required data_type str type of data, either \u2018binding\u2019 or \u2018expression\u2019 required Returns: Type Description DataFrame pd.DataFrame: dataframe with the following columns: feature, {binding/expression}_effect, {binding/expression}_pvalue, source Raises: Type Description FileExistsError if data_path does not exist KeyError if identifier_col, effect_col, or pval_col is not in the data, or if the identifier_col is something other than feature and the column feature also exists in the data AttributeError if there are NA values in the effect or pvalue columns Source code in callingcardstools/Analysis/yeast/read_in_data.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def read_in_data ( data_path : str , identifier_col : str , effect_col : str , pval_col : str , source : str , data_type : Literal [ \"binding\" , \"expression\" ], ) -> pd . DataFrame : \"\"\" Read in data from a file and return a dataframe with the following columns: gene_id, {binding/expression}_effect, {binding/expression}_pvalue, source Args: data_path (str): path to the data file identifier_col (str): name of the feature identifier column in the data effect_col (str): name of the effect column in the data pval_col (str): name of the pvalue column in the data source (str): source of the data data_type (str): type of data, either 'binding' or 'expression' Returns: pd.DataFrame: dataframe with the following columns: feature, {binding/expression}_effect, {binding/expression}_pvalue, source Raises: FileExistsError: if data_path does not exist KeyError: if identifier_col, effect_col, or pval_col is not in the data, or if the `identifier_col` is something other than `feature` and the column `feature` also exists in the data AttributeError: if there are NA values in the effect or pvalue columns \"\"\" if not os . path . exists ( data_path ): raise FileExistsError ( f \" { data_path } does not exist\" ) compressed = data_path . endswith ( \".gz\" ) logger . debug ( \"data compressed: %s \" , compressed ) sep = ( \" \\t \" if any ( data_path . endswith ( ext ) for ext in [ \".tsv\" , \".txt\" , \".tsv.gz\" , \"txt.gz\" ]) else \",\" ) logger . debug ( \"data separator: %s \" , sep ) df = pd . read_csv ( data_path , sep = sep , compression = \"gzip\" if compressed else None ) if identifier_col not in df . columns : raise KeyError ( f \"Column ` { identifier_col } ` is not in { data_path } \" ) if \"feature\" in df . columns and identifier_col != \"feature\" : raise KeyError ( f \"Column `feature` exists in the data, but is not the \" f \"`identifier_col` { identifier_col } . Please rename the \" f \"current `feature` column to avoid confusion.\" ) try : effect_colname = data_type + \"_effect\" # Assuming df is your DataFrame and effect_col is a variable # indicating column name df [ effect_colname ] = df [ effect_col ] if effect_col else float ( \"inf\" ) # Check for NA values in the effect_colname if pd . isna ( df [ effect_colname ]) . any (): raise AttributeError ( f \"NA values found in column { effect_colname } . This must not be.\" ) except KeyError as exc : raise KeyError ( f \"Column { effect_col } is not `none` and does not exist in { data_path } \" ) from exc try : pval_colname = data_type + \"_pvalue\" df [ pval_colname ] = df [ pval_col ] if pval_col else 0.0 # Check for NA values in the pval_colname if pd . isna ( df [ pval_colname ]) . any (): raise AttributeError ( f \"NA values found in column { pval_colname } . This must not be.\" ) # Check for 'inf' values in the pval column if np . isinf ( df [ pval_colname ]) . any (): # Remove inf values and check the largest remaining value non_inf_pvals = df [ pval_colname ] . replace ([ np . inf , - np . inf ], np . nan ) . dropna () # if the largest remaining p-value is 0, and there exists at least 1 # negative value, then the pvalue column is logged. # convert positive infinity to negative infinity (this is for the raw # chipexo data) if ( not non_inf_pvals . empty and non_inf_pvals . max () <= 0 and non_inf_pvals . min () < 0 ): # Convert positive infinity (resulting from log(0)) to negative infinity logger . warning ( \"The pvalue column is logged, but there exist \" \"positive infinity values. This is assumed to be an error. \" \"Converting positive infinity to negative infinity.\" ) df [ pval_colname ] . replace ( np . inf , - np . inf , inplace = True ) except KeyError as exc : raise KeyError ( f \"Column { pval_col } is not `none` and does not exist in { data_path } \" ) from exc source_colname = data_type + \"_source\" df [ source_colname ] = source df . rename ( columns = { identifier_col : \"feature\" }, inplace = True ) return df [[ \"feature\" , effect_colname , pval_colname , source_colname ]]","title":"read_in_data"},{"location":"API/BarcodeParser/BarcodeParser/","text":"An object which parses a barcode string extracted from a read according to a barcode_details json file. Note that all sequences are cast to upper BarcodeParser \u00b6 Using a json which describes acceptable values for given barcodes, check the edit distance between the barcode components (substrings) and the Source code in callingcardstools/BarcodeParser/BarcodeParser.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 class BarcodeParser : \"\"\"Using a json which describes acceptable values for given barcodes, check the edit distance between the barcode components (substrings) and the \"\"\" # class properties -------------------------------------------------------- _key_dict = { \"components\" : \"components\" , \"insert_seq\" : \"insert_seq\" , \"match_allowance\" : \"match_allowance\" , } _barcode_dict = {} _barcode_details_json = \"\" _barcode = \"\" # TODO rename this: this is the maximum endpoint of an r1 component in the # barcode string of a given read _max_r1 = - 1 _restricted_bam_tags = { 'XS' , 'XI' , 'XE' , 'XZ' } # constructor ------------------------------------------------------------- def __init__ ( self , barcode_details_json : str ) -> None : \"\"\"BarcodeParser Constructor Args: barcode_details_json (str): Path to the barcode details json file \"\"\" # set the initialized barcode details # note that this also sets the match allowances self . barcode_details_json = barcode_details_json # property getter/setters ------------------------------------------------- @property def barcode_details_json ( self ): \"\"\"path to the barcode details json file\"\"\" return self . _barcode_details_json @barcode_details_json . setter def barcode_details_json ( self , new_barcode_details_json ): required_keys = { 'r1' , 'r2' , 'batch' } # check that barcode json exists if not os . path . exists ( new_barcode_details_json ): raise FileNotFoundError ( f \"Invalid Path: { new_barcode_details_json } \" ) # open json, read in as dict with open ( new_barcode_details_json , 'r' ) as f1 : # pylint:disable=W1514 barcode_dict = json . load ( f1 ) # check keys if not len ( required_keys - set ( barcode_dict )) == 0 : raise KeyError ( f 'the following keys are required: { required_keys } ' ) # update the barcode_dict attribute logger . info ( \"Updating the barcode dict to reflect \" \"the new barcode details json...\" ) self . barcode_dict = barcode_dict # if that works, update the barcode_details_json path logger . info ( \"Success! Setting the barcode details json path\" ) self . _barcode_details_json = new_barcode_details_json # and update some properties self . max_r1 = self . __get_max_r1 () self . max_mismatches = self . __calculate_max_mismatches () @property def key_dict ( self ): \"\"\"A dictionary which describes the defined fields of a valid barcode dict json file\"\"\" return self . _key_dict @property def barcode_dict ( self ): \"\"\"The barcode details json file verified and parsed into a python dictionary\"\"\" return self . _barcode_dict @barcode_dict . setter def barcode_dict ( self , new_barcode_dict ): # check that the indicies and components match # set barcode dict error_msg = 'Invalid barcode dict -- check the barcode_details json' try : self . __check_component_keys ( new_barcode_dict ) self . _barcode_dict = new_barcode_dict except KeyError as exc : raise KeyError ( error_msg ) from exc except ValueError as exc : raise ValueError ( error_msg ) from exc @property def components ( self ) -> set : \"\"\"get the components described in the components field of the barcode dict. Note that this does 'recurse' into the subdictionaries if necessary Returns: set: A set of components which make up a given read barcode (all separate, eg for the compound TF, this returns the components separately) \"\"\" # extract keys in the components attribute component_keys = set () for k , v in self . barcode_dict [ 'components' ] . items (): if v . get ( 'components' , None ): component_keys . update ({ x for x in v [ 'components' ]}) # else, add the key value else : component_keys . update ({ k }) return component_keys @property def insert_length ( self ): \"\"\"Extract the insertion sequence length from the barcode dictionary\"\"\" try : insert_seq_length = \\ len ( self . barcode_dict [ self . key_dict [ 'insert_seq' ]][ 0 ]) except ( KeyError , IndexError ): insert_seq_length = 1 return insert_seq_length @property def insert_seq ( self ): \"\"\"Getter for the insert seq sequence from the barcode details json. Returns upper case. Raises: AttributeError: Raised if the current barcode details json does not have an insert seq key \"\"\" if self . key_dict [ 'insert_seq' ] in self . barcode_dict : return self . barcode_dict [ self . key_dict [ 'insert_seq' ]] else : raise AttributeError ( f 'Current barcode details ' f ' { self . barcode_details_json } does not ' f 'have an insert seq component' ) @property def tagged_components ( self ): return { k : v [ 'bam_tag' ] for k , v in self . barcode_dict [ 'components' ] . items () if v . get ( 'bam_tag' , None )} @property def max_r1 ( self ): return self . _max_r1 @max_r1 . setter def max_r1 ( self , new_max_r1 ): \"\"\"getting for the maximum index on the r1 barcode strand\"\"\" if new_max_r1 < 0 : raise ValueError ( 'Max R1 is invalid. Check the barcode details json' ) self . _max_r1 = new_max_r1 @property def max_mismatches ( self ): \"\"\"maximum number of mismatches allowed in a barcode\"\"\" return self . _max_mismatches @max_mismatches . setter def max_mismatches ( self , new_max_mismatches ): self . _max_mismatches = new_max_mismatches @property def annotation_tags ( self ): \"\"\"iterate over the component dictionaries and extract the bam tags\"\"\" annotation_tag_list = [] for comp , comp_dict in self . barcode_dict [ 'components' ] . items (): if comp_dict . get ( 'annotation' , None ): if comp_dict . get ( 'bam_tag' , None ): annotation_tag_list . append ( comp_dict . get ( 'bam_tag' )) else : # TODO check this condition in barcodeparser constructor raise KeyError ( f 'Component { comp } has annotation set ' f 'to true, but no bam_tag. If annotation ' f 'is true, the bam_tag field must be set' ) return annotation_tag_list # private methods --------------------------------------------------------- def __check_bam_tags ( self ): \"\"\"check the tags set by the user in barcode_details.json against the list of restricted bam tags -- error if a restricted bam tag is used\"\"\" # TODO implement raise NotImplementedError def __get_max_r1 ( self ): max_r1 = - 1 for k , v in self . barcode_dict [ 'r1' ] . items (): if v [ 'index' ][ 1 ] > max_r1 : max_r1 = v [ 'index' ][ 1 ] if max_r1 == - 1 : raise ValueError ( 'Maximum index on the r1 barcode not found!' ) return max_r1 def __check_component_keys ( self , barcode_dict ) -> bool : \"\"\"_summary_ Args: barcode_dict (_type_): _description_ Raises: KeyError: if keys 'r1', 'r2' and 'components' DNE ValueError: _description_ Returns: bool: _description_ \"\"\" # extract keys in r1 and r2 and append appropriate read in as prefix read_keys = { 'r1_' + x for x in barcode_dict [ 'r1' ] . keys ()} \\ . union ({ 'r2_' + x for x in barcode_dict [ 'r2' ] . keys ()}) # check to make sure that the keys within r1 and r2 have unique names if len ( read_keys ) != len ( barcode_dict [ 'r1' ]) + len ( barcode_dict [ 'r2' ]): raise ValueError ( 'keys within r1 and r2 must have unique names' ) # extract keys in the components attribute component_keys = set () for k , v in barcode_dict [ 'components' ] . items (): if not isinstance ( v , dict ): ValueError ( 'Entries in components must be dictionaries' ) # Admittedly, this is confusing. but, a subkey of component may # also be components in the event that this given component is # compound, eg the yeast TF where the TF bc is made up of r1_primer # and r2_transposon. If this is the case, then extract those # items listed in the subkey 'components' if v . get ( 'components' , None ): component_keys . update ({ x for x in v [ 'components' ]}) # else, add the key value else : component_keys . update ({ k }) if len ( read_keys - component_keys ) != 0 : raise KeyError ( \"The components do not fully describe the \" + \"sequences extracted from the barcode\" ) # return true if no errors are raised return True def __calculate_max_mismatches ( self ) -> int : \"\"\"either extract or calculate the maximum number of mismatches allowed in a given read's barcode. If not provided, the max is the sum of the match_allowances for each component. These are 0 if not set in barcode zeroes. This allows the user to allow a number of mismatches in each component, but to set a value less that the sum of those allowances in the max_mismatch attribute to avoid a barcode with too many total mismatches. Raises: TypeError: if the extracted or calculated max_mismatch value is not an integer Returns: int: an integer describing the total number of allowable mismatches between a barcode and a set of barcode components. \"\"\" max_mismatch = self . barcode_dict . get ( 'max_mismatch' , None ) component_mismatch_sum = sum ([ self . barcode_dict [ 'components' ][ k ] . get ( 'match_allowance' , 0 ) for k in self . barcode_dict [ 'components' ] . keys ()]) if max_mismatch : if max_mismatch > component_mismatch_sum : logger . info ( 'max_mismatch in barcode_details: ' f ' { max_mismatch } which is greater than the sum ' f 'of component mismatch allowances: ' f ' { component_mismatch_sum } ' ) if not max_mismatch : max_mismatch = component_mismatch_sum if not isinstance ( max_mismatch , int ): raise TypeError ( 'max_mismatch must be an integer. ' 'check barcode_details.__get_max_mismatches()' ) return max_mismatch # public methods ---------------------------------------------------------- # TODO docstring def decompose_barcode ( self , barcode : str ): component_dict = {} for end in [ 'r1' , 'r2' ]: for component in self . barcode_dict [ end ] . keys (): # extract the start/end indicies of a given barcode component. # adjust if the indicies are from r2 under the assumption that # the sequence is created by appending the r1 seq to the r2 seq # note that max_r1 is the maximum r1 barcode endpoint in the # barcode sequence seq_start = self . barcode_dict [ end ][ component ][ 'index' ][ 0 ] \\ if end == 'r1' \\ else \\ self . barcode_dict [ end ][ component ][ 'index' ][ 0 ] + self . max_r1 seq_end = self . barcode_dict [ end ][ component ][ 'index' ][ 1 ] \\ if end == 'r1' \\ else \\ self . barcode_dict [ end ][ component ][ 'index' ][ 1 ] + self . max_r1 # get the component subsequence out of the barcode subseq = barcode [ seq_start : seq_end ] # add the component: best_match_dict key value pair to the # barcode_breakdown dict component_dict . setdefault ( \"_\" . join ([ end , component ]), subseq ) return self . component_check ( component_dict ) def component_check ( self , component_dict : dict ) -> dict : \"\"\"Determine if the barcode passes (True) or fails (False) given the edit distances between it and the expected components, and the allowable edit distance between a given component and the actual value. Args: component_dict (dict): a dictionary where the keys are component names and the values are the actual sequences, eg {'r1_primer': 'TGATA', 'r1_transposon': 'AATTCACTACGTCAACA', 'r2_transposon': 'ACCTGCTT', 'r2_restriction': 'TCGAGCGCCCGG'} Returns: dict: A dict of structure {\"pass\": Boolean, True if the barcode passes, \"tf\": Str, where the value is either \"*\" if unknown or a TF string from the barcode_details} \"\"\" component_check_dict = {} for k , v in self . barcode_dict [ self . key_dict [ 'components' ]] . items (): # if the value of a given component is a list, create a dict # from from the list where the keys and values are the same. target_dict = { x : x for x in v [ 'map' ]} \\ if isinstance ( v . get ( 'map' , None ), list ) \\ else v . get ( 'map' , None ) if not isinstance ( target_dict , dict ): raise ValueError ( 'Each component must have a map entry which ' 'is either a list or a dictionary' ) # if this is a compound component (eg the tf for yeast), # construct the sequence from the components. Else, extract the # sequence from the input component dict for the given key if v . get ( 'components' , None ): target_dict_offset = 0 # first, calculate the distances for the components # separately for component in v . get ( 'components' , None ): comp_split = component . split ( '_' ) query_seq = component_dict . get ( component , None ) start = target_dict_offset end = start + ( self . barcode_dict [ comp_split [ 0 ]] [ comp_split [ 1 ]] [ 'index' ][ 1 ] - self . barcode_dict [ comp_split [ 0 ]] [ comp_split [ 1 ]][ 'index' ][ 0 ]) target_dict_offset = end component_target_dict = {} for seq , id in target_dict . items (): component_target_dict . update ({ seq [ start : end ]: id }) component_check_dict [ component ] = \\ self . get_best_match ( query_seq , component_target_dict , v . get ( 'match_type' , 'edit_distance' )) # next, calculate the distances for the compound component # by concatenating the component sequences query_seq = '' . join ([ component_dict . get ( x , '' ) for x in v . get ( 'components' , None )]) component_check_dict [ k ] = \\ self . get_best_match ( query_seq , target_dict , v . get ( 'match_type' , 'edit_distance' )) else : query_seq = component_dict [ k ] component_check_dict [ k ] = \\ self . get_best_match ( query_seq , target_dict , v . get ( 'match_type' , 'edit_distance' )) if v . get ( 'bam_tag' , None ): component_check_dict [ k ][ 'bam_tag' ] = v . get ( 'bam_tag' ) # figure out if the barcode passes based on edit distance allowances passing = True total_mismatches = 0 for component in self . barcode_dict [ 'components' ] . keys (): component_metrics = component_check_dict [ component ] # if the total_mismatches exceed the maximum number of mismatches # in a given barcode, set the passing value to false and exit the # loop if total_mismatches > self . max_mismatches : passing = False logger . debug ( 'total_mismatches = {total_mismatches} ; ' + 'max_mismatches = {self.max_mismatches} ' ) break # else, for a given component, extract the mismatch tolerance match_allowance = \\ self . barcode_dict [ 'components' ][ component ] \\ . get ( 'match_allowance' , 0 ) # if the edit dist exceeds the match_allowance, set the barcode to # failing and break the loop if component_metrics [ 'dist' ] > match_allowance and \\ self . barcode_dict [ 'components' ] \\ . get ( component , {}) \\ . get ( 'require' , True ): passing = False break # if we reach this point, add the edit dist to the total_mismatch # note that if this is 0 it won't change anything, and move on to # the next component else : if self . barcode_dict [ 'components' ] \\ . get ( component , {}) \\ . get ( 'require' , True ): logger . debug ( f 'incrementing total mismatches ' f 'b/c of { component_metrics } ' ) total_mismatches = ( total_mismatches + component_metrics [ 'dist' ]) return { 'passing' : passing , 'details' : component_check_dict } @staticmethod def get_best_match ( query : str , component_dict : dict , match_type : Literal [ 'edit_distance' , 'greedy' ] = 'edit_distance' , seed : int = 42 ) -> dict : \"\"\"Given a match method, return a dictionary describing the best match between query and component_dict Args: query (str): the string to compare to the component dict values component_dict (dict): a dictionary where the keys are the sequences to compare to the query and the values are the names of the sequences match_type (str, optional): Either 'edit_distance' or 'greedy'. Defaults to 'edit_distance'. seed (int, optional): seed for random number generator. Defaults to 42. Returns: dict: A dictionary of structure {'name': either the sequence match, or the name if map is a named dictionary, 'dist': edit dist between query and best match -- always 0 or infinty if match is greedy depending on if exact match is found or not. If not, return is 'name': '*', 'dist': inf} \"\"\" if not isinstance ( seed , int ): logger . warning ( 'seed must be an integer. Setting to 42' ) seed = 42 random . seed ( seed ) # if no match found, these are the default values. Note that if # the match type is not recognized, this function errors component_name = \"*\" dist = infinity # if the match_type is edit_distance, then return a dict with the # keys query, with the query sequence, name, with the component # to which the query best matched, and edit_dist, with the edit # distance between query and the best match if match_type == 'edit_distance' : # iterate over the component dict and align strings. dictionary # is one where the key is the edit_distance and the value is a # list of elements which have that edit distance compared to the # query d = {} for k , v in component_dict . items (): # pylint:disable=C0103 d . setdefault ( align ( query , k )[ 'editDistance' ], []) . append ( v ) # if the minimum edit distance is 0, then the first and only # element in the list os the correct one. Else, select an element # with the minimum element distance at random, if there are more # than 1 possibility with an a given edit distance element_selector = 0 if min ( d ) == 0 \\ else random . randrange ( 0 , len ( d . get ( min ( d ))), 1 ) # set name and dist component_name = d [ min ( d )][ element_selector ] dist = min ( d ) # if the match type is greedy, return the first exact match. same # return structure as above, where the matched value is the key and # the edit distance is 0. If none are found, value is \"*\" and the # edit distance is infinity elif match_type == 'greedy' : for k , v in component_dict . items (): # set name and dist and break the for loop if k in query : component_name = v dist = 0 break else : raise IOError ( ' %s is not a recognized match_type argument' % match_type ) return { 'query' : query , 'name' : component_name , 'dist' : dist } annotation_tags property \u00b6 iterate over the component dictionaries and extract the bam tags barcode_details_json property writable \u00b6 path to the barcode details json file barcode_dict property writable \u00b6 The barcode details json file verified and parsed into a python dictionary components : set property \u00b6 get the components described in the components field of the barcode dict. Note that this does \u2018recurse\u2019 into the subdictionaries if necessary Returns: Name Type Description set set A set of components which make up a given read barcode (all set separate, eg for the compound TF, this returns the components set separately) insert_length property \u00b6 Extract the insertion sequence length from the barcode dictionary insert_seq property \u00b6 Getter for the insert seq sequence from the barcode details json. Returns upper case. Raises: Type Description AttributeError Raised if the current barcode details json does not have an insert seq key key_dict property \u00b6 A dictionary which describes the defined fields of a valid barcode dict json file max_mismatches property writable \u00b6 maximum number of mismatches allowed in a barcode __calculate_max_mismatches () \u00b6 either extract or calculate the maximum number of mismatches allowed in a given read\u2019s barcode. If not provided, the max is the sum of the match_allowances for each component. These are 0 if not set in barcode zeroes. This allows the user to allow a number of mismatches in each component, but to set a value less that the sum of those allowances in the max_mismatch attribute to avoid a barcode with too many total mismatches. Raises: Type Description TypeError if the extracted or calculated max_mismatch value is not an integer Returns: Name Type Description int int an integer describing the total number of allowable mismatches between a barcode and a set of barcode components. Source code in callingcardstools/BarcodeParser/BarcodeParser.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def __calculate_max_mismatches ( self ) -> int : \"\"\"either extract or calculate the maximum number of mismatches allowed in a given read's barcode. If not provided, the max is the sum of the match_allowances for each component. These are 0 if not set in barcode zeroes. This allows the user to allow a number of mismatches in each component, but to set a value less that the sum of those allowances in the max_mismatch attribute to avoid a barcode with too many total mismatches. Raises: TypeError: if the extracted or calculated max_mismatch value is not an integer Returns: int: an integer describing the total number of allowable mismatches between a barcode and a set of barcode components. \"\"\" max_mismatch = self . barcode_dict . get ( 'max_mismatch' , None ) component_mismatch_sum = sum ([ self . barcode_dict [ 'components' ][ k ] . get ( 'match_allowance' , 0 ) for k in self . barcode_dict [ 'components' ] . keys ()]) if max_mismatch : if max_mismatch > component_mismatch_sum : logger . info ( 'max_mismatch in barcode_details: ' f ' { max_mismatch } which is greater than the sum ' f 'of component mismatch allowances: ' f ' { component_mismatch_sum } ' ) if not max_mismatch : max_mismatch = component_mismatch_sum if not isinstance ( max_mismatch , int ): raise TypeError ( 'max_mismatch must be an integer. ' 'check barcode_details.__get_max_mismatches()' ) return max_mismatch __check_bam_tags () \u00b6 check the tags set by the user in barcode_details.json against the list of restricted bam tags \u2013 error if a restricted bam tag is used Source code in callingcardstools/BarcodeParser/BarcodeParser.py 198 199 200 201 202 203 def __check_bam_tags ( self ): \"\"\"check the tags set by the user in barcode_details.json against the list of restricted bam tags -- error if a restricted bam tag is used\"\"\" # TODO implement raise NotImplementedError __check_component_keys ( barcode_dict ) \u00b6 summary Parameters: Name Type Description Default barcode_dict _type_ description required Raises: Type Description KeyError if keys \u2018r1\u2019, \u2018r2\u2019 and \u2018components\u2019 DNE ValueError description Returns: Name Type Description bool bool description Source code in callingcardstools/BarcodeParser/BarcodeParser.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def __check_component_keys ( self , barcode_dict ) -> bool : \"\"\"_summary_ Args: barcode_dict (_type_): _description_ Raises: KeyError: if keys 'r1', 'r2' and 'components' DNE ValueError: _description_ Returns: bool: _description_ \"\"\" # extract keys in r1 and r2 and append appropriate read in as prefix read_keys = { 'r1_' + x for x in barcode_dict [ 'r1' ] . keys ()} \\ . union ({ 'r2_' + x for x in barcode_dict [ 'r2' ] . keys ()}) # check to make sure that the keys within r1 and r2 have unique names if len ( read_keys ) != len ( barcode_dict [ 'r1' ]) + len ( barcode_dict [ 'r2' ]): raise ValueError ( 'keys within r1 and r2 must have unique names' ) # extract keys in the components attribute component_keys = set () for k , v in barcode_dict [ 'components' ] . items (): if not isinstance ( v , dict ): ValueError ( 'Entries in components must be dictionaries' ) # Admittedly, this is confusing. but, a subkey of component may # also be components in the event that this given component is # compound, eg the yeast TF where the TF bc is made up of r1_primer # and r2_transposon. If this is the case, then extract those # items listed in the subkey 'components' if v . get ( 'components' , None ): component_keys . update ({ x for x in v [ 'components' ]}) # else, add the key value else : component_keys . update ({ k }) if len ( read_keys - component_keys ) != 0 : raise KeyError ( \"The components do not fully describe the \" + \"sequences extracted from the barcode\" ) # return true if no errors are raised return True __init__ ( barcode_details_json ) \u00b6 BarcodeParser Constructor Parameters: Name Type Description Default barcode_details_json str Path to the barcode details json file required Source code in callingcardstools/BarcodeParser/BarcodeParser.py 41 42 43 44 45 46 47 48 49 def __init__ ( self , barcode_details_json : str ) -> None : \"\"\"BarcodeParser Constructor Args: barcode_details_json (str): Path to the barcode details json file \"\"\" # set the initialized barcode details # note that this also sets the match allowances self . barcode_details_json = barcode_details_json component_check ( component_dict ) \u00b6 Determine if the barcode passes (True) or fails (False) given the edit distances between it and the expected components, and the allowable edit distance between a given component and the actual value. Parameters: Name Type Description Default component_dict dict a dictionary where the keys are component names and the values are the actual sequences, eg {\u2018r1_primer\u2019: \u2018TGATA\u2019, \u2018r1_transposon\u2019: \u2018AATTCACTACGTCAACA\u2019, \u2018r2_transposon\u2019: \u2018ACCTGCTT\u2019, \u2018r2_restriction\u2019: \u2018TCGAGCGCCCGG\u2019} required Returns: Name Type Description dict dict A dict of structure {\u201cpass\u201d: Boolean, True if the barcode passes, \u201ctf\u201d: Str, where the value is either \u201c*\u201d if unknown or a TF string from the barcode_details} Source code in callingcardstools/BarcodeParser/BarcodeParser.py 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 def component_check ( self , component_dict : dict ) -> dict : \"\"\"Determine if the barcode passes (True) or fails (False) given the edit distances between it and the expected components, and the allowable edit distance between a given component and the actual value. Args: component_dict (dict): a dictionary where the keys are component names and the values are the actual sequences, eg {'r1_primer': 'TGATA', 'r1_transposon': 'AATTCACTACGTCAACA', 'r2_transposon': 'ACCTGCTT', 'r2_restriction': 'TCGAGCGCCCGG'} Returns: dict: A dict of structure {\"pass\": Boolean, True if the barcode passes, \"tf\": Str, where the value is either \"*\" if unknown or a TF string from the barcode_details} \"\"\" component_check_dict = {} for k , v in self . barcode_dict [ self . key_dict [ 'components' ]] . items (): # if the value of a given component is a list, create a dict # from from the list where the keys and values are the same. target_dict = { x : x for x in v [ 'map' ]} \\ if isinstance ( v . get ( 'map' , None ), list ) \\ else v . get ( 'map' , None ) if not isinstance ( target_dict , dict ): raise ValueError ( 'Each component must have a map entry which ' 'is either a list or a dictionary' ) # if this is a compound component (eg the tf for yeast), # construct the sequence from the components. Else, extract the # sequence from the input component dict for the given key if v . get ( 'components' , None ): target_dict_offset = 0 # first, calculate the distances for the components # separately for component in v . get ( 'components' , None ): comp_split = component . split ( '_' ) query_seq = component_dict . get ( component , None ) start = target_dict_offset end = start + ( self . barcode_dict [ comp_split [ 0 ]] [ comp_split [ 1 ]] [ 'index' ][ 1 ] - self . barcode_dict [ comp_split [ 0 ]] [ comp_split [ 1 ]][ 'index' ][ 0 ]) target_dict_offset = end component_target_dict = {} for seq , id in target_dict . items (): component_target_dict . update ({ seq [ start : end ]: id }) component_check_dict [ component ] = \\ self . get_best_match ( query_seq , component_target_dict , v . get ( 'match_type' , 'edit_distance' )) # next, calculate the distances for the compound component # by concatenating the component sequences query_seq = '' . join ([ component_dict . get ( x , '' ) for x in v . get ( 'components' , None )]) component_check_dict [ k ] = \\ self . get_best_match ( query_seq , target_dict , v . get ( 'match_type' , 'edit_distance' )) else : query_seq = component_dict [ k ] component_check_dict [ k ] = \\ self . get_best_match ( query_seq , target_dict , v . get ( 'match_type' , 'edit_distance' )) if v . get ( 'bam_tag' , None ): component_check_dict [ k ][ 'bam_tag' ] = v . get ( 'bam_tag' ) # figure out if the barcode passes based on edit distance allowances passing = True total_mismatches = 0 for component in self . barcode_dict [ 'components' ] . keys (): component_metrics = component_check_dict [ component ] # if the total_mismatches exceed the maximum number of mismatches # in a given barcode, set the passing value to false and exit the # loop if total_mismatches > self . max_mismatches : passing = False logger . debug ( 'total_mismatches = {total_mismatches} ; ' + 'max_mismatches = {self.max_mismatches} ' ) break # else, for a given component, extract the mismatch tolerance match_allowance = \\ self . barcode_dict [ 'components' ][ component ] \\ . get ( 'match_allowance' , 0 ) # if the edit dist exceeds the match_allowance, set the barcode to # failing and break the loop if component_metrics [ 'dist' ] > match_allowance and \\ self . barcode_dict [ 'components' ] \\ . get ( component , {}) \\ . get ( 'require' , True ): passing = False break # if we reach this point, add the edit dist to the total_mismatch # note that if this is 0 it won't change anything, and move on to # the next component else : if self . barcode_dict [ 'components' ] \\ . get ( component , {}) \\ . get ( 'require' , True ): logger . debug ( f 'incrementing total mismatches ' f 'b/c of { component_metrics } ' ) total_mismatches = ( total_mismatches + component_metrics [ 'dist' ]) return { 'passing' : passing , 'details' : component_check_dict } get_best_match ( query , component_dict , match_type = 'edit_distance' , seed = 42 ) staticmethod \u00b6 Given a match method, return a dictionary describing the best match between query and component_dict Parameters: Name Type Description Default query str the string to compare to the component dict values required component_dict dict a dictionary where the keys are the sequences to compare to the query and the values are the names of the sequences required match_type str Either \u2018edit_distance\u2019 or \u2018greedy\u2019. Defaults to \u2018edit_distance\u2019. 'edit_distance' seed int seed for random number generator. Defaults to 42. 42 Returns: Name Type Description dict dict A dictionary of structure {\u2018name\u2019: either the sequence match, or the name if map is a named dictionary, \u2018dist\u2019: edit dist between query and best match \u2013 always 0 or infinty if match is greedy depending on if exact match is found or not. If not, return is \u2018name\u2019: \u2018*\u2019, \u2018dist\u2019: inf} Source code in callingcardstools/BarcodeParser/BarcodeParser.py 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 @staticmethod def get_best_match ( query : str , component_dict : dict , match_type : Literal [ 'edit_distance' , 'greedy' ] = 'edit_distance' , seed : int = 42 ) -> dict : \"\"\"Given a match method, return a dictionary describing the best match between query and component_dict Args: query (str): the string to compare to the component dict values component_dict (dict): a dictionary where the keys are the sequences to compare to the query and the values are the names of the sequences match_type (str, optional): Either 'edit_distance' or 'greedy'. Defaults to 'edit_distance'. seed (int, optional): seed for random number generator. Defaults to 42. Returns: dict: A dictionary of structure {'name': either the sequence match, or the name if map is a named dictionary, 'dist': edit dist between query and best match -- always 0 or infinty if match is greedy depending on if exact match is found or not. If not, return is 'name': '*', 'dist': inf} \"\"\" if not isinstance ( seed , int ): logger . warning ( 'seed must be an integer. Setting to 42' ) seed = 42 random . seed ( seed ) # if no match found, these are the default values. Note that if # the match type is not recognized, this function errors component_name = \"*\" dist = infinity # if the match_type is edit_distance, then return a dict with the # keys query, with the query sequence, name, with the component # to which the query best matched, and edit_dist, with the edit # distance between query and the best match if match_type == 'edit_distance' : # iterate over the component dict and align strings. dictionary # is one where the key is the edit_distance and the value is a # list of elements which have that edit distance compared to the # query d = {} for k , v in component_dict . items (): # pylint:disable=C0103 d . setdefault ( align ( query , k )[ 'editDistance' ], []) . append ( v ) # if the minimum edit distance is 0, then the first and only # element in the list os the correct one. Else, select an element # with the minimum element distance at random, if there are more # than 1 possibility with an a given edit distance element_selector = 0 if min ( d ) == 0 \\ else random . randrange ( 0 , len ( d . get ( min ( d ))), 1 ) # set name and dist component_name = d [ min ( d )][ element_selector ] dist = min ( d ) # if the match type is greedy, return the first exact match. same # return structure as above, where the matched value is the key and # the edit distance is 0. If none are found, value is \"*\" and the # edit distance is infinity elif match_type == 'greedy' : for k , v in component_dict . items (): # set name and dist and break the for loop if k in query : component_name = v dist = 0 break else : raise IOError ( ' %s is not a recognized match_type argument' % match_type ) return { 'query' : query , 'name' : component_name , 'dist' : dist }","title":"BarcodeParser"},{"location":"API/BarcodeParser/BarcodeParser/#callingcardstools.BarcodeParser.BarcodeParser.BarcodeParser","text":"Using a json which describes acceptable values for given barcodes, check the edit distance between the barcode components (substrings) and the Source code in callingcardstools/BarcodeParser/BarcodeParser.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 class BarcodeParser : \"\"\"Using a json which describes acceptable values for given barcodes, check the edit distance between the barcode components (substrings) and the \"\"\" # class properties -------------------------------------------------------- _key_dict = { \"components\" : \"components\" , \"insert_seq\" : \"insert_seq\" , \"match_allowance\" : \"match_allowance\" , } _barcode_dict = {} _barcode_details_json = \"\" _barcode = \"\" # TODO rename this: this is the maximum endpoint of an r1 component in the # barcode string of a given read _max_r1 = - 1 _restricted_bam_tags = { 'XS' , 'XI' , 'XE' , 'XZ' } # constructor ------------------------------------------------------------- def __init__ ( self , barcode_details_json : str ) -> None : \"\"\"BarcodeParser Constructor Args: barcode_details_json (str): Path to the barcode details json file \"\"\" # set the initialized barcode details # note that this also sets the match allowances self . barcode_details_json = barcode_details_json # property getter/setters ------------------------------------------------- @property def barcode_details_json ( self ): \"\"\"path to the barcode details json file\"\"\" return self . _barcode_details_json @barcode_details_json . setter def barcode_details_json ( self , new_barcode_details_json ): required_keys = { 'r1' , 'r2' , 'batch' } # check that barcode json exists if not os . path . exists ( new_barcode_details_json ): raise FileNotFoundError ( f \"Invalid Path: { new_barcode_details_json } \" ) # open json, read in as dict with open ( new_barcode_details_json , 'r' ) as f1 : # pylint:disable=W1514 barcode_dict = json . load ( f1 ) # check keys if not len ( required_keys - set ( barcode_dict )) == 0 : raise KeyError ( f 'the following keys are required: { required_keys } ' ) # update the barcode_dict attribute logger . info ( \"Updating the barcode dict to reflect \" \"the new barcode details json...\" ) self . barcode_dict = barcode_dict # if that works, update the barcode_details_json path logger . info ( \"Success! Setting the barcode details json path\" ) self . _barcode_details_json = new_barcode_details_json # and update some properties self . max_r1 = self . __get_max_r1 () self . max_mismatches = self . __calculate_max_mismatches () @property def key_dict ( self ): \"\"\"A dictionary which describes the defined fields of a valid barcode dict json file\"\"\" return self . _key_dict @property def barcode_dict ( self ): \"\"\"The barcode details json file verified and parsed into a python dictionary\"\"\" return self . _barcode_dict @barcode_dict . setter def barcode_dict ( self , new_barcode_dict ): # check that the indicies and components match # set barcode dict error_msg = 'Invalid barcode dict -- check the barcode_details json' try : self . __check_component_keys ( new_barcode_dict ) self . _barcode_dict = new_barcode_dict except KeyError as exc : raise KeyError ( error_msg ) from exc except ValueError as exc : raise ValueError ( error_msg ) from exc @property def components ( self ) -> set : \"\"\"get the components described in the components field of the barcode dict. Note that this does 'recurse' into the subdictionaries if necessary Returns: set: A set of components which make up a given read barcode (all separate, eg for the compound TF, this returns the components separately) \"\"\" # extract keys in the components attribute component_keys = set () for k , v in self . barcode_dict [ 'components' ] . items (): if v . get ( 'components' , None ): component_keys . update ({ x for x in v [ 'components' ]}) # else, add the key value else : component_keys . update ({ k }) return component_keys @property def insert_length ( self ): \"\"\"Extract the insertion sequence length from the barcode dictionary\"\"\" try : insert_seq_length = \\ len ( self . barcode_dict [ self . key_dict [ 'insert_seq' ]][ 0 ]) except ( KeyError , IndexError ): insert_seq_length = 1 return insert_seq_length @property def insert_seq ( self ): \"\"\"Getter for the insert seq sequence from the barcode details json. Returns upper case. Raises: AttributeError: Raised if the current barcode details json does not have an insert seq key \"\"\" if self . key_dict [ 'insert_seq' ] in self . barcode_dict : return self . barcode_dict [ self . key_dict [ 'insert_seq' ]] else : raise AttributeError ( f 'Current barcode details ' f ' { self . barcode_details_json } does not ' f 'have an insert seq component' ) @property def tagged_components ( self ): return { k : v [ 'bam_tag' ] for k , v in self . barcode_dict [ 'components' ] . items () if v . get ( 'bam_tag' , None )} @property def max_r1 ( self ): return self . _max_r1 @max_r1 . setter def max_r1 ( self , new_max_r1 ): \"\"\"getting for the maximum index on the r1 barcode strand\"\"\" if new_max_r1 < 0 : raise ValueError ( 'Max R1 is invalid. Check the barcode details json' ) self . _max_r1 = new_max_r1 @property def max_mismatches ( self ): \"\"\"maximum number of mismatches allowed in a barcode\"\"\" return self . _max_mismatches @max_mismatches . setter def max_mismatches ( self , new_max_mismatches ): self . _max_mismatches = new_max_mismatches @property def annotation_tags ( self ): \"\"\"iterate over the component dictionaries and extract the bam tags\"\"\" annotation_tag_list = [] for comp , comp_dict in self . barcode_dict [ 'components' ] . items (): if comp_dict . get ( 'annotation' , None ): if comp_dict . get ( 'bam_tag' , None ): annotation_tag_list . append ( comp_dict . get ( 'bam_tag' )) else : # TODO check this condition in barcodeparser constructor raise KeyError ( f 'Component { comp } has annotation set ' f 'to true, but no bam_tag. If annotation ' f 'is true, the bam_tag field must be set' ) return annotation_tag_list # private methods --------------------------------------------------------- def __check_bam_tags ( self ): \"\"\"check the tags set by the user in barcode_details.json against the list of restricted bam tags -- error if a restricted bam tag is used\"\"\" # TODO implement raise NotImplementedError def __get_max_r1 ( self ): max_r1 = - 1 for k , v in self . barcode_dict [ 'r1' ] . items (): if v [ 'index' ][ 1 ] > max_r1 : max_r1 = v [ 'index' ][ 1 ] if max_r1 == - 1 : raise ValueError ( 'Maximum index on the r1 barcode not found!' ) return max_r1 def __check_component_keys ( self , barcode_dict ) -> bool : \"\"\"_summary_ Args: barcode_dict (_type_): _description_ Raises: KeyError: if keys 'r1', 'r2' and 'components' DNE ValueError: _description_ Returns: bool: _description_ \"\"\" # extract keys in r1 and r2 and append appropriate read in as prefix read_keys = { 'r1_' + x for x in barcode_dict [ 'r1' ] . keys ()} \\ . union ({ 'r2_' + x for x in barcode_dict [ 'r2' ] . keys ()}) # check to make sure that the keys within r1 and r2 have unique names if len ( read_keys ) != len ( barcode_dict [ 'r1' ]) + len ( barcode_dict [ 'r2' ]): raise ValueError ( 'keys within r1 and r2 must have unique names' ) # extract keys in the components attribute component_keys = set () for k , v in barcode_dict [ 'components' ] . items (): if not isinstance ( v , dict ): ValueError ( 'Entries in components must be dictionaries' ) # Admittedly, this is confusing. but, a subkey of component may # also be components in the event that this given component is # compound, eg the yeast TF where the TF bc is made up of r1_primer # and r2_transposon. If this is the case, then extract those # items listed in the subkey 'components' if v . get ( 'components' , None ): component_keys . update ({ x for x in v [ 'components' ]}) # else, add the key value else : component_keys . update ({ k }) if len ( read_keys - component_keys ) != 0 : raise KeyError ( \"The components do not fully describe the \" + \"sequences extracted from the barcode\" ) # return true if no errors are raised return True def __calculate_max_mismatches ( self ) -> int : \"\"\"either extract or calculate the maximum number of mismatches allowed in a given read's barcode. If not provided, the max is the sum of the match_allowances for each component. These are 0 if not set in barcode zeroes. This allows the user to allow a number of mismatches in each component, but to set a value less that the sum of those allowances in the max_mismatch attribute to avoid a barcode with too many total mismatches. Raises: TypeError: if the extracted or calculated max_mismatch value is not an integer Returns: int: an integer describing the total number of allowable mismatches between a barcode and a set of barcode components. \"\"\" max_mismatch = self . barcode_dict . get ( 'max_mismatch' , None ) component_mismatch_sum = sum ([ self . barcode_dict [ 'components' ][ k ] . get ( 'match_allowance' , 0 ) for k in self . barcode_dict [ 'components' ] . keys ()]) if max_mismatch : if max_mismatch > component_mismatch_sum : logger . info ( 'max_mismatch in barcode_details: ' f ' { max_mismatch } which is greater than the sum ' f 'of component mismatch allowances: ' f ' { component_mismatch_sum } ' ) if not max_mismatch : max_mismatch = component_mismatch_sum if not isinstance ( max_mismatch , int ): raise TypeError ( 'max_mismatch must be an integer. ' 'check barcode_details.__get_max_mismatches()' ) return max_mismatch # public methods ---------------------------------------------------------- # TODO docstring def decompose_barcode ( self , barcode : str ): component_dict = {} for end in [ 'r1' , 'r2' ]: for component in self . barcode_dict [ end ] . keys (): # extract the start/end indicies of a given barcode component. # adjust if the indicies are from r2 under the assumption that # the sequence is created by appending the r1 seq to the r2 seq # note that max_r1 is the maximum r1 barcode endpoint in the # barcode sequence seq_start = self . barcode_dict [ end ][ component ][ 'index' ][ 0 ] \\ if end == 'r1' \\ else \\ self . barcode_dict [ end ][ component ][ 'index' ][ 0 ] + self . max_r1 seq_end = self . barcode_dict [ end ][ component ][ 'index' ][ 1 ] \\ if end == 'r1' \\ else \\ self . barcode_dict [ end ][ component ][ 'index' ][ 1 ] + self . max_r1 # get the component subsequence out of the barcode subseq = barcode [ seq_start : seq_end ] # add the component: best_match_dict key value pair to the # barcode_breakdown dict component_dict . setdefault ( \"_\" . join ([ end , component ]), subseq ) return self . component_check ( component_dict ) def component_check ( self , component_dict : dict ) -> dict : \"\"\"Determine if the barcode passes (True) or fails (False) given the edit distances between it and the expected components, and the allowable edit distance between a given component and the actual value. Args: component_dict (dict): a dictionary where the keys are component names and the values are the actual sequences, eg {'r1_primer': 'TGATA', 'r1_transposon': 'AATTCACTACGTCAACA', 'r2_transposon': 'ACCTGCTT', 'r2_restriction': 'TCGAGCGCCCGG'} Returns: dict: A dict of structure {\"pass\": Boolean, True if the barcode passes, \"tf\": Str, where the value is either \"*\" if unknown or a TF string from the barcode_details} \"\"\" component_check_dict = {} for k , v in self . barcode_dict [ self . key_dict [ 'components' ]] . items (): # if the value of a given component is a list, create a dict # from from the list where the keys and values are the same. target_dict = { x : x for x in v [ 'map' ]} \\ if isinstance ( v . get ( 'map' , None ), list ) \\ else v . get ( 'map' , None ) if not isinstance ( target_dict , dict ): raise ValueError ( 'Each component must have a map entry which ' 'is either a list or a dictionary' ) # if this is a compound component (eg the tf for yeast), # construct the sequence from the components. Else, extract the # sequence from the input component dict for the given key if v . get ( 'components' , None ): target_dict_offset = 0 # first, calculate the distances for the components # separately for component in v . get ( 'components' , None ): comp_split = component . split ( '_' ) query_seq = component_dict . get ( component , None ) start = target_dict_offset end = start + ( self . barcode_dict [ comp_split [ 0 ]] [ comp_split [ 1 ]] [ 'index' ][ 1 ] - self . barcode_dict [ comp_split [ 0 ]] [ comp_split [ 1 ]][ 'index' ][ 0 ]) target_dict_offset = end component_target_dict = {} for seq , id in target_dict . items (): component_target_dict . update ({ seq [ start : end ]: id }) component_check_dict [ component ] = \\ self . get_best_match ( query_seq , component_target_dict , v . get ( 'match_type' , 'edit_distance' )) # next, calculate the distances for the compound component # by concatenating the component sequences query_seq = '' . join ([ component_dict . get ( x , '' ) for x in v . get ( 'components' , None )]) component_check_dict [ k ] = \\ self . get_best_match ( query_seq , target_dict , v . get ( 'match_type' , 'edit_distance' )) else : query_seq = component_dict [ k ] component_check_dict [ k ] = \\ self . get_best_match ( query_seq , target_dict , v . get ( 'match_type' , 'edit_distance' )) if v . get ( 'bam_tag' , None ): component_check_dict [ k ][ 'bam_tag' ] = v . get ( 'bam_tag' ) # figure out if the barcode passes based on edit distance allowances passing = True total_mismatches = 0 for component in self . barcode_dict [ 'components' ] . keys (): component_metrics = component_check_dict [ component ] # if the total_mismatches exceed the maximum number of mismatches # in a given barcode, set the passing value to false and exit the # loop if total_mismatches > self . max_mismatches : passing = False logger . debug ( 'total_mismatches = {total_mismatches} ; ' + 'max_mismatches = {self.max_mismatches} ' ) break # else, for a given component, extract the mismatch tolerance match_allowance = \\ self . barcode_dict [ 'components' ][ component ] \\ . get ( 'match_allowance' , 0 ) # if the edit dist exceeds the match_allowance, set the barcode to # failing and break the loop if component_metrics [ 'dist' ] > match_allowance and \\ self . barcode_dict [ 'components' ] \\ . get ( component , {}) \\ . get ( 'require' , True ): passing = False break # if we reach this point, add the edit dist to the total_mismatch # note that if this is 0 it won't change anything, and move on to # the next component else : if self . barcode_dict [ 'components' ] \\ . get ( component , {}) \\ . get ( 'require' , True ): logger . debug ( f 'incrementing total mismatches ' f 'b/c of { component_metrics } ' ) total_mismatches = ( total_mismatches + component_metrics [ 'dist' ]) return { 'passing' : passing , 'details' : component_check_dict } @staticmethod def get_best_match ( query : str , component_dict : dict , match_type : Literal [ 'edit_distance' , 'greedy' ] = 'edit_distance' , seed : int = 42 ) -> dict : \"\"\"Given a match method, return a dictionary describing the best match between query and component_dict Args: query (str): the string to compare to the component dict values component_dict (dict): a dictionary where the keys are the sequences to compare to the query and the values are the names of the sequences match_type (str, optional): Either 'edit_distance' or 'greedy'. Defaults to 'edit_distance'. seed (int, optional): seed for random number generator. Defaults to 42. Returns: dict: A dictionary of structure {'name': either the sequence match, or the name if map is a named dictionary, 'dist': edit dist between query and best match -- always 0 or infinty if match is greedy depending on if exact match is found or not. If not, return is 'name': '*', 'dist': inf} \"\"\" if not isinstance ( seed , int ): logger . warning ( 'seed must be an integer. Setting to 42' ) seed = 42 random . seed ( seed ) # if no match found, these are the default values. Note that if # the match type is not recognized, this function errors component_name = \"*\" dist = infinity # if the match_type is edit_distance, then return a dict with the # keys query, with the query sequence, name, with the component # to which the query best matched, and edit_dist, with the edit # distance between query and the best match if match_type == 'edit_distance' : # iterate over the component dict and align strings. dictionary # is one where the key is the edit_distance and the value is a # list of elements which have that edit distance compared to the # query d = {} for k , v in component_dict . items (): # pylint:disable=C0103 d . setdefault ( align ( query , k )[ 'editDistance' ], []) . append ( v ) # if the minimum edit distance is 0, then the first and only # element in the list os the correct one. Else, select an element # with the minimum element distance at random, if there are more # than 1 possibility with an a given edit distance element_selector = 0 if min ( d ) == 0 \\ else random . randrange ( 0 , len ( d . get ( min ( d ))), 1 ) # set name and dist component_name = d [ min ( d )][ element_selector ] dist = min ( d ) # if the match type is greedy, return the first exact match. same # return structure as above, where the matched value is the key and # the edit distance is 0. If none are found, value is \"*\" and the # edit distance is infinity elif match_type == 'greedy' : for k , v in component_dict . items (): # set name and dist and break the for loop if k in query : component_name = v dist = 0 break else : raise IOError ( ' %s is not a recognized match_type argument' % match_type ) return { 'query' : query , 'name' : component_name , 'dist' : dist }","title":"BarcodeParser"},{"location":"API/BarcodeParser/BarcodeParser/#callingcardstools.BarcodeParser.BarcodeParser.BarcodeParser.annotation_tags","text":"iterate over the component dictionaries and extract the bam tags","title":"annotation_tags"},{"location":"API/BarcodeParser/BarcodeParser/#callingcardstools.BarcodeParser.BarcodeParser.BarcodeParser.barcode_details_json","text":"path to the barcode details json file","title":"barcode_details_json"},{"location":"API/BarcodeParser/BarcodeParser/#callingcardstools.BarcodeParser.BarcodeParser.BarcodeParser.barcode_dict","text":"The barcode details json file verified and parsed into a python dictionary","title":"barcode_dict"},{"location":"API/BarcodeParser/BarcodeParser/#callingcardstools.BarcodeParser.BarcodeParser.BarcodeParser.components","text":"get the components described in the components field of the barcode dict. Note that this does \u2018recurse\u2019 into the subdictionaries if necessary Returns: Name Type Description set set A set of components which make up a given read barcode (all set separate, eg for the compound TF, this returns the components set separately)","title":"components"},{"location":"API/BarcodeParser/BarcodeParser/#callingcardstools.BarcodeParser.BarcodeParser.BarcodeParser.insert_length","text":"Extract the insertion sequence length from the barcode dictionary","title":"insert_length"},{"location":"API/BarcodeParser/BarcodeParser/#callingcardstools.BarcodeParser.BarcodeParser.BarcodeParser.insert_seq","text":"Getter for the insert seq sequence from the barcode details json. Returns upper case. Raises: Type Description AttributeError Raised if the current barcode details json does not have an insert seq key","title":"insert_seq"},{"location":"API/BarcodeParser/BarcodeParser/#callingcardstools.BarcodeParser.BarcodeParser.BarcodeParser.key_dict","text":"A dictionary which describes the defined fields of a valid barcode dict json file","title":"key_dict"},{"location":"API/BarcodeParser/BarcodeParser/#callingcardstools.BarcodeParser.BarcodeParser.BarcodeParser.max_mismatches","text":"maximum number of mismatches allowed in a barcode","title":"max_mismatches"},{"location":"API/BarcodeParser/BarcodeParser/#callingcardstools.BarcodeParser.BarcodeParser.BarcodeParser.__calculate_max_mismatches","text":"either extract or calculate the maximum number of mismatches allowed in a given read\u2019s barcode. If not provided, the max is the sum of the match_allowances for each component. These are 0 if not set in barcode zeroes. This allows the user to allow a number of mismatches in each component, but to set a value less that the sum of those allowances in the max_mismatch attribute to avoid a barcode with too many total mismatches. Raises: Type Description TypeError if the extracted or calculated max_mismatch value is not an integer Returns: Name Type Description int int an integer describing the total number of allowable mismatches between a barcode and a set of barcode components. Source code in callingcardstools/BarcodeParser/BarcodeParser.py 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def __calculate_max_mismatches ( self ) -> int : \"\"\"either extract or calculate the maximum number of mismatches allowed in a given read's barcode. If not provided, the max is the sum of the match_allowances for each component. These are 0 if not set in barcode zeroes. This allows the user to allow a number of mismatches in each component, but to set a value less that the sum of those allowances in the max_mismatch attribute to avoid a barcode with too many total mismatches. Raises: TypeError: if the extracted or calculated max_mismatch value is not an integer Returns: int: an integer describing the total number of allowable mismatches between a barcode and a set of barcode components. \"\"\" max_mismatch = self . barcode_dict . get ( 'max_mismatch' , None ) component_mismatch_sum = sum ([ self . barcode_dict [ 'components' ][ k ] . get ( 'match_allowance' , 0 ) for k in self . barcode_dict [ 'components' ] . keys ()]) if max_mismatch : if max_mismatch > component_mismatch_sum : logger . info ( 'max_mismatch in barcode_details: ' f ' { max_mismatch } which is greater than the sum ' f 'of component mismatch allowances: ' f ' { component_mismatch_sum } ' ) if not max_mismatch : max_mismatch = component_mismatch_sum if not isinstance ( max_mismatch , int ): raise TypeError ( 'max_mismatch must be an integer. ' 'check barcode_details.__get_max_mismatches()' ) return max_mismatch","title":"__calculate_max_mismatches()"},{"location":"API/BarcodeParser/BarcodeParser/#callingcardstools.BarcodeParser.BarcodeParser.BarcodeParser.__check_bam_tags","text":"check the tags set by the user in barcode_details.json against the list of restricted bam tags \u2013 error if a restricted bam tag is used Source code in callingcardstools/BarcodeParser/BarcodeParser.py 198 199 200 201 202 203 def __check_bam_tags ( self ): \"\"\"check the tags set by the user in barcode_details.json against the list of restricted bam tags -- error if a restricted bam tag is used\"\"\" # TODO implement raise NotImplementedError","title":"__check_bam_tags()"},{"location":"API/BarcodeParser/BarcodeParser/#callingcardstools.BarcodeParser.BarcodeParser.BarcodeParser.__check_component_keys","text":"summary Parameters: Name Type Description Default barcode_dict _type_ description required Raises: Type Description KeyError if keys \u2018r1\u2019, \u2018r2\u2019 and \u2018components\u2019 DNE ValueError description Returns: Name Type Description bool bool description Source code in callingcardstools/BarcodeParser/BarcodeParser.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def __check_component_keys ( self , barcode_dict ) -> bool : \"\"\"_summary_ Args: barcode_dict (_type_): _description_ Raises: KeyError: if keys 'r1', 'r2' and 'components' DNE ValueError: _description_ Returns: bool: _description_ \"\"\" # extract keys in r1 and r2 and append appropriate read in as prefix read_keys = { 'r1_' + x for x in barcode_dict [ 'r1' ] . keys ()} \\ . union ({ 'r2_' + x for x in barcode_dict [ 'r2' ] . keys ()}) # check to make sure that the keys within r1 and r2 have unique names if len ( read_keys ) != len ( barcode_dict [ 'r1' ]) + len ( barcode_dict [ 'r2' ]): raise ValueError ( 'keys within r1 and r2 must have unique names' ) # extract keys in the components attribute component_keys = set () for k , v in barcode_dict [ 'components' ] . items (): if not isinstance ( v , dict ): ValueError ( 'Entries in components must be dictionaries' ) # Admittedly, this is confusing. but, a subkey of component may # also be components in the event that this given component is # compound, eg the yeast TF where the TF bc is made up of r1_primer # and r2_transposon. If this is the case, then extract those # items listed in the subkey 'components' if v . get ( 'components' , None ): component_keys . update ({ x for x in v [ 'components' ]}) # else, add the key value else : component_keys . update ({ k }) if len ( read_keys - component_keys ) != 0 : raise KeyError ( \"The components do not fully describe the \" + \"sequences extracted from the barcode\" ) # return true if no errors are raised return True","title":"__check_component_keys()"},{"location":"API/BarcodeParser/BarcodeParser/#callingcardstools.BarcodeParser.BarcodeParser.BarcodeParser.__init__","text":"BarcodeParser Constructor Parameters: Name Type Description Default barcode_details_json str Path to the barcode details json file required Source code in callingcardstools/BarcodeParser/BarcodeParser.py 41 42 43 44 45 46 47 48 49 def __init__ ( self , barcode_details_json : str ) -> None : \"\"\"BarcodeParser Constructor Args: barcode_details_json (str): Path to the barcode details json file \"\"\" # set the initialized barcode details # note that this also sets the match allowances self . barcode_details_json = barcode_details_json","title":"__init__()"},{"location":"API/BarcodeParser/BarcodeParser/#callingcardstools.BarcodeParser.BarcodeParser.BarcodeParser.component_check","text":"Determine if the barcode passes (True) or fails (False) given the edit distances between it and the expected components, and the allowable edit distance between a given component and the actual value. Parameters: Name Type Description Default component_dict dict a dictionary where the keys are component names and the values are the actual sequences, eg {\u2018r1_primer\u2019: \u2018TGATA\u2019, \u2018r1_transposon\u2019: \u2018AATTCACTACGTCAACA\u2019, \u2018r2_transposon\u2019: \u2018ACCTGCTT\u2019, \u2018r2_restriction\u2019: \u2018TCGAGCGCCCGG\u2019} required Returns: Name Type Description dict dict A dict of structure {\u201cpass\u201d: Boolean, True if the barcode passes, \u201ctf\u201d: Str, where the value is either \u201c*\u201d if unknown or a TF string from the barcode_details} Source code in callingcardstools/BarcodeParser/BarcodeParser.py 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 def component_check ( self , component_dict : dict ) -> dict : \"\"\"Determine if the barcode passes (True) or fails (False) given the edit distances between it and the expected components, and the allowable edit distance between a given component and the actual value. Args: component_dict (dict): a dictionary where the keys are component names and the values are the actual sequences, eg {'r1_primer': 'TGATA', 'r1_transposon': 'AATTCACTACGTCAACA', 'r2_transposon': 'ACCTGCTT', 'r2_restriction': 'TCGAGCGCCCGG'} Returns: dict: A dict of structure {\"pass\": Boolean, True if the barcode passes, \"tf\": Str, where the value is either \"*\" if unknown or a TF string from the barcode_details} \"\"\" component_check_dict = {} for k , v in self . barcode_dict [ self . key_dict [ 'components' ]] . items (): # if the value of a given component is a list, create a dict # from from the list where the keys and values are the same. target_dict = { x : x for x in v [ 'map' ]} \\ if isinstance ( v . get ( 'map' , None ), list ) \\ else v . get ( 'map' , None ) if not isinstance ( target_dict , dict ): raise ValueError ( 'Each component must have a map entry which ' 'is either a list or a dictionary' ) # if this is a compound component (eg the tf for yeast), # construct the sequence from the components. Else, extract the # sequence from the input component dict for the given key if v . get ( 'components' , None ): target_dict_offset = 0 # first, calculate the distances for the components # separately for component in v . get ( 'components' , None ): comp_split = component . split ( '_' ) query_seq = component_dict . get ( component , None ) start = target_dict_offset end = start + ( self . barcode_dict [ comp_split [ 0 ]] [ comp_split [ 1 ]] [ 'index' ][ 1 ] - self . barcode_dict [ comp_split [ 0 ]] [ comp_split [ 1 ]][ 'index' ][ 0 ]) target_dict_offset = end component_target_dict = {} for seq , id in target_dict . items (): component_target_dict . update ({ seq [ start : end ]: id }) component_check_dict [ component ] = \\ self . get_best_match ( query_seq , component_target_dict , v . get ( 'match_type' , 'edit_distance' )) # next, calculate the distances for the compound component # by concatenating the component sequences query_seq = '' . join ([ component_dict . get ( x , '' ) for x in v . get ( 'components' , None )]) component_check_dict [ k ] = \\ self . get_best_match ( query_seq , target_dict , v . get ( 'match_type' , 'edit_distance' )) else : query_seq = component_dict [ k ] component_check_dict [ k ] = \\ self . get_best_match ( query_seq , target_dict , v . get ( 'match_type' , 'edit_distance' )) if v . get ( 'bam_tag' , None ): component_check_dict [ k ][ 'bam_tag' ] = v . get ( 'bam_tag' ) # figure out if the barcode passes based on edit distance allowances passing = True total_mismatches = 0 for component in self . barcode_dict [ 'components' ] . keys (): component_metrics = component_check_dict [ component ] # if the total_mismatches exceed the maximum number of mismatches # in a given barcode, set the passing value to false and exit the # loop if total_mismatches > self . max_mismatches : passing = False logger . debug ( 'total_mismatches = {total_mismatches} ; ' + 'max_mismatches = {self.max_mismatches} ' ) break # else, for a given component, extract the mismatch tolerance match_allowance = \\ self . barcode_dict [ 'components' ][ component ] \\ . get ( 'match_allowance' , 0 ) # if the edit dist exceeds the match_allowance, set the barcode to # failing and break the loop if component_metrics [ 'dist' ] > match_allowance and \\ self . barcode_dict [ 'components' ] \\ . get ( component , {}) \\ . get ( 'require' , True ): passing = False break # if we reach this point, add the edit dist to the total_mismatch # note that if this is 0 it won't change anything, and move on to # the next component else : if self . barcode_dict [ 'components' ] \\ . get ( component , {}) \\ . get ( 'require' , True ): logger . debug ( f 'incrementing total mismatches ' f 'b/c of { component_metrics } ' ) total_mismatches = ( total_mismatches + component_metrics [ 'dist' ]) return { 'passing' : passing , 'details' : component_check_dict }","title":"component_check()"},{"location":"API/BarcodeParser/BarcodeParser/#callingcardstools.BarcodeParser.BarcodeParser.BarcodeParser.get_best_match","text":"Given a match method, return a dictionary describing the best match between query and component_dict Parameters: Name Type Description Default query str the string to compare to the component dict values required component_dict dict a dictionary where the keys are the sequences to compare to the query and the values are the names of the sequences required match_type str Either \u2018edit_distance\u2019 or \u2018greedy\u2019. Defaults to \u2018edit_distance\u2019. 'edit_distance' seed int seed for random number generator. Defaults to 42. 42 Returns: Name Type Description dict dict A dictionary of structure {\u2018name\u2019: either the sequence match, or the name if map is a named dictionary, \u2018dist\u2019: edit dist between query and best match \u2013 always 0 or infinty if match is greedy depending on if exact match is found or not. If not, return is \u2018name\u2019: \u2018*\u2019, \u2018dist\u2019: inf} Source code in callingcardstools/BarcodeParser/BarcodeParser.py 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 @staticmethod def get_best_match ( query : str , component_dict : dict , match_type : Literal [ 'edit_distance' , 'greedy' ] = 'edit_distance' , seed : int = 42 ) -> dict : \"\"\"Given a match method, return a dictionary describing the best match between query and component_dict Args: query (str): the string to compare to the component dict values component_dict (dict): a dictionary where the keys are the sequences to compare to the query and the values are the names of the sequences match_type (str, optional): Either 'edit_distance' or 'greedy'. Defaults to 'edit_distance'. seed (int, optional): seed for random number generator. Defaults to 42. Returns: dict: A dictionary of structure {'name': either the sequence match, or the name if map is a named dictionary, 'dist': edit dist between query and best match -- always 0 or infinty if match is greedy depending on if exact match is found or not. If not, return is 'name': '*', 'dist': inf} \"\"\" if not isinstance ( seed , int ): logger . warning ( 'seed must be an integer. Setting to 42' ) seed = 42 random . seed ( seed ) # if no match found, these are the default values. Note that if # the match type is not recognized, this function errors component_name = \"*\" dist = infinity # if the match_type is edit_distance, then return a dict with the # keys query, with the query sequence, name, with the component # to which the query best matched, and edit_dist, with the edit # distance between query and the best match if match_type == 'edit_distance' : # iterate over the component dict and align strings. dictionary # is one where the key is the edit_distance and the value is a # list of elements which have that edit distance compared to the # query d = {} for k , v in component_dict . items (): # pylint:disable=C0103 d . setdefault ( align ( query , k )[ 'editDistance' ], []) . append ( v ) # if the minimum edit distance is 0, then the first and only # element in the list os the correct one. Else, select an element # with the minimum element distance at random, if there are more # than 1 possibility with an a given edit distance element_selector = 0 if min ( d ) == 0 \\ else random . randrange ( 0 , len ( d . get ( min ( d ))), 1 ) # set name and dist component_name = d [ min ( d )][ element_selector ] dist = min ( d ) # if the match type is greedy, return the first exact match. same # return structure as above, where the matched value is the key and # the edit distance is 0. If none are found, value is \"*\" and the # edit distance is infinity elif match_type == 'greedy' : for k , v in component_dict . items (): # set name and dist and break the for loop if k in query : component_name = v dist = 0 break else : raise IOError ( ' %s is not a recognized match_type argument' % match_type ) return { 'query' : query , 'name' : component_name , 'dist' : dist }","title":"get_best_match()"},{"location":"API/BarcodeParser/mammals/BarcodeCounterQc/","text":"This module contains the BarcodeQcCounter class, which is used for analyzing barcode quality control data. The class can process and update barcode metrics, combine multiple objects, and write the results to output files. Author: Chase Mateusiak Date: 2023-05-16 BarcodeQcCounter \u00b6 A class for counting and processing barcode quality control data. Attributes: Name Type Description metrics DefaultDict A nested defaultdict containing the barcode metrics. ltr1_seq_dict DefaultDict A defaultdict storing the R1 transposon sequences. Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 class BarcodeQcCounter : \"\"\"A class for counting and processing barcode quality control data. Attributes: metrics (DefaultDict): A nested defaultdict containing the barcode metrics. ltr1_seq_dict (DefaultDict): A defaultdict storing the R1 transposon sequences. \"\"\" _metrics : DefaultDict _bc_status : DefaultDict def __init__ ( self , pickle_path : str = None ) -> None : \"\"\"Initializes a BarcodeQcCounter instance. Args: pickle_path (str, optional): Path to a pickled BarcodeQcCounter object. If provided, loads the object from the file. Defaults to None. Raises: FileNotFoundError: If the provided pickle path does not exist. \"\"\" if pickle_path : if not os . path . exists ( pickle_path ): msg = f \"Path to pickle file { pickle_path } does not exist\" raise FileNotFoundError ( msg ) self . load ( pickle_path ) else : self . metrics = OuterDefaultDict ( int ) self . bc_status = OuterDefaultDict ( bool ) @property def metrics ( self ) -> defaultdict : \"\"\"Returns the _metrics attribute. Returns: defaultdict: The _metrics attribute. \"\"\" return self . _metrics @metrics . setter def metrics ( self , value : defaultdict ) -> None : \"\"\"Sets the _metrics attribute\"\"\" self . _metrics = value @property def bc_status ( self ) -> defaultdict : \"\"\"Returns the _bc_status attribute. Returns: defaultdict: The _bc_status attribute. \"\"\" return self . _bc_status @bc_status . setter def bc_status ( self , value : defaultdict ) -> None : \"\"\"Sets the _bc_status attribute\"\"\" self . _bc_status = value # private methods --------------------------------------------------------- def _combine ( self , other : \"BarcodeQcCounter\" ) -> None : \"\"\"Combine the metrics from another BarcodeQcCounter object. Args: other (BarcodeQcCounter): Another BarcodeQcCounter object whose metrics will be combined with this object. \"\"\" def combine_dicts_additive ( d1 , d2 ): \"\"\"Recursive function to combine two nested dictionaries.\"\"\" for k , v in d2 . items (): if isinstance ( v , dict ): d1 [ k ] = combine_dicts_additive ( d1 . get ( k , {}), v ) else : d1 [ k ] = d1 . get ( k , 0 ) + v return d1 def combine_dicts_bool ( d1 , d2 ): \"\"\"Recursive function to combine two nested dictionaries.\"\"\" for k , v in d2 . items (): if isinstance ( v , dict ): d1 [ k ] = combine_dicts_bool ( d1 . get ( k , {}), v ) else : d1 [ k ] = v return d1 # combine the metrics dictionaries self . _metrics = combine_dicts_additive ( self . _metrics , other . metrics ) # combine the bc_status dictionaries self . _bc_status = combine_dicts_bool ( self . _bc_status , other . bc_status ) # public methods ---------------------------------------------------------- def load ( self , file_path : str ) -> None : \"\"\"Load a BarcodeQcCounter object from a file using Pickle. Args: file_path (str): The file path where the object is stored. \"\"\" logger . info ( \"loading BarcodeQcCounter object from %s \" , file_path ) with open ( file_path , \"rb\" ) as file : file_data = pickle . load ( file ) if not isinstance ( file_data , BarcodeQcCounter ): raise TypeError ( f \" { file_path } is not a BarcodeQcCounter object\" ) # copy the data from the loaded object to the current instance self . metrics = file_data . metrics self . bc_status = file_data . bc_status @classmethod def combine ( cls , counters : Iterable [ \"BarcodeQcCounter\" ]) -> \"BarcodeQcCounter\" : \"\"\"Combine multiple BarcodeQcCounter objects into a single object. Args: counters (Iterable[BarcodeQcCounter]): An iterable of BarcodeQcCounter objects. Returns: BarcodeQcCounter: A new BarcodeQcCounter object with the combined metrics. \"\"\" result = BarcodeQcCounter () for counter in counters : result . _combine ( counter ) return result def __add__ ( self , other : \"BarcodeQcCounter\" ) -> \"BarcodeQcCounter\" : \"\"\"Add two BarcodeQcCounter objects together with the + operator.\"\"\" if not isinstance ( other , BarcodeQcCounter ): raise TypeError ( \"Both objects must be of type 'BarcodeQcCounter'\" ) result = BarcodeQcCounter () return result . combine ([ self , other ]) def update ( self , pb_seq : str , ltr1_seq : str , ltr2_seq : str , srt_seq : str , bc_status : bool ) -> None : \"\"\"Updates the metrics with given component and deviation tuples. Args: pb_seq (str): The primer binding sequence. ltr1_seq (str): The left transposon sequence. ltr2_seq (str): The right transposon sequence. srt_seq (str): The sample barcode sequence. bc_status (bool): The barcode status. \"\"\" ( self . _metrics [ pb_seq ] [ ltr1_seq ] [ ltr2_seq ] [ srt_seq ]) += 1 ( self . _bc_status [ pb_seq ] [ ltr1_seq ] [ ltr2_seq ] [ srt_seq ]) = bc_status def write ( self , filename : str , suffix : str = \"\" , raw : bool = False ) -> None : \"\"\"Write a pickle and/or a comma-delimited file summarizing the barcode QC metrics. Args: filename (str, optional): The base filename for the output files. Defaults to \"barcode_qc\". suffix (str, optional): A suffix to be appended to the base filename. Defaults to an empty string. raw (bool, optional): If True, pickles the object. Defaults to False. \"\"\" # if raw is true, then pickle the object if raw : pickle_path = filename + '_' + suffix + '_barcode_qc.pkl' \\ if suffix else filename + '_barcode_qc.pkl' logger . info ( \"pickling barcode_qc object to %s{pick_path} \" ) with open ( pickle_path , \"wb\" ) as pickle_file : pickle . dump ( self , pickle_file ) else : # write the barcode qc metrics to a csv file tsv_path = filename + '_' + suffix + \"_barcode_qc.tsv\" \\ if suffix else filename + '_barcode_qc.tsv' logger . info ( \"writing barcode qc metrics to %s \" , tsv_path ) with open ( tsv_path , \"w\" , encoding = 'utf-8' ) as tsv_file : csv_writer = csv . writer ( tsv_file , delimiter = ' \\t ' ) csv_writer . writerow ([ \"pb_seq\" , \"ltr1_seq\" , \"ltr2_seq\" , \"srt_seq\" , \"count\" , \"barcode_status\" ]) for pb_seq , ltr1_dict in self . _metrics . items (): for ltr1_seq , ltr2_dict in ltr1_dict . items (): for ltr2_seq , srt_dict in ltr2_dict . items (): for srt_seq , count in srt_dict . items (): bc_status = ( \"pass\" if ( self . _bc_status [ pb_seq ] [ ltr1_seq ] [ ltr2_seq ] [ srt_seq ]) else \"false\" ) csv_writer . writerow ([ pb_seq , ltr1_seq , ltr2_seq , srt_seq , count , bc_status ]) bc_status : defaultdict property writable \u00b6 Returns the _bc_status attribute. Returns: Name Type Description defaultdict defaultdict The _bc_status attribute. metrics : defaultdict property writable \u00b6 Returns the _metrics attribute. Returns: Name Type Description defaultdict defaultdict The _metrics attribute. __add__ ( other ) \u00b6 Add two BarcodeQcCounter objects together with the + operator. Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 194 195 196 197 198 199 200 def __add__ ( self , other : \"BarcodeQcCounter\" ) -> \"BarcodeQcCounter\" : \"\"\"Add two BarcodeQcCounter objects together with the + operator.\"\"\" if not isinstance ( other , BarcodeQcCounter ): raise TypeError ( \"Both objects must be of type 'BarcodeQcCounter'\" ) result = BarcodeQcCounter () return result . combine ([ self , other ]) __init__ ( pickle_path = None ) \u00b6 Initializes a BarcodeQcCounter instance. Parameters: Name Type Description Default pickle_path str Path to a pickled BarcodeQcCounter object. If provided, loads the object from the file. Defaults to None. None Raises: Type Description FileNotFoundError If the provided pickle path does not exist. Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , pickle_path : str = None ) -> None : \"\"\"Initializes a BarcodeQcCounter instance. Args: pickle_path (str, optional): Path to a pickled BarcodeQcCounter object. If provided, loads the object from the file. Defaults to None. Raises: FileNotFoundError: If the provided pickle path does not exist. \"\"\" if pickle_path : if not os . path . exists ( pickle_path ): msg = f \"Path to pickle file { pickle_path } does not exist\" raise FileNotFoundError ( msg ) self . load ( pickle_path ) else : self . metrics = OuterDefaultDict ( int ) self . bc_status = OuterDefaultDict ( bool ) combine ( counters ) classmethod \u00b6 Combine multiple BarcodeQcCounter objects into a single object. Parameters: Name Type Description Default counters Iterable [ BarcodeQcCounter ] An iterable of BarcodeQcCounter objects. required Returns: Name Type Description BarcodeQcCounter BarcodeQcCounter A new BarcodeQcCounter object with the combined metrics. Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 @classmethod def combine ( cls , counters : Iterable [ \"BarcodeQcCounter\" ]) -> \"BarcodeQcCounter\" : \"\"\"Combine multiple BarcodeQcCounter objects into a single object. Args: counters (Iterable[BarcodeQcCounter]): An iterable of BarcodeQcCounter objects. Returns: BarcodeQcCounter: A new BarcodeQcCounter object with the combined metrics. \"\"\" result = BarcodeQcCounter () for counter in counters : result . _combine ( counter ) return result load ( file_path ) \u00b6 Load a BarcodeQcCounter object from a file using Pickle. Parameters: Name Type Description Default file_path str The file path where the object is stored. required Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def load ( self , file_path : str ) -> None : \"\"\"Load a BarcodeQcCounter object from a file using Pickle. Args: file_path (str): The file path where the object is stored. \"\"\" logger . info ( \"loading BarcodeQcCounter object from %s \" , file_path ) with open ( file_path , \"rb\" ) as file : file_data = pickle . load ( file ) if not isinstance ( file_data , BarcodeQcCounter ): raise TypeError ( f \" { file_path } is not a BarcodeQcCounter object\" ) # copy the data from the loaded object to the current instance self . metrics = file_data . metrics self . bc_status = file_data . bc_status update ( pb_seq , ltr1_seq , ltr2_seq , srt_seq , bc_status ) \u00b6 Updates the metrics with given component and deviation tuples. Parameters: Name Type Description Default pb_seq str The primer binding sequence. required ltr1_seq str The left transposon sequence. required ltr2_seq str The right transposon sequence. required srt_seq str The sample barcode sequence. required bc_status bool The barcode status. required Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def update ( self , pb_seq : str , ltr1_seq : str , ltr2_seq : str , srt_seq : str , bc_status : bool ) -> None : \"\"\"Updates the metrics with given component and deviation tuples. Args: pb_seq (str): The primer binding sequence. ltr1_seq (str): The left transposon sequence. ltr2_seq (str): The right transposon sequence. srt_seq (str): The sample barcode sequence. bc_status (bool): The barcode status. \"\"\" ( self . _metrics [ pb_seq ] [ ltr1_seq ] [ ltr2_seq ] [ srt_seq ]) += 1 ( self . _bc_status [ pb_seq ] [ ltr1_seq ] [ ltr2_seq ] [ srt_seq ]) = bc_status write ( filename , suffix = '' , raw = False ) \u00b6 Write a pickle and/or a comma-delimited file summarizing the barcode QC metrics. Parameters: Name Type Description Default filename str The base filename for the output files. Defaults to \u201cbarcode_qc\u201d. required suffix str A suffix to be appended to the base filename. Defaults to an empty string. '' raw bool If True, pickles the object. Defaults to False. False Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 def write ( self , filename : str , suffix : str = \"\" , raw : bool = False ) -> None : \"\"\"Write a pickle and/or a comma-delimited file summarizing the barcode QC metrics. Args: filename (str, optional): The base filename for the output files. Defaults to \"barcode_qc\". suffix (str, optional): A suffix to be appended to the base filename. Defaults to an empty string. raw (bool, optional): If True, pickles the object. Defaults to False. \"\"\" # if raw is true, then pickle the object if raw : pickle_path = filename + '_' + suffix + '_barcode_qc.pkl' \\ if suffix else filename + '_barcode_qc.pkl' logger . info ( \"pickling barcode_qc object to %s{pick_path} \" ) with open ( pickle_path , \"wb\" ) as pickle_file : pickle . dump ( self , pickle_file ) else : # write the barcode qc metrics to a csv file tsv_path = filename + '_' + suffix + \"_barcode_qc.tsv\" \\ if suffix else filename + '_barcode_qc.tsv' logger . info ( \"writing barcode qc metrics to %s \" , tsv_path ) with open ( tsv_path , \"w\" , encoding = 'utf-8' ) as tsv_file : csv_writer = csv . writer ( tsv_file , delimiter = ' \\t ' ) csv_writer . writerow ([ \"pb_seq\" , \"ltr1_seq\" , \"ltr2_seq\" , \"srt_seq\" , \"count\" , \"barcode_status\" ]) for pb_seq , ltr1_dict in self . _metrics . items (): for ltr1_seq , ltr2_dict in ltr1_dict . items (): for ltr2_seq , srt_dict in ltr2_dict . items (): for srt_seq , count in srt_dict . items (): bc_status = ( \"pass\" if ( self . _bc_status [ pb_seq ] [ ltr1_seq ] [ ltr2_seq ] [ srt_seq ]) else \"false\" ) csv_writer . writerow ([ pb_seq , ltr1_seq , ltr2_seq , srt_seq , count , bc_status ]) InnerDefaultDict \u00b6 Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 20 21 22 23 24 25 26 27 28 class InnerDefaultDict ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( data_type ) MiddleDefaultDict1 \u00b6 Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 31 32 33 34 35 36 37 38 39 class MiddleDefaultDict1 ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( partial ( InnerDefaultDict , data_type )) MiddleDefaultDict2 \u00b6 Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 42 43 44 45 46 47 48 49 50 class MiddleDefaultDict2 ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( partial ( MiddleDefaultDict1 , data_type )) OuterDefaultDict \u00b6 Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 53 54 55 56 57 58 59 60 61 class OuterDefaultDict ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( partial ( MiddleDefaultDict2 , data_type ))","title":"BarcodeCounterQc"},{"location":"API/BarcodeParser/mammals/BarcodeCounterQc/#callingcardstools.BarcodeParser.mammals.BarcodeQcCounter.BarcodeQcCounter","text":"A class for counting and processing barcode quality control data. Attributes: Name Type Description metrics DefaultDict A nested defaultdict containing the barcode metrics. ltr1_seq_dict DefaultDict A defaultdict storing the R1 transposon sequences. Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 class BarcodeQcCounter : \"\"\"A class for counting and processing barcode quality control data. Attributes: metrics (DefaultDict): A nested defaultdict containing the barcode metrics. ltr1_seq_dict (DefaultDict): A defaultdict storing the R1 transposon sequences. \"\"\" _metrics : DefaultDict _bc_status : DefaultDict def __init__ ( self , pickle_path : str = None ) -> None : \"\"\"Initializes a BarcodeQcCounter instance. Args: pickle_path (str, optional): Path to a pickled BarcodeQcCounter object. If provided, loads the object from the file. Defaults to None. Raises: FileNotFoundError: If the provided pickle path does not exist. \"\"\" if pickle_path : if not os . path . exists ( pickle_path ): msg = f \"Path to pickle file { pickle_path } does not exist\" raise FileNotFoundError ( msg ) self . load ( pickle_path ) else : self . metrics = OuterDefaultDict ( int ) self . bc_status = OuterDefaultDict ( bool ) @property def metrics ( self ) -> defaultdict : \"\"\"Returns the _metrics attribute. Returns: defaultdict: The _metrics attribute. \"\"\" return self . _metrics @metrics . setter def metrics ( self , value : defaultdict ) -> None : \"\"\"Sets the _metrics attribute\"\"\" self . _metrics = value @property def bc_status ( self ) -> defaultdict : \"\"\"Returns the _bc_status attribute. Returns: defaultdict: The _bc_status attribute. \"\"\" return self . _bc_status @bc_status . setter def bc_status ( self , value : defaultdict ) -> None : \"\"\"Sets the _bc_status attribute\"\"\" self . _bc_status = value # private methods --------------------------------------------------------- def _combine ( self , other : \"BarcodeQcCounter\" ) -> None : \"\"\"Combine the metrics from another BarcodeQcCounter object. Args: other (BarcodeQcCounter): Another BarcodeQcCounter object whose metrics will be combined with this object. \"\"\" def combine_dicts_additive ( d1 , d2 ): \"\"\"Recursive function to combine two nested dictionaries.\"\"\" for k , v in d2 . items (): if isinstance ( v , dict ): d1 [ k ] = combine_dicts_additive ( d1 . get ( k , {}), v ) else : d1 [ k ] = d1 . get ( k , 0 ) + v return d1 def combine_dicts_bool ( d1 , d2 ): \"\"\"Recursive function to combine two nested dictionaries.\"\"\" for k , v in d2 . items (): if isinstance ( v , dict ): d1 [ k ] = combine_dicts_bool ( d1 . get ( k , {}), v ) else : d1 [ k ] = v return d1 # combine the metrics dictionaries self . _metrics = combine_dicts_additive ( self . _metrics , other . metrics ) # combine the bc_status dictionaries self . _bc_status = combine_dicts_bool ( self . _bc_status , other . bc_status ) # public methods ---------------------------------------------------------- def load ( self , file_path : str ) -> None : \"\"\"Load a BarcodeQcCounter object from a file using Pickle. Args: file_path (str): The file path where the object is stored. \"\"\" logger . info ( \"loading BarcodeQcCounter object from %s \" , file_path ) with open ( file_path , \"rb\" ) as file : file_data = pickle . load ( file ) if not isinstance ( file_data , BarcodeQcCounter ): raise TypeError ( f \" { file_path } is not a BarcodeQcCounter object\" ) # copy the data from the loaded object to the current instance self . metrics = file_data . metrics self . bc_status = file_data . bc_status @classmethod def combine ( cls , counters : Iterable [ \"BarcodeQcCounter\" ]) -> \"BarcodeQcCounter\" : \"\"\"Combine multiple BarcodeQcCounter objects into a single object. Args: counters (Iterable[BarcodeQcCounter]): An iterable of BarcodeQcCounter objects. Returns: BarcodeQcCounter: A new BarcodeQcCounter object with the combined metrics. \"\"\" result = BarcodeQcCounter () for counter in counters : result . _combine ( counter ) return result def __add__ ( self , other : \"BarcodeQcCounter\" ) -> \"BarcodeQcCounter\" : \"\"\"Add two BarcodeQcCounter objects together with the + operator.\"\"\" if not isinstance ( other , BarcodeQcCounter ): raise TypeError ( \"Both objects must be of type 'BarcodeQcCounter'\" ) result = BarcodeQcCounter () return result . combine ([ self , other ]) def update ( self , pb_seq : str , ltr1_seq : str , ltr2_seq : str , srt_seq : str , bc_status : bool ) -> None : \"\"\"Updates the metrics with given component and deviation tuples. Args: pb_seq (str): The primer binding sequence. ltr1_seq (str): The left transposon sequence. ltr2_seq (str): The right transposon sequence. srt_seq (str): The sample barcode sequence. bc_status (bool): The barcode status. \"\"\" ( self . _metrics [ pb_seq ] [ ltr1_seq ] [ ltr2_seq ] [ srt_seq ]) += 1 ( self . _bc_status [ pb_seq ] [ ltr1_seq ] [ ltr2_seq ] [ srt_seq ]) = bc_status def write ( self , filename : str , suffix : str = \"\" , raw : bool = False ) -> None : \"\"\"Write a pickle and/or a comma-delimited file summarizing the barcode QC metrics. Args: filename (str, optional): The base filename for the output files. Defaults to \"barcode_qc\". suffix (str, optional): A suffix to be appended to the base filename. Defaults to an empty string. raw (bool, optional): If True, pickles the object. Defaults to False. \"\"\" # if raw is true, then pickle the object if raw : pickle_path = filename + '_' + suffix + '_barcode_qc.pkl' \\ if suffix else filename + '_barcode_qc.pkl' logger . info ( \"pickling barcode_qc object to %s{pick_path} \" ) with open ( pickle_path , \"wb\" ) as pickle_file : pickle . dump ( self , pickle_file ) else : # write the barcode qc metrics to a csv file tsv_path = filename + '_' + suffix + \"_barcode_qc.tsv\" \\ if suffix else filename + '_barcode_qc.tsv' logger . info ( \"writing barcode qc metrics to %s \" , tsv_path ) with open ( tsv_path , \"w\" , encoding = 'utf-8' ) as tsv_file : csv_writer = csv . writer ( tsv_file , delimiter = ' \\t ' ) csv_writer . writerow ([ \"pb_seq\" , \"ltr1_seq\" , \"ltr2_seq\" , \"srt_seq\" , \"count\" , \"barcode_status\" ]) for pb_seq , ltr1_dict in self . _metrics . items (): for ltr1_seq , ltr2_dict in ltr1_dict . items (): for ltr2_seq , srt_dict in ltr2_dict . items (): for srt_seq , count in srt_dict . items (): bc_status = ( \"pass\" if ( self . _bc_status [ pb_seq ] [ ltr1_seq ] [ ltr2_seq ] [ srt_seq ]) else \"false\" ) csv_writer . writerow ([ pb_seq , ltr1_seq , ltr2_seq , srt_seq , count , bc_status ])","title":"BarcodeQcCounter"},{"location":"API/BarcodeParser/mammals/BarcodeCounterQc/#callingcardstools.BarcodeParser.mammals.BarcodeQcCounter.BarcodeQcCounter.bc_status","text":"Returns the _bc_status attribute. Returns: Name Type Description defaultdict defaultdict The _bc_status attribute.","title":"bc_status"},{"location":"API/BarcodeParser/mammals/BarcodeCounterQc/#callingcardstools.BarcodeParser.mammals.BarcodeQcCounter.BarcodeQcCounter.metrics","text":"Returns the _metrics attribute. Returns: Name Type Description defaultdict defaultdict The _metrics attribute.","title":"metrics"},{"location":"API/BarcodeParser/mammals/BarcodeCounterQc/#callingcardstools.BarcodeParser.mammals.BarcodeQcCounter.BarcodeQcCounter.__add__","text":"Add two BarcodeQcCounter objects together with the + operator. Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 194 195 196 197 198 199 200 def __add__ ( self , other : \"BarcodeQcCounter\" ) -> \"BarcodeQcCounter\" : \"\"\"Add two BarcodeQcCounter objects together with the + operator.\"\"\" if not isinstance ( other , BarcodeQcCounter ): raise TypeError ( \"Both objects must be of type 'BarcodeQcCounter'\" ) result = BarcodeQcCounter () return result . combine ([ self , other ])","title":"__add__()"},{"location":"API/BarcodeParser/mammals/BarcodeCounterQc/#callingcardstools.BarcodeParser.mammals.BarcodeQcCounter.BarcodeQcCounter.__init__","text":"Initializes a BarcodeQcCounter instance. Parameters: Name Type Description Default pickle_path str Path to a pickled BarcodeQcCounter object. If provided, loads the object from the file. Defaults to None. None Raises: Type Description FileNotFoundError If the provided pickle path does not exist. Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , pickle_path : str = None ) -> None : \"\"\"Initializes a BarcodeQcCounter instance. Args: pickle_path (str, optional): Path to a pickled BarcodeQcCounter object. If provided, loads the object from the file. Defaults to None. Raises: FileNotFoundError: If the provided pickle path does not exist. \"\"\" if pickle_path : if not os . path . exists ( pickle_path ): msg = f \"Path to pickle file { pickle_path } does not exist\" raise FileNotFoundError ( msg ) self . load ( pickle_path ) else : self . metrics = OuterDefaultDict ( int ) self . bc_status = OuterDefaultDict ( bool )","title":"__init__()"},{"location":"API/BarcodeParser/mammals/BarcodeCounterQc/#callingcardstools.BarcodeParser.mammals.BarcodeQcCounter.BarcodeQcCounter.combine","text":"Combine multiple BarcodeQcCounter objects into a single object. Parameters: Name Type Description Default counters Iterable [ BarcodeQcCounter ] An iterable of BarcodeQcCounter objects. required Returns: Name Type Description BarcodeQcCounter BarcodeQcCounter A new BarcodeQcCounter object with the combined metrics. Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 @classmethod def combine ( cls , counters : Iterable [ \"BarcodeQcCounter\" ]) -> \"BarcodeQcCounter\" : \"\"\"Combine multiple BarcodeQcCounter objects into a single object. Args: counters (Iterable[BarcodeQcCounter]): An iterable of BarcodeQcCounter objects. Returns: BarcodeQcCounter: A new BarcodeQcCounter object with the combined metrics. \"\"\" result = BarcodeQcCounter () for counter in counters : result . _combine ( counter ) return result","title":"combine()"},{"location":"API/BarcodeParser/mammals/BarcodeCounterQc/#callingcardstools.BarcodeParser.mammals.BarcodeQcCounter.BarcodeQcCounter.load","text":"Load a BarcodeQcCounter object from a file using Pickle. Parameters: Name Type Description Default file_path str The file path where the object is stored. required Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def load ( self , file_path : str ) -> None : \"\"\"Load a BarcodeQcCounter object from a file using Pickle. Args: file_path (str): The file path where the object is stored. \"\"\" logger . info ( \"loading BarcodeQcCounter object from %s \" , file_path ) with open ( file_path , \"rb\" ) as file : file_data = pickle . load ( file ) if not isinstance ( file_data , BarcodeQcCounter ): raise TypeError ( f \" { file_path } is not a BarcodeQcCounter object\" ) # copy the data from the loaded object to the current instance self . metrics = file_data . metrics self . bc_status = file_data . bc_status","title":"load()"},{"location":"API/BarcodeParser/mammals/BarcodeCounterQc/#callingcardstools.BarcodeParser.mammals.BarcodeQcCounter.BarcodeQcCounter.update","text":"Updates the metrics with given component and deviation tuples. Parameters: Name Type Description Default pb_seq str The primer binding sequence. required ltr1_seq str The left transposon sequence. required ltr2_seq str The right transposon sequence. required srt_seq str The sample barcode sequence. required bc_status bool The barcode status. required Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def update ( self , pb_seq : str , ltr1_seq : str , ltr2_seq : str , srt_seq : str , bc_status : bool ) -> None : \"\"\"Updates the metrics with given component and deviation tuples. Args: pb_seq (str): The primer binding sequence. ltr1_seq (str): The left transposon sequence. ltr2_seq (str): The right transposon sequence. srt_seq (str): The sample barcode sequence. bc_status (bool): The barcode status. \"\"\" ( self . _metrics [ pb_seq ] [ ltr1_seq ] [ ltr2_seq ] [ srt_seq ]) += 1 ( self . _bc_status [ pb_seq ] [ ltr1_seq ] [ ltr2_seq ] [ srt_seq ]) = bc_status","title":"update()"},{"location":"API/BarcodeParser/mammals/BarcodeCounterQc/#callingcardstools.BarcodeParser.mammals.BarcodeQcCounter.BarcodeQcCounter.write","text":"Write a pickle and/or a comma-delimited file summarizing the barcode QC metrics. Parameters: Name Type Description Default filename str The base filename for the output files. Defaults to \u201cbarcode_qc\u201d. required suffix str A suffix to be appended to the base filename. Defaults to an empty string. '' raw bool If True, pickles the object. Defaults to False. False Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 def write ( self , filename : str , suffix : str = \"\" , raw : bool = False ) -> None : \"\"\"Write a pickle and/or a comma-delimited file summarizing the barcode QC metrics. Args: filename (str, optional): The base filename for the output files. Defaults to \"barcode_qc\". suffix (str, optional): A suffix to be appended to the base filename. Defaults to an empty string. raw (bool, optional): If True, pickles the object. Defaults to False. \"\"\" # if raw is true, then pickle the object if raw : pickle_path = filename + '_' + suffix + '_barcode_qc.pkl' \\ if suffix else filename + '_barcode_qc.pkl' logger . info ( \"pickling barcode_qc object to %s{pick_path} \" ) with open ( pickle_path , \"wb\" ) as pickle_file : pickle . dump ( self , pickle_file ) else : # write the barcode qc metrics to a csv file tsv_path = filename + '_' + suffix + \"_barcode_qc.tsv\" \\ if suffix else filename + '_barcode_qc.tsv' logger . info ( \"writing barcode qc metrics to %s \" , tsv_path ) with open ( tsv_path , \"w\" , encoding = 'utf-8' ) as tsv_file : csv_writer = csv . writer ( tsv_file , delimiter = ' \\t ' ) csv_writer . writerow ([ \"pb_seq\" , \"ltr1_seq\" , \"ltr2_seq\" , \"srt_seq\" , \"count\" , \"barcode_status\" ]) for pb_seq , ltr1_dict in self . _metrics . items (): for ltr1_seq , ltr2_dict in ltr1_dict . items (): for ltr2_seq , srt_dict in ltr2_dict . items (): for srt_seq , count in srt_dict . items (): bc_status = ( \"pass\" if ( self . _bc_status [ pb_seq ] [ ltr1_seq ] [ ltr2_seq ] [ srt_seq ]) else \"false\" ) csv_writer . writerow ([ pb_seq , ltr1_seq , ltr2_seq , srt_seq , count , bc_status ])","title":"write()"},{"location":"API/BarcodeParser/mammals/BarcodeCounterQc/#callingcardstools.BarcodeParser.mammals.BarcodeQcCounter.InnerDefaultDict","text":"Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 20 21 22 23 24 25 26 27 28 class InnerDefaultDict ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( data_type )","title":"InnerDefaultDict"},{"location":"API/BarcodeParser/mammals/BarcodeCounterQc/#callingcardstools.BarcodeParser.mammals.BarcodeQcCounter.MiddleDefaultDict1","text":"Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 31 32 33 34 35 36 37 38 39 class MiddleDefaultDict1 ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( partial ( InnerDefaultDict , data_type ))","title":"MiddleDefaultDict1"},{"location":"API/BarcodeParser/mammals/BarcodeCounterQc/#callingcardstools.BarcodeParser.mammals.BarcodeQcCounter.MiddleDefaultDict2","text":"Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 42 43 44 45 46 47 48 49 50 class MiddleDefaultDict2 ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( partial ( MiddleDefaultDict1 , data_type ))","title":"MiddleDefaultDict2"},{"location":"API/BarcodeParser/mammals/BarcodeCounterQc/#callingcardstools.BarcodeParser.mammals.BarcodeQcCounter.OuterDefaultDict","text":"Bases: defaultdict A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict Source code in callingcardstools/BarcodeParser/mammals/BarcodeQcCounter.py 53 54 55 56 57 58 59 60 61 class OuterDefaultDict ( defaultdict ): \"\"\"A nested defaultdict class. :param defaultdict: a nested defaultdict class :type defaultdict: defaultdict \"\"\" def __init__ ( self , data_type = int ): super () . __init__ ( partial ( MiddleDefaultDict2 , data_type ))","title":"OuterDefaultDict"},{"location":"API/BarcodeParser/yeast/BarcodeCounterQc/","text":"This module contains the BarcodeQcCounter class, which is used for analyzing barcode quality control data. The class can process and update barcode metrics, combine multiple objects, and write the results to output files. Author: Chase Mateusiak Date: 2023-05-01 BarcodeQcCounter \u00b6 A class for counting and processing barcode quality control data. Attributes: Name Type Description metrics DefaultDict A nested defaultdict containing the barcode metrics. r1_transposon_seq_dict DefaultDict A defaultdict storing the R1 transposon sequences. Source code in callingcardstools/BarcodeParser/yeast/BarcodeQcCounter.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 class BarcodeQcCounter : \"\"\"A class for counting and processing barcode quality control data. Attributes: metrics (DefaultDict): A nested defaultdict containing the barcode metrics. r1_transposon_seq_dict (DefaultDict): A defaultdict storing the R1 transposon sequences. \"\"\" _metrics : DefaultDict _r1_transposon_seq_dict : DefaultDict def __init__ ( self , pickle_path : str = None ) -> None : \"\"\"Initializes a BarcodeQcCounter instance. Args: pickle_path (str, optional): Path to a pickled BarcodeQcCounter object. If provided, loads the object from the file. Defaults to None. Raises: FileNotFoundError: If the provided pickle path does not exist. \"\"\" if pickle_path : if not os . path . exists ( pickle_path ): msg = f \"Path to pickle file { pickle_path } does not exist\" raise FileNotFoundError ( msg ) self . load ( pickle_path ) else : self . _metrics = OuterDefaultDict ( int ) self . _r1_transposon_seq_dict = defaultdict ( set ) @property def metrics ( self ) -> defaultdict : \"\"\"Returns the _metrics attribute. Returns: defaultdict: The _metrics attribute. \"\"\" return self . _metrics @property def r1_transposon_dict ( self ) -> defaultdict : \"\"\"Returns the _r1_transposon_seq_dict attribute. Returns: defaultdict: the _r1_transposon_seq_dict attribute. \"\"\" return self . _r1_transposon_seq_dict # private methods --------------------------------------------------------- def _combine ( self , other : \"BarcodeQcCounter\" ) -> None : \"\"\"Combine the metrics from another BarcodeQcCounter object. Args: other (BarcodeQcCounter): Another BarcodeQcCounter object whose metrics will be combined with this object. \"\"\" # Combine _metrics dictionaries for r1_transposon_edit_dist , r1_primer_dict in other . metrics . items (): for r1_primer_seq , r2_transposon_dict in r1_primer_dict . items (): for r2_transposon_seq , r2_restriction_enzyme_dict in \\ r2_transposon_dict . items (): for r2_restriction_enzyme_name , count in \\ r2_restriction_enzyme_dict . items (): ( self . _metrics [ r1_transposon_edit_dist ] [ r1_primer_seq ] [ r2_transposon_seq ] [ r2_restriction_enzyme_name ]) += count # Combine _r1_transposon_seq_dict dictionaries for r1_transposon_edit_dist , r1_transposon_seq_set in \\ other . r1_transposon_dict . items (): self . _r1_transposon_seq_dict [ r1_transposon_edit_dist ] \\ . update ( r1_transposon_seq_set ) # public methods ---------------------------------------------------------- def load ( self , file_path : str ) -> None : \"\"\"Load a BarcodeQcCounter object from a file using Pickle. Args: file_path (str): The file path where the object is stored. \"\"\" logger . info ( \"loading BarcodeQcCounter object from %s \" , file_path ) with open ( file_path , \"rb\" ) as file : file_data = pickle . load ( file ) if not isinstance ( file_data , BarcodeQcCounter ): raise TypeError ( f \" { file_path } is not a BarcodeQcCounter object\" ) # copy the data from the loaded object to the current instance self . _metrics = file_data . _metrics self . _r1_transposon_seq_dict = file_data . _r1_transposon_seq_dict @classmethod def combine ( cls , counters : Iterable [ \"BarcodeQcCounter\" ]) -> \"BarcodeQcCounter\" : \"\"\"Combine multiple BarcodeQcCounter objects into a single object. Args: counters (Iterable[BarcodeQcCounter]): An iterable of BarcodeQcCounter objects. Returns: BarcodeQcCounter: A new BarcodeQcCounter object with the combined metrics. \"\"\" result = BarcodeQcCounter () for counter in counters : result . _combine ( counter ) return result def __add__ ( self , other : \"BarcodeQcCounter\" ) -> \"BarcodeQcCounter\" : \"\"\"Add two BarcodeQcCounter objects together with the + operator.\"\"\" if not isinstance ( other , BarcodeQcCounter ): raise TypeError ( \"Both objects must be of type 'BarcodeQcCounter'\" ) result = BarcodeQcCounter () result . combine ( self ) result . combine ( other ) return result def update ( self , component_tuple : tuple , r1_transposon_edit_dist : int , r2_restriction_enzyme_name : str ) -> None : \"\"\"Updates the metrics with given component and deviation tuples. Args: component_tuple (tuple): A tuple containing R1 primer, R1 transposon, and R2 transposon sequences. r1_transposon_edit_dist (int): The edit distance between the R1 transposon sequence and the expected R1 transposon r2_restriction_enzyme_name (str): The R2 restriction enzyme name. \"\"\" ( r1_primer_seq , r1_transposon_seq , r2_transposon_seq ) = component_tuple ( self . _metrics [ r1_transposon_edit_dist ] [ r1_primer_seq ] [ r2_transposon_seq ] [ r2_restriction_enzyme_name ]) += 1 self . _r1_transposon_seq_dict [ r1_transposon_edit_dist ] \\ . add ( r1_transposon_seq ) def _summarize_by_tf ( self , component_dict : dict ) -> None : \"\"\"Summarizes the metrics by transcription factor (TF). Args: component_dict (dict): A dictionary containing keys for 'tf', 'r1_primers', and 'r2_transposons', and their respective lists of values. Returns: tuple: A tuple containing R1 primer summary and R2 transposon summary. \"\"\" # r1_primer_summary = [] r2_transposon_summary = [] # only iterate over those reads which had an r1 transposon seq # edit distance of n or less # r1_for_given_r2_dict = defaultdict(lambda: defaultdict(set)) r1_for_given_r2_dict = MiddleDefaultDict1 ( set ) for i , r1_transposon_dict in self . _metrics . items (): # first level of iteration is over the r1 primer keys. # The dictionary is a nested dictionary with the keys being # r2_transposon sequences and values another dicitonary with # the restriciton enzyme and count for r1_primer_seq , r1_primer_dict in r1_transposon_dict . items (): # if the r1 primer sequence is the expected sequence # for a given tf, then iterate over the r2 transposon # entries and record the results for r2_transposon_seq , r2_transposon_seq_dict in \\ r1_primer_dict . items (): # if the r2_transposon_seq is recognized, then save the # r1_primer_seq. structure of the dict is: # {'valid_r2_trans_seq': set(r1_primer_seq1, ...)} if r2_transposon_seq in \\ component_dict [ 'r2_transposon' ]: ( r1_for_given_r2_dict [ i ] [ r2_transposon_seq ] . add ( r1_primer_seq )) # if the r1_primer_seq is an expected sequence, then # iterate over the r2_transposon_seq_dict and record the # results if r1_primer_seq in component_dict [ 'r1_primer' ]: r1_primer_index = \\ component_dict [ 'r1_primer' ] . index ( r1_primer_seq ) r2_transposon_target_seq = \\ component_dict [ 'r2_transposon' ][ r1_primer_index ] edit_dist = \\ align ( r2_transposon_seq , r2_transposon_target_seq ) r1_primer_record = { \"tf\" : component_dict [ 'tf' ][ r1_primer_index ], \"r1_primer_seq\" : component_dict [ 'r1_primer' ][ r1_primer_index ], \"r1_transposon_edit_dist\" : i , \"r2_transposon_seq\" : r2_transposon_seq , \"r2_transposon_edit_dist\" : edit_dist . get ( \"editDistance\" )} for restriction_enzyme , count in \\ r2_transposon_seq_dict . items (): record_copy = r1_primer_record . copy () record_copy . update ({ 'restriction_enzyme' : restriction_enzyme , 'count' : count }) r1_primer_summary . append ( record_copy ) # in the second iteration, iterate over only those r1_primer_seqs with # a valid r2_transposon_seq for r1_transposon_ed , r1_transposon_ed_dict in \\ r1_for_given_r2_dict . items (): for r2_transposon_seq , r1_primer_seq_set in \\ r1_transposon_ed_dict . items (): # extract the TF and expected r1_primer sequence for this # r2_transposon_seq and TF index = component_dict [ 'r2_transposon' ] \\ . index ( r2_transposon_seq ) tf = component_dict [ 'tf' ][ index ] r1_primer_expected = component_dict [ \"r1_primer\" ][ index ] # iterate over all of the `r1_primer_seq` for this # r2_transposon_seq for r1_primer_query in r1_primer_seq_set : # align the r1_primer to the expected r1_primer for this # r2_transposon_seq and TF edit_dist = align ( r1_primer_query , r1_primer_expected ) # create the base record r2_transposon_record = { \"tf\" : tf , \"r2_transposon_seq\" : r2_transposon_seq , \"r1_transposon_edit_dist\" : r1_transposon_ed , \"r1_primer_seq\" : r1_primer_query , \"r1_primer_edit_dist\" : edit_dist . get ( \"editDistance\" )} for restriction_enzyme , count in \\ ( self . _metrics [ r1_transposon_ed ] [ r1_primer_query ] [ r2_transposon_seq ] . items ()): # make a copy of the record record_copy = r2_transposon_record . copy () # add additional restriction enzyme info record_copy . update ({ 'restriction_enzyme' : restriction_enzyme , 'count' : count }) r2_transposon_summary . append ( record_copy ) return r1_primer_summary , r2_transposon_summary def write ( self , raw : bool = False , component_dict : dict = None , output_dirpath : str = \".\" , filename : str = \"barcode_qc\" , suffix : str = \"\" ) -> None : \"\"\"Write a pickle and/or a comma-delimited file summarizing the barcode QC metrics. Args: raw (bool, optional): If True, pickles the object. Defaults to False. component_dict (dict, optional): A dictionary containing keys for 'tf', 'r1_primers', and 'r2_transposons', and their respective lists of values. If provided, writes summaries for each component. Defaults to None. output_dirpath (str, optional): The output directory path where the files will be saved. Defaults to the current directory. filename (str, optional): The base filename for the output files. Defaults to \"barcode_qc\". suffix (str, optional): A suffix to be appended to the base filename. Defaults to an empty string. \"\"\" # check that the output_dirpath is a valid directory if not os . path . join ( output_dirpath ): raise ValueError ( \"output_dirpath must be a valid directory\" ) # if raw is true, then pickle the object if raw : pickle_path = os . path . join ( output_dirpath , filename + '_' + suffix + \".pickle\" ) logger . info ( \"pickling barcode_qc object to %s{pick_path} \" ) with open ( pickle_path , \"wb\" ) as pickle_file : pickle . dump ( self , pickle_file ) # if component_dict is passed if component_dict : # input checks if not isinstance ( component_dict , dict ): raise TypeError ( \"component_dict must be a dictionary\" ) if not { 'tf' , 'r1_primer' , 'r2_transposon' } == \\ set ( list ( component_dict . keys ())): raise ValueError ( \"component_dict must be a dictionary \" \"where the keys are 'tf', 'r1_primers', \" \"'r2_transposons' and the values are \" \"lists of the same length. The index of \" \"each list corresponds to the same \" \"transcription factor.\" ) for k , v in component_dict . items (): if not isinstance ( v , list ): raise TypeError ( \"component_dict values must be lists\" ) if len ({ len ( x ) for x in component_dict . values ()}) != 1 : raise ValueError ( \"component_dict values must be lists of \" \"the same length\" ) # extract summaries from the metrics r1_primer_summary , r2_transposon_summary = \\ self . _summarize_by_tf ( component_dict ) # write r1_primer_summary to file append_suffix = '_' + suffix if suffix else '' r1_primer_basename = \\ filename + \"_r1_primer_summary\" + append_suffix + \".csv\" r1_primer_summary_path = os . path . join ( output_dirpath , r1_primer_basename ) r1_primer_summary_df = pd . DataFrame ( r1_primer_summary ) logger . info ( \"writing r1_primer_summary \" \"to %s{r1_primer_summary_path} \" ) r1_primer_summary_df . to_csv ( r1_primer_summary_path , index = False ) # write r2_transposon summary to file r2_transposon_summary_basename = \\ filename + \"_r2_transposon_summary\" + append_suffix + \".csv\" r2_transposon_summary_path = os . path . join ( output_dirpath , r2_transposon_summary_basename ) r2_transposon_summary_df = pd . DataFrame ( r2_transposon_summary ) logger . info ( \"writing r2_transposon_summary \" \"to %s{r2_transposon_summary_path} \" ) r2_transposon_summary_df . to_csv ( r2_transposon_summary_path , index = False ) metrics : defaultdict property \u00b6 Returns the _metrics attribute. Returns: Name Type Description defaultdict defaultdict The _metrics attribute. r1_transposon_dict : defaultdict property \u00b6 Returns the _r1_transposon_seq_dict attribute. Returns: Name Type Description defaultdict defaultdict the _r1_transposon_seq_dict attribute. __add__ ( other ) \u00b6 Add two BarcodeQcCounter objects together with the + operator. Source code in callingcardstools/BarcodeParser/yeast/BarcodeQcCounter.py 163 164 165 166 167 168 169 170 171 def __add__ ( self , other : \"BarcodeQcCounter\" ) -> \"BarcodeQcCounter\" : \"\"\"Add two BarcodeQcCounter objects together with the + operator.\"\"\" if not isinstance ( other , BarcodeQcCounter ): raise TypeError ( \"Both objects must be of type 'BarcodeQcCounter'\" ) result = BarcodeQcCounter () result . combine ( self ) result . combine ( other ) return result __init__ ( pickle_path = None ) \u00b6 Initializes a BarcodeQcCounter instance. Parameters: Name Type Description Default pickle_path str Path to a pickled BarcodeQcCounter object. If provided, loads the object from the file. Defaults to None. None Raises: Type Description FileNotFoundError If the provided pickle path does not exist. Source code in callingcardstools/BarcodeParser/yeast/BarcodeQcCounter.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , pickle_path : str = None ) -> None : \"\"\"Initializes a BarcodeQcCounter instance. Args: pickle_path (str, optional): Path to a pickled BarcodeQcCounter object. If provided, loads the object from the file. Defaults to None. Raises: FileNotFoundError: If the provided pickle path does not exist. \"\"\" if pickle_path : if not os . path . exists ( pickle_path ): msg = f \"Path to pickle file { pickle_path } does not exist\" raise FileNotFoundError ( msg ) self . load ( pickle_path ) else : self . _metrics = OuterDefaultDict ( int ) self . _r1_transposon_seq_dict = defaultdict ( set ) combine ( counters ) classmethod \u00b6 Combine multiple BarcodeQcCounter objects into a single object. Parameters: Name Type Description Default counters Iterable [ BarcodeQcCounter ] An iterable of BarcodeQcCounter objects. required Returns: Name Type Description BarcodeQcCounter BarcodeQcCounter A new BarcodeQcCounter object with the combined metrics. Source code in callingcardstools/BarcodeParser/yeast/BarcodeQcCounter.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 @classmethod def combine ( cls , counters : Iterable [ \"BarcodeQcCounter\" ]) -> \"BarcodeQcCounter\" : \"\"\"Combine multiple BarcodeQcCounter objects into a single object. Args: counters (Iterable[BarcodeQcCounter]): An iterable of BarcodeQcCounter objects. Returns: BarcodeQcCounter: A new BarcodeQcCounter object with the combined metrics. \"\"\" result = BarcodeQcCounter () for counter in counters : result . _combine ( counter ) return result load ( file_path ) \u00b6 Load a BarcodeQcCounter object from a file using Pickle. Parameters: Name Type Description Default file_path str The file path where the object is stored. required Source code in callingcardstools/BarcodeParser/yeast/BarcodeQcCounter.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def load ( self , file_path : str ) -> None : \"\"\"Load a BarcodeQcCounter object from a file using Pickle. Args: file_path (str): The file path where the object is stored. \"\"\" logger . info ( \"loading BarcodeQcCounter object from %s \" , file_path ) with open ( file_path , \"rb\" ) as file : file_data = pickle . load ( file ) if not isinstance ( file_data , BarcodeQcCounter ): raise TypeError ( f \" { file_path } is not a BarcodeQcCounter object\" ) # copy the data from the loaded object to the current instance self . _metrics = file_data . _metrics self . _r1_transposon_seq_dict = file_data . _r1_transposon_seq_dict update ( component_tuple , r1_transposon_edit_dist , r2_restriction_enzyme_name ) \u00b6 Updates the metrics with given component and deviation tuples. Parameters: Name Type Description Default component_tuple tuple A tuple containing R1 primer, R1 transposon, and R2 transposon sequences. required r1_transposon_edit_dist int The edit distance between the R1 transposon sequence and the expected R1 transposon required r2_restriction_enzyme_name str The R2 restriction enzyme name. required Source code in callingcardstools/BarcodeParser/yeast/BarcodeQcCounter.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def update ( self , component_tuple : tuple , r1_transposon_edit_dist : int , r2_restriction_enzyme_name : str ) -> None : \"\"\"Updates the metrics with given component and deviation tuples. Args: component_tuple (tuple): A tuple containing R1 primer, R1 transposon, and R2 transposon sequences. r1_transposon_edit_dist (int): The edit distance between the R1 transposon sequence and the expected R1 transposon r2_restriction_enzyme_name (str): The R2 restriction enzyme name. \"\"\" ( r1_primer_seq , r1_transposon_seq , r2_transposon_seq ) = component_tuple ( self . _metrics [ r1_transposon_edit_dist ] [ r1_primer_seq ] [ r2_transposon_seq ] [ r2_restriction_enzyme_name ]) += 1 self . _r1_transposon_seq_dict [ r1_transposon_edit_dist ] \\ . add ( r1_transposon_seq ) write ( raw = False , component_dict = None , output_dirpath = '.' , filename = 'barcode_qc' , suffix = '' ) \u00b6 Write a pickle and/or a comma-delimited file summarizing the barcode QC metrics. Parameters: Name Type Description Default raw bool If True, pickles the object. Defaults to False. False component_dict dict A dictionary containing keys for \u2018tf\u2019, \u2018r1_primers\u2019, and \u2018r2_transposons\u2019, and their respective lists of values. If provided, writes summaries for each component. Defaults to None. None output_dirpath str The output directory path where the files will be saved. Defaults to the current directory. '.' filename str The base filename for the output files. Defaults to \u201cbarcode_qc\u201d. 'barcode_qc' suffix str A suffix to be appended to the base filename. Defaults to an empty string. '' Source code in callingcardstools/BarcodeParser/yeast/BarcodeQcCounter.py 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 def write ( self , raw : bool = False , component_dict : dict = None , output_dirpath : str = \".\" , filename : str = \"barcode_qc\" , suffix : str = \"\" ) -> None : \"\"\"Write a pickle and/or a comma-delimited file summarizing the barcode QC metrics. Args: raw (bool, optional): If True, pickles the object. Defaults to False. component_dict (dict, optional): A dictionary containing keys for 'tf', 'r1_primers', and 'r2_transposons', and their respective lists of values. If provided, writes summaries for each component. Defaults to None. output_dirpath (str, optional): The output directory path where the files will be saved. Defaults to the current directory. filename (str, optional): The base filename for the output files. Defaults to \"barcode_qc\". suffix (str, optional): A suffix to be appended to the base filename. Defaults to an empty string. \"\"\" # check that the output_dirpath is a valid directory if not os . path . join ( output_dirpath ): raise ValueError ( \"output_dirpath must be a valid directory\" ) # if raw is true, then pickle the object if raw : pickle_path = os . path . join ( output_dirpath , filename + '_' + suffix + \".pickle\" ) logger . info ( \"pickling barcode_qc object to %s{pick_path} \" ) with open ( pickle_path , \"wb\" ) as pickle_file : pickle . dump ( self , pickle_file ) # if component_dict is passed if component_dict : # input checks if not isinstance ( component_dict , dict ): raise TypeError ( \"component_dict must be a dictionary\" ) if not { 'tf' , 'r1_primer' , 'r2_transposon' } == \\ set ( list ( component_dict . keys ())): raise ValueError ( \"component_dict must be a dictionary \" \"where the keys are 'tf', 'r1_primers', \" \"'r2_transposons' and the values are \" \"lists of the same length. The index of \" \"each list corresponds to the same \" \"transcription factor.\" ) for k , v in component_dict . items (): if not isinstance ( v , list ): raise TypeError ( \"component_dict values must be lists\" ) if len ({ len ( x ) for x in component_dict . values ()}) != 1 : raise ValueError ( \"component_dict values must be lists of \" \"the same length\" ) # extract summaries from the metrics r1_primer_summary , r2_transposon_summary = \\ self . _summarize_by_tf ( component_dict ) # write r1_primer_summary to file append_suffix = '_' + suffix if suffix else '' r1_primer_basename = \\ filename + \"_r1_primer_summary\" + append_suffix + \".csv\" r1_primer_summary_path = os . path . join ( output_dirpath , r1_primer_basename ) r1_primer_summary_df = pd . DataFrame ( r1_primer_summary ) logger . info ( \"writing r1_primer_summary \" \"to %s{r1_primer_summary_path} \" ) r1_primer_summary_df . to_csv ( r1_primer_summary_path , index = False ) # write r2_transposon summary to file r2_transposon_summary_basename = \\ filename + \"_r2_transposon_summary\" + append_suffix + \".csv\" r2_transposon_summary_path = os . path . join ( output_dirpath , r2_transposon_summary_basename ) r2_transposon_summary_df = pd . DataFrame ( r2_transposon_summary ) logger . info ( \"writing r2_transposon_summary \" \"to %s{r2_transposon_summary_path} \" ) r2_transposon_summary_df . to_csv ( r2_transposon_summary_path , index = False )","title":"BarcodeCounterQc"},{"location":"API/BarcodeParser/yeast/BarcodeCounterQc/#callingcardstools.BarcodeParser.yeast.BarcodeQcCounter.BarcodeQcCounter","text":"A class for counting and processing barcode quality control data. Attributes: Name Type Description metrics DefaultDict A nested defaultdict containing the barcode metrics. r1_transposon_seq_dict DefaultDict A defaultdict storing the R1 transposon sequences. Source code in callingcardstools/BarcodeParser/yeast/BarcodeQcCounter.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 class BarcodeQcCounter : \"\"\"A class for counting and processing barcode quality control data. Attributes: metrics (DefaultDict): A nested defaultdict containing the barcode metrics. r1_transposon_seq_dict (DefaultDict): A defaultdict storing the R1 transposon sequences. \"\"\" _metrics : DefaultDict _r1_transposon_seq_dict : DefaultDict def __init__ ( self , pickle_path : str = None ) -> None : \"\"\"Initializes a BarcodeQcCounter instance. Args: pickle_path (str, optional): Path to a pickled BarcodeQcCounter object. If provided, loads the object from the file. Defaults to None. Raises: FileNotFoundError: If the provided pickle path does not exist. \"\"\" if pickle_path : if not os . path . exists ( pickle_path ): msg = f \"Path to pickle file { pickle_path } does not exist\" raise FileNotFoundError ( msg ) self . load ( pickle_path ) else : self . _metrics = OuterDefaultDict ( int ) self . _r1_transposon_seq_dict = defaultdict ( set ) @property def metrics ( self ) -> defaultdict : \"\"\"Returns the _metrics attribute. Returns: defaultdict: The _metrics attribute. \"\"\" return self . _metrics @property def r1_transposon_dict ( self ) -> defaultdict : \"\"\"Returns the _r1_transposon_seq_dict attribute. Returns: defaultdict: the _r1_transposon_seq_dict attribute. \"\"\" return self . _r1_transposon_seq_dict # private methods --------------------------------------------------------- def _combine ( self , other : \"BarcodeQcCounter\" ) -> None : \"\"\"Combine the metrics from another BarcodeQcCounter object. Args: other (BarcodeQcCounter): Another BarcodeQcCounter object whose metrics will be combined with this object. \"\"\" # Combine _metrics dictionaries for r1_transposon_edit_dist , r1_primer_dict in other . metrics . items (): for r1_primer_seq , r2_transposon_dict in r1_primer_dict . items (): for r2_transposon_seq , r2_restriction_enzyme_dict in \\ r2_transposon_dict . items (): for r2_restriction_enzyme_name , count in \\ r2_restriction_enzyme_dict . items (): ( self . _metrics [ r1_transposon_edit_dist ] [ r1_primer_seq ] [ r2_transposon_seq ] [ r2_restriction_enzyme_name ]) += count # Combine _r1_transposon_seq_dict dictionaries for r1_transposon_edit_dist , r1_transposon_seq_set in \\ other . r1_transposon_dict . items (): self . _r1_transposon_seq_dict [ r1_transposon_edit_dist ] \\ . update ( r1_transposon_seq_set ) # public methods ---------------------------------------------------------- def load ( self , file_path : str ) -> None : \"\"\"Load a BarcodeQcCounter object from a file using Pickle. Args: file_path (str): The file path where the object is stored. \"\"\" logger . info ( \"loading BarcodeQcCounter object from %s \" , file_path ) with open ( file_path , \"rb\" ) as file : file_data = pickle . load ( file ) if not isinstance ( file_data , BarcodeQcCounter ): raise TypeError ( f \" { file_path } is not a BarcodeQcCounter object\" ) # copy the data from the loaded object to the current instance self . _metrics = file_data . _metrics self . _r1_transposon_seq_dict = file_data . _r1_transposon_seq_dict @classmethod def combine ( cls , counters : Iterable [ \"BarcodeQcCounter\" ]) -> \"BarcodeQcCounter\" : \"\"\"Combine multiple BarcodeQcCounter objects into a single object. Args: counters (Iterable[BarcodeQcCounter]): An iterable of BarcodeQcCounter objects. Returns: BarcodeQcCounter: A new BarcodeQcCounter object with the combined metrics. \"\"\" result = BarcodeQcCounter () for counter in counters : result . _combine ( counter ) return result def __add__ ( self , other : \"BarcodeQcCounter\" ) -> \"BarcodeQcCounter\" : \"\"\"Add two BarcodeQcCounter objects together with the + operator.\"\"\" if not isinstance ( other , BarcodeQcCounter ): raise TypeError ( \"Both objects must be of type 'BarcodeQcCounter'\" ) result = BarcodeQcCounter () result . combine ( self ) result . combine ( other ) return result def update ( self , component_tuple : tuple , r1_transposon_edit_dist : int , r2_restriction_enzyme_name : str ) -> None : \"\"\"Updates the metrics with given component and deviation tuples. Args: component_tuple (tuple): A tuple containing R1 primer, R1 transposon, and R2 transposon sequences. r1_transposon_edit_dist (int): The edit distance between the R1 transposon sequence and the expected R1 transposon r2_restriction_enzyme_name (str): The R2 restriction enzyme name. \"\"\" ( r1_primer_seq , r1_transposon_seq , r2_transposon_seq ) = component_tuple ( self . _metrics [ r1_transposon_edit_dist ] [ r1_primer_seq ] [ r2_transposon_seq ] [ r2_restriction_enzyme_name ]) += 1 self . _r1_transposon_seq_dict [ r1_transposon_edit_dist ] \\ . add ( r1_transposon_seq ) def _summarize_by_tf ( self , component_dict : dict ) -> None : \"\"\"Summarizes the metrics by transcription factor (TF). Args: component_dict (dict): A dictionary containing keys for 'tf', 'r1_primers', and 'r2_transposons', and their respective lists of values. Returns: tuple: A tuple containing R1 primer summary and R2 transposon summary. \"\"\" # r1_primer_summary = [] r2_transposon_summary = [] # only iterate over those reads which had an r1 transposon seq # edit distance of n or less # r1_for_given_r2_dict = defaultdict(lambda: defaultdict(set)) r1_for_given_r2_dict = MiddleDefaultDict1 ( set ) for i , r1_transposon_dict in self . _metrics . items (): # first level of iteration is over the r1 primer keys. # The dictionary is a nested dictionary with the keys being # r2_transposon sequences and values another dicitonary with # the restriciton enzyme and count for r1_primer_seq , r1_primer_dict in r1_transposon_dict . items (): # if the r1 primer sequence is the expected sequence # for a given tf, then iterate over the r2 transposon # entries and record the results for r2_transposon_seq , r2_transposon_seq_dict in \\ r1_primer_dict . items (): # if the r2_transposon_seq is recognized, then save the # r1_primer_seq. structure of the dict is: # {'valid_r2_trans_seq': set(r1_primer_seq1, ...)} if r2_transposon_seq in \\ component_dict [ 'r2_transposon' ]: ( r1_for_given_r2_dict [ i ] [ r2_transposon_seq ] . add ( r1_primer_seq )) # if the r1_primer_seq is an expected sequence, then # iterate over the r2_transposon_seq_dict and record the # results if r1_primer_seq in component_dict [ 'r1_primer' ]: r1_primer_index = \\ component_dict [ 'r1_primer' ] . index ( r1_primer_seq ) r2_transposon_target_seq = \\ component_dict [ 'r2_transposon' ][ r1_primer_index ] edit_dist = \\ align ( r2_transposon_seq , r2_transposon_target_seq ) r1_primer_record = { \"tf\" : component_dict [ 'tf' ][ r1_primer_index ], \"r1_primer_seq\" : component_dict [ 'r1_primer' ][ r1_primer_index ], \"r1_transposon_edit_dist\" : i , \"r2_transposon_seq\" : r2_transposon_seq , \"r2_transposon_edit_dist\" : edit_dist . get ( \"editDistance\" )} for restriction_enzyme , count in \\ r2_transposon_seq_dict . items (): record_copy = r1_primer_record . copy () record_copy . update ({ 'restriction_enzyme' : restriction_enzyme , 'count' : count }) r1_primer_summary . append ( record_copy ) # in the second iteration, iterate over only those r1_primer_seqs with # a valid r2_transposon_seq for r1_transposon_ed , r1_transposon_ed_dict in \\ r1_for_given_r2_dict . items (): for r2_transposon_seq , r1_primer_seq_set in \\ r1_transposon_ed_dict . items (): # extract the TF and expected r1_primer sequence for this # r2_transposon_seq and TF index = component_dict [ 'r2_transposon' ] \\ . index ( r2_transposon_seq ) tf = component_dict [ 'tf' ][ index ] r1_primer_expected = component_dict [ \"r1_primer\" ][ index ] # iterate over all of the `r1_primer_seq` for this # r2_transposon_seq for r1_primer_query in r1_primer_seq_set : # align the r1_primer to the expected r1_primer for this # r2_transposon_seq and TF edit_dist = align ( r1_primer_query , r1_primer_expected ) # create the base record r2_transposon_record = { \"tf\" : tf , \"r2_transposon_seq\" : r2_transposon_seq , \"r1_transposon_edit_dist\" : r1_transposon_ed , \"r1_primer_seq\" : r1_primer_query , \"r1_primer_edit_dist\" : edit_dist . get ( \"editDistance\" )} for restriction_enzyme , count in \\ ( self . _metrics [ r1_transposon_ed ] [ r1_primer_query ] [ r2_transposon_seq ] . items ()): # make a copy of the record record_copy = r2_transposon_record . copy () # add additional restriction enzyme info record_copy . update ({ 'restriction_enzyme' : restriction_enzyme , 'count' : count }) r2_transposon_summary . append ( record_copy ) return r1_primer_summary , r2_transposon_summary def write ( self , raw : bool = False , component_dict : dict = None , output_dirpath : str = \".\" , filename : str = \"barcode_qc\" , suffix : str = \"\" ) -> None : \"\"\"Write a pickle and/or a comma-delimited file summarizing the barcode QC metrics. Args: raw (bool, optional): If True, pickles the object. Defaults to False. component_dict (dict, optional): A dictionary containing keys for 'tf', 'r1_primers', and 'r2_transposons', and their respective lists of values. If provided, writes summaries for each component. Defaults to None. output_dirpath (str, optional): The output directory path where the files will be saved. Defaults to the current directory. filename (str, optional): The base filename for the output files. Defaults to \"barcode_qc\". suffix (str, optional): A suffix to be appended to the base filename. Defaults to an empty string. \"\"\" # check that the output_dirpath is a valid directory if not os . path . join ( output_dirpath ): raise ValueError ( \"output_dirpath must be a valid directory\" ) # if raw is true, then pickle the object if raw : pickle_path = os . path . join ( output_dirpath , filename + '_' + suffix + \".pickle\" ) logger . info ( \"pickling barcode_qc object to %s{pick_path} \" ) with open ( pickle_path , \"wb\" ) as pickle_file : pickle . dump ( self , pickle_file ) # if component_dict is passed if component_dict : # input checks if not isinstance ( component_dict , dict ): raise TypeError ( \"component_dict must be a dictionary\" ) if not { 'tf' , 'r1_primer' , 'r2_transposon' } == \\ set ( list ( component_dict . keys ())): raise ValueError ( \"component_dict must be a dictionary \" \"where the keys are 'tf', 'r1_primers', \" \"'r2_transposons' and the values are \" \"lists of the same length. The index of \" \"each list corresponds to the same \" \"transcription factor.\" ) for k , v in component_dict . items (): if not isinstance ( v , list ): raise TypeError ( \"component_dict values must be lists\" ) if len ({ len ( x ) for x in component_dict . values ()}) != 1 : raise ValueError ( \"component_dict values must be lists of \" \"the same length\" ) # extract summaries from the metrics r1_primer_summary , r2_transposon_summary = \\ self . _summarize_by_tf ( component_dict ) # write r1_primer_summary to file append_suffix = '_' + suffix if suffix else '' r1_primer_basename = \\ filename + \"_r1_primer_summary\" + append_suffix + \".csv\" r1_primer_summary_path = os . path . join ( output_dirpath , r1_primer_basename ) r1_primer_summary_df = pd . DataFrame ( r1_primer_summary ) logger . info ( \"writing r1_primer_summary \" \"to %s{r1_primer_summary_path} \" ) r1_primer_summary_df . to_csv ( r1_primer_summary_path , index = False ) # write r2_transposon summary to file r2_transposon_summary_basename = \\ filename + \"_r2_transposon_summary\" + append_suffix + \".csv\" r2_transposon_summary_path = os . path . join ( output_dirpath , r2_transposon_summary_basename ) r2_transposon_summary_df = pd . DataFrame ( r2_transposon_summary ) logger . info ( \"writing r2_transposon_summary \" \"to %s{r2_transposon_summary_path} \" ) r2_transposon_summary_df . to_csv ( r2_transposon_summary_path , index = False )","title":"BarcodeQcCounter"},{"location":"API/BarcodeParser/yeast/BarcodeCounterQc/#callingcardstools.BarcodeParser.yeast.BarcodeQcCounter.BarcodeQcCounter.metrics","text":"Returns the _metrics attribute. Returns: Name Type Description defaultdict defaultdict The _metrics attribute.","title":"metrics"},{"location":"API/BarcodeParser/yeast/BarcodeCounterQc/#callingcardstools.BarcodeParser.yeast.BarcodeQcCounter.BarcodeQcCounter.r1_transposon_dict","text":"Returns the _r1_transposon_seq_dict attribute. Returns: Name Type Description defaultdict defaultdict the _r1_transposon_seq_dict attribute.","title":"r1_transposon_dict"},{"location":"API/BarcodeParser/yeast/BarcodeCounterQc/#callingcardstools.BarcodeParser.yeast.BarcodeQcCounter.BarcodeQcCounter.__add__","text":"Add two BarcodeQcCounter objects together with the + operator. Source code in callingcardstools/BarcodeParser/yeast/BarcodeQcCounter.py 163 164 165 166 167 168 169 170 171 def __add__ ( self , other : \"BarcodeQcCounter\" ) -> \"BarcodeQcCounter\" : \"\"\"Add two BarcodeQcCounter objects together with the + operator.\"\"\" if not isinstance ( other , BarcodeQcCounter ): raise TypeError ( \"Both objects must be of type 'BarcodeQcCounter'\" ) result = BarcodeQcCounter () result . combine ( self ) result . combine ( other ) return result","title":"__add__()"},{"location":"API/BarcodeParser/yeast/BarcodeCounterQc/#callingcardstools.BarcodeParser.yeast.BarcodeQcCounter.BarcodeQcCounter.__init__","text":"Initializes a BarcodeQcCounter instance. Parameters: Name Type Description Default pickle_path str Path to a pickled BarcodeQcCounter object. If provided, loads the object from the file. Defaults to None. None Raises: Type Description FileNotFoundError If the provided pickle path does not exist. Source code in callingcardstools/BarcodeParser/yeast/BarcodeQcCounter.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , pickle_path : str = None ) -> None : \"\"\"Initializes a BarcodeQcCounter instance. Args: pickle_path (str, optional): Path to a pickled BarcodeQcCounter object. If provided, loads the object from the file. Defaults to None. Raises: FileNotFoundError: If the provided pickle path does not exist. \"\"\" if pickle_path : if not os . path . exists ( pickle_path ): msg = f \"Path to pickle file { pickle_path } does not exist\" raise FileNotFoundError ( msg ) self . load ( pickle_path ) else : self . _metrics = OuterDefaultDict ( int ) self . _r1_transposon_seq_dict = defaultdict ( set )","title":"__init__()"},{"location":"API/BarcodeParser/yeast/BarcodeCounterQc/#callingcardstools.BarcodeParser.yeast.BarcodeQcCounter.BarcodeQcCounter.combine","text":"Combine multiple BarcodeQcCounter objects into a single object. Parameters: Name Type Description Default counters Iterable [ BarcodeQcCounter ] An iterable of BarcodeQcCounter objects. required Returns: Name Type Description BarcodeQcCounter BarcodeQcCounter A new BarcodeQcCounter object with the combined metrics. Source code in callingcardstools/BarcodeParser/yeast/BarcodeQcCounter.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 @classmethod def combine ( cls , counters : Iterable [ \"BarcodeQcCounter\" ]) -> \"BarcodeQcCounter\" : \"\"\"Combine multiple BarcodeQcCounter objects into a single object. Args: counters (Iterable[BarcodeQcCounter]): An iterable of BarcodeQcCounter objects. Returns: BarcodeQcCounter: A new BarcodeQcCounter object with the combined metrics. \"\"\" result = BarcodeQcCounter () for counter in counters : result . _combine ( counter ) return result","title":"combine()"},{"location":"API/BarcodeParser/yeast/BarcodeCounterQc/#callingcardstools.BarcodeParser.yeast.BarcodeQcCounter.BarcodeQcCounter.load","text":"Load a BarcodeQcCounter object from a file using Pickle. Parameters: Name Type Description Default file_path str The file path where the object is stored. required Source code in callingcardstools/BarcodeParser/yeast/BarcodeQcCounter.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def load ( self , file_path : str ) -> None : \"\"\"Load a BarcodeQcCounter object from a file using Pickle. Args: file_path (str): The file path where the object is stored. \"\"\" logger . info ( \"loading BarcodeQcCounter object from %s \" , file_path ) with open ( file_path , \"rb\" ) as file : file_data = pickle . load ( file ) if not isinstance ( file_data , BarcodeQcCounter ): raise TypeError ( f \" { file_path } is not a BarcodeQcCounter object\" ) # copy the data from the loaded object to the current instance self . _metrics = file_data . _metrics self . _r1_transposon_seq_dict = file_data . _r1_transposon_seq_dict","title":"load()"},{"location":"API/BarcodeParser/yeast/BarcodeCounterQc/#callingcardstools.BarcodeParser.yeast.BarcodeQcCounter.BarcodeQcCounter.update","text":"Updates the metrics with given component and deviation tuples. Parameters: Name Type Description Default component_tuple tuple A tuple containing R1 primer, R1 transposon, and R2 transposon sequences. required r1_transposon_edit_dist int The edit distance between the R1 transposon sequence and the expected R1 transposon required r2_restriction_enzyme_name str The R2 restriction enzyme name. required Source code in callingcardstools/BarcodeParser/yeast/BarcodeQcCounter.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def update ( self , component_tuple : tuple , r1_transposon_edit_dist : int , r2_restriction_enzyme_name : str ) -> None : \"\"\"Updates the metrics with given component and deviation tuples. Args: component_tuple (tuple): A tuple containing R1 primer, R1 transposon, and R2 transposon sequences. r1_transposon_edit_dist (int): The edit distance between the R1 transposon sequence and the expected R1 transposon r2_restriction_enzyme_name (str): The R2 restriction enzyme name. \"\"\" ( r1_primer_seq , r1_transposon_seq , r2_transposon_seq ) = component_tuple ( self . _metrics [ r1_transposon_edit_dist ] [ r1_primer_seq ] [ r2_transposon_seq ] [ r2_restriction_enzyme_name ]) += 1 self . _r1_transposon_seq_dict [ r1_transposon_edit_dist ] \\ . add ( r1_transposon_seq )","title":"update()"},{"location":"API/BarcodeParser/yeast/BarcodeCounterQc/#callingcardstools.BarcodeParser.yeast.BarcodeQcCounter.BarcodeQcCounter.write","text":"Write a pickle and/or a comma-delimited file summarizing the barcode QC metrics. Parameters: Name Type Description Default raw bool If True, pickles the object. Defaults to False. False component_dict dict A dictionary containing keys for \u2018tf\u2019, \u2018r1_primers\u2019, and \u2018r2_transposons\u2019, and their respective lists of values. If provided, writes summaries for each component. Defaults to None. None output_dirpath str The output directory path where the files will be saved. Defaults to the current directory. '.' filename str The base filename for the output files. Defaults to \u201cbarcode_qc\u201d. 'barcode_qc' suffix str A suffix to be appended to the base filename. Defaults to an empty string. '' Source code in callingcardstools/BarcodeParser/yeast/BarcodeQcCounter.py 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 def write ( self , raw : bool = False , component_dict : dict = None , output_dirpath : str = \".\" , filename : str = \"barcode_qc\" , suffix : str = \"\" ) -> None : \"\"\"Write a pickle and/or a comma-delimited file summarizing the barcode QC metrics. Args: raw (bool, optional): If True, pickles the object. Defaults to False. component_dict (dict, optional): A dictionary containing keys for 'tf', 'r1_primers', and 'r2_transposons', and their respective lists of values. If provided, writes summaries for each component. Defaults to None. output_dirpath (str, optional): The output directory path where the files will be saved. Defaults to the current directory. filename (str, optional): The base filename for the output files. Defaults to \"barcode_qc\". suffix (str, optional): A suffix to be appended to the base filename. Defaults to an empty string. \"\"\" # check that the output_dirpath is a valid directory if not os . path . join ( output_dirpath ): raise ValueError ( \"output_dirpath must be a valid directory\" ) # if raw is true, then pickle the object if raw : pickle_path = os . path . join ( output_dirpath , filename + '_' + suffix + \".pickle\" ) logger . info ( \"pickling barcode_qc object to %s{pick_path} \" ) with open ( pickle_path , \"wb\" ) as pickle_file : pickle . dump ( self , pickle_file ) # if component_dict is passed if component_dict : # input checks if not isinstance ( component_dict , dict ): raise TypeError ( \"component_dict must be a dictionary\" ) if not { 'tf' , 'r1_primer' , 'r2_transposon' } == \\ set ( list ( component_dict . keys ())): raise ValueError ( \"component_dict must be a dictionary \" \"where the keys are 'tf', 'r1_primers', \" \"'r2_transposons' and the values are \" \"lists of the same length. The index of \" \"each list corresponds to the same \" \"transcription factor.\" ) for k , v in component_dict . items (): if not isinstance ( v , list ): raise TypeError ( \"component_dict values must be lists\" ) if len ({ len ( x ) for x in component_dict . values ()}) != 1 : raise ValueError ( \"component_dict values must be lists of \" \"the same length\" ) # extract summaries from the metrics r1_primer_summary , r2_transposon_summary = \\ self . _summarize_by_tf ( component_dict ) # write r1_primer_summary to file append_suffix = '_' + suffix if suffix else '' r1_primer_basename = \\ filename + \"_r1_primer_summary\" + append_suffix + \".csv\" r1_primer_summary_path = os . path . join ( output_dirpath , r1_primer_basename ) r1_primer_summary_df = pd . DataFrame ( r1_primer_summary ) logger . info ( \"writing r1_primer_summary \" \"to %s{r1_primer_summary_path} \" ) r1_primer_summary_df . to_csv ( r1_primer_summary_path , index = False ) # write r2_transposon summary to file r2_transposon_summary_basename = \\ filename + \"_r2_transposon_summary\" + append_suffix + \".csv\" r2_transposon_summary_path = os . path . join ( output_dirpath , r2_transposon_summary_basename ) r2_transposon_summary_df = pd . DataFrame ( r2_transposon_summary ) logger . info ( \"writing r2_transposon_summary \" \"to %s{r2_transposon_summary_path} \" ) r2_transposon_summary_df . to_csv ( r2_transposon_summary_path , index = False )","title":"write()"},{"location":"API/PeakCalling/yeast/enrichment_vectorized/","text":"Compute the Calling Cards effect (enrichment) for the given hops counts. :param total_background_hops: a pandas Series (column of a dataframe) of total number of hops in the background. :type total_background_hops: Series :param total_experiment_hops: a pandas Series (column of a dataframe) of total number of hops in the experiment. :type total_experiment_hops: Series :param background_hops: a pandas Series (column of a dataframe) of number of hops in the background by promoter region. :type background_hops: Series :param experiment_hops: a pandas Series (column of a dataframe) of number of hops in the experiment by promoter region. :type experiment_hops: Series :param pseudocount: Added to the background hops to avoid division by zero, :type pseudocount: float, optional :param kwargs: Additional keyword arguments. None are currently used :return: a pandas Series of length equal to the input Series with the Calling Cards effect (enrichment) value for each row. :rtype: Series Source code in callingcardstools/PeakCalling/yeast/enrichment_vectorized.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def enrichment_vectorized ( total_background_hops : Series , total_experiment_hops : Series , background_hops : Series , experiment_hops : Series , pseudocount : float = 0.1 , ** kwargs ) -> Series : \"\"\" Compute the Calling Cards effect (enrichment) for the given hops counts. :param total_background_hops: a pandas Series (column of a dataframe) of total number of hops in the background. :type total_background_hops: Series :param total_experiment_hops: a pandas Series (column of a dataframe) of total number of hops in the experiment. :type total_experiment_hops: Series :param background_hops: a pandas Series (column of a dataframe) of number of hops in the background by promoter region. :type background_hops: Series :param experiment_hops: a pandas Series (column of a dataframe) of number of hops in the experiment by promoter region. :type experiment_hops: Series :param pseudocount: Added to the background hops to avoid division by zero, :type pseudocount: float, optional :param kwargs: Additional keyword arguments. None are currently used :return: a pandas Series of length equal to the input Series with the Calling Cards effect (enrichment) value for each row. :rtype: Series \"\"\" # raise an error if any one of the 4 input Series is not a Series if not all ( isinstance ( x , Series ) for x in [ total_background_hops , total_experiment_hops , background_hops , experiment_hops , ] ): raise ValueError ( \"`total_background_hops`, `total_experiment_hops`, \" , \"`background_hops` and `experiment_hops` must all \" , \"be pandas Series. At least one is not.\" , ) # validate that all input Series are the same length if ( not len ( total_background_hops ) == len ( total_experiment_hops ) == len ( background_hops ) == len ( experiment_hops ) ): raise ValueError ( \"All input Series must be the same length.\" ) # validate that pseudocount is numeric (int or float). Cast to float if int if not isinstance ( pseudocount , ( int , float )): raise ValueError ( \"pseudocount must be a number.\" ) if isinstance ( pseudocount , int ): logger . warning ( \"pseudocount is an integer. It will be cast to a float.\" ) pseudocount = float ( pseudocount ) # NOTE: the total_experiment_hops and total_background_hops must be > 0 based on # input data verification. See read_in_experiment_data() # and read_in_background_data() in read_in_data.py numerator = experiment_hops / total_experiment_hops # Add a small pseudocount to background_hops to avoid division by zero in the # enrichment calculation below # Consider a `min` where the minimum value is 0.1/total_background_hops denominator = ( background_hops + pseudocount ) / total_background_hops enrichment = numerator / denominator # Check for invalid values if ( enrichment < 0 ) . any (): raise ValueError ( \"Enrichment values must be non-negative.\" ) if enrichment . isnull () . any (): raise ValueError ( \"Enrichment values must not be NaN.\" ) if np . isinf ( enrichment ) . any (): raise ValueError ( \"Enrichment values must not be infinite.\" ) return enrichment","title":"enrichment_vectorized"},{"location":"API/PeakCalling/yeast/hypergeom_pval_vectorized/","text":"Compute the hypergeometric p-value for the given hops counts. :param total_background_hops: a pandas Series (column of a dataframe) of total number of hops in the background. :type total_background_hops: Series[int64] :param total_experiment_hops: a pandas Series (column of a dataframe) of total number of hops in the experiment. :type total_experiment_hops: Series[int64] :param background_hops: a pandas Series (column of a dataframe) of number of hops in the background by promoter region. :type background_hops: Series[int64] :param experiment_hops: a pandas Series (column of a dataframe) of number of hops in the experiment by promoter region. :type experiment_hops: Series[int64] :return: A pandas Series of length equal to the input Series with the hypergeometric p-value for each row. If either of the total hop input Series is 0, the hypergeometric p-value is undefined and the output Series will have a value of 1 for that row. :rtype: NDArray[float] .. note:: This function is vectorized, so it can be applied to pandas Series (columns of dataframes) to compute the hypergeometric p-value for each row. :raises ValueError: If any of the input Series contain negative values, are not dtype int64 or the input Series are not all the same length. :Example: import pandas as pd total_background_hops = pd.Series([100, 200, 300]) total_experiment_hops = pd.Series([10, 20, 30]) background_hops = pd.Series([5, 10, 15]) experiment_hops = pd.Series([2, 4, 6]) vectorized_hypergeom_pval( \u2026 total_background_hops, \u2026 total_experiment_hops, \u2026 background_hops, \u2026 experiment_hops) 0 0.122360 1 0.027644 2 0.006972 dtype: float64 Source code in callingcardstools/PeakCalling/yeast/hypergeom_pval_vectorized.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def hypergeom_pval_vectorized ( total_background_hops : Series , total_experiment_hops : Series , background_hops : Series , experiment_hops : Series , ) -> NDArray : \"\"\" Compute the hypergeometric p-value for the given hops counts. :param total_background_hops: a pandas Series (column of a dataframe) of total number of hops in the background. :type total_background_hops: Series[int64] :param total_experiment_hops: a pandas Series (column of a dataframe) of total number of hops in the experiment. :type total_experiment_hops: Series[int64] :param background_hops: a pandas Series (column of a dataframe) of number of hops in the background by promoter region. :type background_hops: Series[int64] :param experiment_hops: a pandas Series (column of a dataframe) of number of hops in the experiment by promoter region. :type experiment_hops: Series[int64] :return: A pandas Series of length equal to the input Series with the hypergeometric p-value for each row. If either of the `total hop` input Series is 0, the hypergeometric p-value is undefined and the output Series will have a value of 1 for that row. :rtype: NDArray[float] .. note:: This function is vectorized, so it can be applied to pandas Series (columns of dataframes) to compute the hypergeometric p-value for each row. :raises ValueError: If any of the input Series contain negative values, are not dtype int64 or the input Series are not all the same length. :Example: >>> import pandas as pd >>> total_background_hops = pd.Series([100, 200, 300]) >>> total_experiment_hops = pd.Series([10, 20, 30]) >>> background_hops = pd.Series([5, 10, 15]) >>> experiment_hops = pd.Series([2, 4, 6]) >>> vectorized_hypergeom_pval( ... total_background_hops, ... total_experiment_hops, ... background_hops, ... experiment_hops) 0 0.122360 1 0.027644 2 0.006972 dtype: float64 \"\"\" # check input if ( not len ( total_background_hops ) == len ( total_experiment_hops ) == len ( background_hops ) == len ( experiment_hops ) ): raise ValueError ( \"All input Series must be the same length.\" ) if total_background_hops . min () < 0 or total_background_hops . dtype != \"int64\" : raise ValueError (( \"total_background_hops must \" \"be a non-negative integer.\" )) if total_experiment_hops . min () < 0 or total_background_hops . dtype != \"int64\" : raise ValueError (( \"total_experiment_hops must \" \"be a non-negative integer\" )) if background_hops . min () < 0 or background_hops . dtype != \"int64\" : raise ValueError ( \"background_hops must be a non-negative integer\" ) if experiment_hops . min () < 0 or experiment_hops . dtype != \"int64\" : raise ValueError ( \"experiment_hops must be a non-negative integer\" ) # calculate hypergeometric p-values M = total_background_hops + total_experiment_hops n = total_experiment_hops N = background_hops + experiment_hops x = experiment_hops - 1 # Handling edge cases valid = ( M >= 1 ) & ( N >= 1 ) pval = Series ( 1 , index = total_background_hops . index ) pval [ valid ] = 1 - hypergeom . cdf ( x [ valid ], M [ valid ], n [ valid ], N [ valid ]) return pval","title":"hypergeom_pval_vectorized"},{"location":"API/PeakCalling/yeast/poisson_pval_vectorized/","text":"Compute the Poisson p-value for the given hops counts. :param total_background_hops: a pandas Series (column of a dataframe) of total number of hops in the background. :type total_background_hops: Series[int64] :param total_experiment_hops: a pandas Series (column of a dataframe) of total number of hops in the experiment. :type total_experiment_hops: Series[int64] :param background_hops: a pandas Series (column of a dataframe) of number of hops in the background by promoter region. :type background_hops: Series[int64] :param experiment_hops: a pandas Series (column of a dataframe) of number of hops in the experiment by promoter region. :type experiment_hops: Series[int64] :param pseudocount: , defaults to 1.0 :type pseudocount: float, optional :return: a pandas Series of length equal to the input Series with the Poisson p-value for each row. :rtype: Series[float] .. note:: This function is vectorized, so it can be applied to pandas Series (columns of dataframes) to compute the Poisson p-value for each row. :raises ValueError: If any of the input Series contain negative values or the input Series are not all the same length. :Example: import pandas as pd total_background_hops = pd.Series([100, 200, 300]) total_experiment_hops = pd.Series([10, 20, 30]) background_hops = pd.Series([5, 10, 15]) experiment_hops = pd.Series([2, 4, 6]) vectorized_poisson_pval( \u2026 total_background_hops, \u2026 total_experiment_hops, \u2026 background_hops, \u2026 experiment_hops) array([0.01438768, 0.00365985, 0.00092599]) Source code in callingcardstools/PeakCalling/yeast/poisson_pval_vectorized.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def poisson_pval_vectorized ( total_background_hops : Series , total_experiment_hops : Series , background_hops : Series , experiment_hops : Series , pseudocount : float = 0.1 , ** kwargs ) -> Series : \"\"\" Compute the Poisson p-value for the given hops counts. :param total_background_hops: a pandas Series (column of a dataframe) of total number of hops in the background. :type total_background_hops: Series[int64] :param total_experiment_hops: a pandas Series (column of a dataframe) of total number of hops in the experiment. :type total_experiment_hops: Series[int64] :param background_hops: a pandas Series (column of a dataframe) of number of hops in the background by promoter region. :type background_hops: Series[int64] :param experiment_hops: a pandas Series (column of a dataframe) of number of hops in the experiment by promoter region. :type experiment_hops: Series[int64] :param pseudocount: , defaults to 1.0 :type pseudocount: float, optional :return: a pandas Series of length equal to the input Series with the Poisson p-value for each row. :rtype: Series[float] .. note:: This function is vectorized, so it can be applied to pandas Series (columns of dataframes) to compute the Poisson p-value for each row. :raises ValueError: If any of the input Series contain negative values or the input Series are not all the same length. :Example: >>> import pandas as pd >>> total_background_hops = pd.Series([100, 200, 300]) >>> total_experiment_hops = pd.Series([10, 20, 30]) >>> background_hops = pd.Series([5, 10, 15]) >>> experiment_hops = pd.Series([2, 4, 6]) >>> vectorized_poisson_pval( ... total_background_hops, ... total_experiment_hops, ... background_hops, ... experiment_hops) array([0.01438768, 0.00365985, 0.00092599]) \"\"\" # check input if ( not len ( total_background_hops ) == len ( total_experiment_hops ) == len ( background_hops ) == len ( experiment_hops ) ): raise ValueError ( \"All input Series must be the same length.\" ) if total_background_hops . min () < 0 or total_background_hops . dtype != \"int64\" : raise ValueError (( \"total_background_hops must \" \"be a non-negative integer.\" )) if total_experiment_hops . min () < 0 or total_background_hops . dtype != \"int64\" : raise ValueError (( \"total_experiment_hops must \" \"be a non-negative integer\" )) # cast to `float` b/c of scipy # note that read_in_background_data and read_in_experiment_data in # read_in_data.py require that there be at least 1 hop in both the background # and the experiment. Therefore the total_background_hops and total_experiment_hops # is always defined hop_ratio = ( total_experiment_hops / total_background_hops ) . astype ( \"float\" ) # It is possible that there are promoters with no background hops. Add a small # pseudocount to require that mu > 0, per poisson definition mu = (( background_hops + pseudocount ) * hop_ratio ) . astype ( \"float\" ) # there has been a pseudocount added to experiment hops. Not necessary, removed # 20240624 # The way this is calculated, with pyranges and a sum, this value will always be # at minimum 0 x = experiment_hops . astype ( \"float\" ) # 20250306: The p-value is calculated as the probability of observing x or more # hops given the expected number of hops. This is equal to 1 - the cumulative # distribution function (CDF) of the Poisson distribution at x, plus the # probability mass function (PMF) at x. This is a change from Rob's original code # and the code in callingCardsTools # the resolution in the CDF is very low, so this ends up being the PMF value. # However, by inspection, the values after `x` are an order of magnitude or more # smaller, so the pvalue is dominated by the first term. pval = ( 1 - poisson . cdf ( x , mu )) + poisson . pmf ( x , mu ) # return the pvalue as a pandas Series return Series ( pval )","title":"poisson_pval_vectorized"},{"location":"API/PeakCalling/yeast/call_peaks/add_metrics/","text":"Add Calling Cards metrics to the given DataFrame. The kwargs parameter is used to pass additional arguments into underlying functions. Currently, the following are configured: - pseudocount: pseudocount to use when calculating both the enrichment and poisson pvalue. See either function for more documentation. Passed through kwargs so that if none is passed to add_metrics, the default in enrichment_vectorized() and poisson_pval_vectorized() is used. :param dataframe: a pandas DataFrame of promoter regions. :type dataframe: DataFrame :return: a pandas DataFrame of promoter regions with Calling Cards metrics. :rtype: DataFrame Source code in callingcardstools/PeakCalling/yeast/call_peaks.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 def add_metrics ( dataframe : pd . DataFrame , ** kwargs ) -> pd . DataFrame : \"\"\" Add Calling Cards metrics to the given DataFrame. The kwargs parameter is used to pass additional arguments into underlying functions. Currently, the following are configured: - pseudocount: pseudocount to use when calculating both the enrichment and poisson pvalue. See either function for more documentation. Passed through kwargs so that if none is passed to add_metrics, the default in enrichment_vectorized() and poisson_pval_vectorized() is used. :param dataframe: a pandas DataFrame of promoter regions. :type dataframe: DataFrame :return: a pandas DataFrame of promoter regions with Calling Cards metrics. :rtype: DataFrame \"\"\" dataframe [ \"callingcards_enrichment\" ] = enrichment_vectorized ( dataframe [ \"background_total_hops\" ], dataframe [ \"experiment_total_hops\" ], dataframe [ \"background_hops\" ], dataframe [ \"experiment_hops\" ], ** kwargs , ) dataframe [ \"poisson_pval\" ] = poisson_pval_vectorized ( dataframe [ \"background_total_hops\" ], dataframe [ \"experiment_total_hops\" ], dataframe [ \"background_hops\" ], dataframe [ \"experiment_hops\" ], ** kwargs , ) dataframe [ \"hypergeometric_pval\" ] = hypergeom_pval_vectorized ( dataframe [ \"background_total_hops\" ], dataframe [ \"experiment_total_hops\" ], dataframe [ \"background_hops\" ], dataframe [ \"experiment_hops\" ], ) return dataframe","title":"add_metrics"},{"location":"API/PeakCalling/yeast/call_peaks/call_peaks/","text":"Call peaks for the given Calling Cards data. The kwargs parameter is used to pass additional arguments into underlying functions. Currently, the following are configured: - pranges_rename_dict: a dictionary that maps the column names in the promoter data to the column names in the PyRanges object. This is used to rename the columns in the PyRanges object after the promoter data is read in. The default is {\u201cchr\u201d: \u201cChromosome\u201d, \u201cstart\u201d: \u201cStart\u201d, \u201cend\u201d: \u201cEnd\u201d, \u201cstrand\u201d: \u201cStrand\u201d}. - join_validate: the validation method to use when joining the promoter data with the experiment and background data. The default is \u201cone_to_one\u201d. - background_total_hops: the total number of hops in the background data. The default is the number of hops in the background data, calculated from the input background data file - experiment_total_hops: the total number of hops in the experiment data. The default is the number of hops in the experiment data, calculated from the input experiment data file - genomic_only: set this flag to include only genomic chromosomes in the experiment and background. See read_in_ _data for more details. Passed in kwargs so that if none is passed to add_metrics, the default in enrichment_vectorized() and poisson_pval_vectorized() is used. - pseudocount: pseudocount to use when calculating enrichment and poisson pvalue. See either function for more documentation. Passed through kwargs so that if none is passed to add_metrics, the default in enrichment_vectorized() and poisson_pval_vectorized() is used. :param experiment_data_paths: path(s) to the hops (experiment) data file(s). If multiple paths are provided, they will be concatenated, according to the deduplicate and genomic_only flags, prior to processing. On the concatenated data, however, the deduplicated flag is set to False , since within each file file the data was deduplicated, if it was set to True , and in the concatenated data, multiple hops at the same location is meaningful. :type experiment_data_paths: list :param experiment_orig_chr_convention: the chromosome naming convention used in the experiment data file. :type experiment_orig_chr_convention: str :param promoter_data_path: path to the promoter data file. :type promoter_data_path: str :param promoter_orig_chr_convention: the chromosome naming convention used in the promoter data file. :type promoter_orig_chr_convention: str :param background_data_path: path to the background data file. :type background_data_path: str :param background_orig_chr_convention: the chromosome naming convention used in the background data file. :type background_orig_chr_convention: str :param chrmap_data_path: path to the chromosome map file. :type chrmap_data_path: str :param deduplicate_experiment: If this is true, the experiment data will be deduplicated based on chr , start and end such that if an insertion is found at the same coordinate on different strands, only one of those records will be retained. see read_in_experiment_data for more details. :type deduplicate_experiment: bool :param unified_chr_convention: the chromosome naming convention to use in the output DataFrame. :type unified_chr_convention: str :return: a pandas DataFrame of promoter regions with Calling Cards metrics. :rtype: DataFrame Source code in callingcardstools/PeakCalling/yeast/call_peaks.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 def call_peaks ( experiment_data_paths : list , experiment_orig_chr_convention : str , promoter_data_path : str , promoter_orig_chr_convention : str , background_data_path : str , background_orig_chr_convention : str , chrmap_data_path : str , unified_chr_convention : str = \"ucsc\" , deduplicate_experiment : bool = True , ** kwargs , ) -> pd . DataFrame : \"\"\" Call peaks for the given Calling Cards data. The kwargs parameter is used to pass additional arguments into underlying functions. Currently, the following are configured: - pranges_rename_dict: a dictionary that maps the column names in the promoter data to the column names in the PyRanges object. This is used to rename the columns in the PyRanges object after the promoter data is read in. The default is {\"chr\": \"Chromosome\", \"start\": \"Start\", \"end\": \"End\", \"strand\": \"Strand\"}. - join_validate: the validation method to use when joining the promoter data with the experiment and background data. The default is \"one_to_one\". - background_total_hops: the total number of hops in the background data. The default is the number of hops in the background data, calculated from the input background data file - experiment_total_hops: the total number of hops in the experiment data. The default is the number of hops in the experiment data, calculated from the input experiment data file - genomic_only: set this flag to include only genomic chromosomes in the experiment and background. See read_in_<experiment/background>_data for more details. Passed in kwargs so that if none is passed to add_metrics, the default in enrichment_vectorized() and poisson_pval_vectorized() is used. - pseudocount: pseudocount to use when calculating enrichment and poisson pvalue. See either function for more documentation. Passed through kwargs so that if none is passed to add_metrics, the default in enrichment_vectorized() and poisson_pval_vectorized() is used. :param experiment_data_paths: path(s) to the hops (experiment) data file(s). If multiple paths are provided, they will be concatenated, according to the `deduplicate` and `genomic_only` flags, prior to processing. On the concatenated data, however, the `deduplicated` flag is set to `False`, since within each file file the data was deduplicated, if it was set to `True`, and in the concatenated data, multiple hops at the same location is meaningful. :type experiment_data_paths: list :param experiment_orig_chr_convention: the chromosome naming convention used in the experiment data file. :type experiment_orig_chr_convention: str :param promoter_data_path: path to the promoter data file. :type promoter_data_path: str :param promoter_orig_chr_convention: the chromosome naming convention used in the promoter data file. :type promoter_orig_chr_convention: str :param background_data_path: path to the background data file. :type background_data_path: str :param background_orig_chr_convention: the chromosome naming convention used in the background data file. :type background_orig_chr_convention: str :param chrmap_data_path: path to the chromosome map file. :type chrmap_data_path: str :param deduplicate_experiment: If this is true, the experiment data will be deduplicated based on `chr`, `start` and `end` such that if an insertion is found at the same coordinate on different strands, only one of those records will be retained. see `read_in_experiment_data` for more details. :type deduplicate_experiment: bool :param unified_chr_convention: the chromosome naming convention to use in the output DataFrame. :type unified_chr_convention: str :return: a pandas DataFrame of promoter regions with Calling Cards metrics. :rtype: DataFrame \"\"\" if not isinstance ( experiment_data_paths , list ): raise ValueError ( \"experiment_data_paths must be a list of paths to the experiment data files.\" ) if len ( experiment_data_paths ) > 1 : logger . info ( \"Multiple experiment data files provided. These will be concatenated. \" \"The concatenated data will not be deduplicated, but each file within \" \"the concatenated data will be deduplicated if the `deduplicate` \" \"flag is set to `True`.\" ) # Read in the chr map chrmap_df = read_in_chrmap ( chrmap_data_path , { experiment_orig_chr_convention , promoter_orig_chr_convention , background_orig_chr_convention , unified_chr_convention , }, ) # Read in the promoter and background data promoter_df = read_in_promoter_data ( promoter_data_path , promoter_orig_chr_convention , unified_chr_convention , chrmap_df , ) read_in_data_kwargs = {} if \"genomic_only\" in kwargs : read_in_data_kwargs [ \"genomic_only\" ] = kwargs [ \"genomic_only\" ] # Initialize containers for experiment data all_experiment_pr = [] all_experiment_total_hops = 0 # Process each experiment data file for experiment_data_path in experiment_data_paths : experiment_pr , experiment_total_hops = read_in_experiment_data ( experiment_data_path , experiment_orig_chr_convention , unified_chr_convention , chrmap_df , deduplicate_experiment , ** read_in_data_kwargs , ) all_experiment_pr . append ( experiment_pr ) all_experiment_total_hops += experiment_total_hops # Concatenate all experiment data concatenated_experiment_pr = pr . concat ( all_experiment_pr ) # Read and process the background data background_pr , background_total_hops = read_in_background_data ( background_data_path , background_orig_chr_convention , unified_chr_convention , chrmap_df , ** read_in_data_kwargs , ) pyranges_rename_dict = kwargs . get ( \"pranges_rename_dict\" , { \"chr\" : \"Chromosome\" , \"start\" : \"Start\" , \"end\" : \"End\" , \"strand\" : \"Strand\" }, ) promoters_pr = promoter_pyranges ( promoter_df , pyranges_rename_dict ) experiment_hops_df = count_hops ( promoters_pr , concatenated_experiment_pr , \"experiment_hops\" ) . set_index ( \"name\" , drop = True ) background_hops_df = count_hops ( promoters_pr , background_pr , \"background_hops\" ) . set_index ( \"name\" , drop = True ) promoter_hops_df = ( promoter_df . drop ( \"score\" , axis = 1 ) . set_index ( \"name\" ) . join ( [ experiment_hops_df , background_hops_df ], how = \"left\" , validate = kwargs . get ( \"join_validate\" , \"one_to_one\" ), ) . fillna ( 0 ) . assign ( background_total_hops = kwargs . get ( \"background_total_hops\" , background_total_hops ), experiment_total_hops = kwargs . get ( \"experiment_total_hops\" , all_experiment_total_hops ), ) . astype ( { \"background_hops\" : \"int64\" , \"experiment_hops\" : \"int64\" , \"background_total_hops\" : \"int64\" , \"experiment_total_hops\" : \"int64\" , } ) . reset_index () ) # Extract the add_metric kwargs if they are present add_metric_kwargs = {} if \"pseudocount\" in kwargs : add_metric_kwargs [ \"pseudocount\" ] = kwargs [ \"pseudocount\" ] start_time = time . time () result_df = add_metrics ( promoter_hops_df , ** add_metric_kwargs ) logger . debug ( \"Time taken to process %s promoters: %s seconds\" , len ( promoter_hops_df ), time . time () - start_time , ) return result_df","title":"call_peaks"},{"location":"API/PeakCalling/yeast/call_peaks/count_hops/","text":"Use pyranges to join the promoter regions with the qbed data and count the number of qbed records that overlap with each promoter. additional keyword arguments are passed to the join method of the PyRanges object. Currently, the following are configured: - slack: which defaults to 0 - suffix: which defaults to \u201c_b\u201d - strandedness: which defaults to False :param promoter_pr: a PyRanges of promoter regions. :type promoter_df: pr.PyRanges :param qbed_pr: a pandas DataFrame of qbed data from the experiment. :type qbed_pr: pr.PyRanges :param hops_colname: the name of the column in the qbed_df that contains the number of hops. :return: a pandas DataFrame of promoter regions with a column containing the number of hops in the qbed_df for each promoter. :rtype: DataFrame Source code in callingcardstools/PeakCalling/yeast/call_peaks.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def count_hops ( promoters_pr : pr . PyRanges , qbed_pr : pr . PyRanges , hops_colname : str , ** kwargs , ) -> pd . DataFrame : \"\"\" Use pyranges to join the promoter regions with the qbed data and count the number of qbed records that overlap with each promoter. additional keyword arguments are passed to the join method of the PyRanges object. Currently, the following are configured: - slack: which defaults to 0 - suffix: which defaults to \"_b\" - strandedness: which defaults to False :param promoter_pr: a PyRanges of promoter regions. :type promoter_df: pr.PyRanges :param qbed_pr: a pandas DataFrame of qbed data from the experiment. :type qbed_pr: pr.PyRanges :param hops_colname: the name of the column in the qbed_df that contains the number of hops. :return: a pandas DataFrame of promoter regions with a column containing the number of hops in the qbed_df for each promoter. :rtype: DataFrame \"\"\" overlaps = promoters_pr . join ( qbed_pr , how = \"left\" , slack = kwargs . get ( \"slack\" , 0 ), suffix = kwargs . get ( \"suffix\" , \"_b\" ), strandedness = kwargs . get ( \"strandedness\" , False ), ) # Group by 'name' and count the number of records in each group # `observed` set to true b/c grouping is over categorical variable. This is default # in pandas 2.0. Without this set, memory usage skyrockets. # Setting \"Start_b >= 0\" to remove rows where there is no overlap, which are # represented by -1 in the _b columns by pyranges. overlap_counts = ( overlaps . df . query ( \"Start_b >= 0\" ) . groupby ( \"name\" , observed = True ) . size () . reset_index ( name = \"Count\" ) . rename ( columns = { \"Count\" : hops_colname }) ) return overlap_counts","title":"count_hops"},{"location":"API/PeakCalling/yeast/call_peaks/main/","text":"Call peaks for the given Calling Cards data. Source code in callingcardstools/PeakCalling/yeast/call_peaks.py 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 def main ( args : argparse . Namespace ) -> None : \"\"\" Call peaks for the given Calling Cards data. \"\"\" # note the * -- unpack the list of paths check_files = [ * args . experiment_data_paths , args . promoter_data_path , args . background_data_path , args . chrmap_data_path , ] for file in check_files : if not os . path . isfile ( file ): raise FileNotFoundError ( f \"The following path does not exist: { file } \" ) try : result_df = call_peaks ( args . experiment_data_paths , args . experiment_orig_chr_convention , args . promoter_data_path , args . promoter_orig_chr_convention , args . background_data_path , args . background_orig_chr_convention , args . chrmap_data_path , args . unified_chr_convention , args . deduplicate_experiment , genomic_only = args . genomic_only , pseudocount = args . pseudocount , ) result_df . to_csv ( args . output_path , compression = \"gzip\" if args . compress_output else None , index = False , ) except Exception as e : logger . error ( \"Error processing experiment files: %s . Error: %s \" , args . experiment_data_paths , e , ) raise","title":"main"},{"location":"API/PeakCalling/yeast/call_peaks/parse_args/","text":"Parse the command line arguments. :param subparser: the subparser object. :type subparser: argparse.ArgumentParser :param script_desc: the description of the script. :type script_desc: str :param common_args: the common arguments. :type common_args: argparse.ArgumentParser :return: the parser. :rtype: argparse.ArgumentParser Source code in callingcardstools/PeakCalling/yeast/call_peaks.py 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 def parse_args ( subparser : argparse . ArgumentParser , script_desc : str , common_args : argparse . ArgumentParser , ) -> argparse . ArgumentParser : \"\"\" Parse the command line arguments. :param subparser: the subparser object. :type subparser: argparse.ArgumentParser :param script_desc: the description of the script. :type script_desc: str :param common_args: the common arguments. :type common_args: argparse.ArgumentParser :return: the parser. :rtype: argparse.ArgumentParser \"\"\" parser = subparser . add_parser ( \"yeast_call_peaks\" , help = script_desc , prog = \"yeast_call_peaks\" , parents = [ common_args ], ) parser . set_defaults ( func = main ) parser . add_argument ( \"--experiment_data_paths\" , type = str , nargs = \"+\" , help = \"paths to the experiment data files.\" , required = True , ) parser . add_argument ( \"--experiment_orig_chr_convention\" , type = str , help = \"the chromosome naming convention used in the experiment data \" \"file.\" , required = True , ) parser . add_argument ( \"--promoter_data_path\" , type = str , help = \"path to the promoter data file.\" , required = True , ) parser . add_argument ( \"--promoter_orig_chr_convention\" , type = str , help = \"the chromosome naming convention used in the promoter data \" \"file.\" , required = True , ) parser . add_argument ( \"--background_data_path\" , type = str , help = \"path to the background data file.\" , required = True , ) parser . add_argument ( \"--background_orig_chr_convention\" , type = str , help = \"the chromosome naming convention used in the background data \" \"file.\" , required = True , ) parser . add_argument ( \"--chrmap_data_path\" , type = str , help = \"path to the chromosome map file. this must include the data \" \"files' current naming conventions, the desired naming, and a column \" \"`type` that indicates whether the chromosome is 'genomic' or \" \"something else, eg 'mitochondrial' or 'plasmid'.\" , required = True , ) parser . add_argument ( \"--unified_chr_convention\" , type = str , help = \"the chromosome naming convention to use in the output \" \"DataFrame.\" , required = False , default = \"ucsc\" , ) parser . add_argument ( \"--deduplicate_experiment\" , action = \"store_true\" , help = \"set this flag to deduplicate the experiment data based on `chr`, \" \"`start` and `end` such that if an insertion is found at the same \" \"coordinate on different strands, only one of those records will be \" \"retained.\" , ) parser . add_argument ( \"--genomic_only\" , action = \"store_true\" , help = \"set this flag to include only genomic chromosomes in the \" \"experiment and background.\" , ) parser . add_argument ( \"--output_path\" , default = \"sig_results.csv\" , type = str , help = \"path to the output file.\" , ) parser . add_argument ( \"--pseudocount\" , type = float , help = \"pseudocount to use when calculating poisson pvalue. Note that \" \"this is used only when the background hops are 0 for a given promoter.\" , required = False , default = 0.1 , ) parser . add_argument ( \"--compress_output\" , action = \"store_true\" , help = \"set this flag to gzip the output csv file.\" , ) return subparser","title":"parse_args"},{"location":"API/PeakCalling/yeast/read_in_data/qbed_df_to_pyranges/","text":"Convert a qbed dataframe to a pyranges dataframe. :param df: The qbed dataframe. :type df: pd.DataFrame :return: The pyranges dataframe. :rtype: pd.DataFrame :Example: import pandas as pd df = pd.DataFrame({\u2018chr\u2019: [\u2018chr1\u2019, \u2018chr1\u2019], \u2026 \u2018start\u2019: [1, 2], \u2026 \u2018end\u2019: [2, 3], \u2026 \u2018strand\u2019: [\u2018+\u2019, \u2018-\u2018], \u2026 \u2018depth\u2019: [1, 2]}) pyranges_df = qbed_df_to_pyranges(df) list(pyranges_df.columns) == [\u2018Chromosome\u2019, \u2018Start\u2019, \u2018End\u2019, \u2018Strand\u2019, \u2018depth\u2019] True Source code in callingcardstools/PeakCalling/yeast/read_in_data.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def qbed_df_to_pyranges ( df : pd . DataFrame ) -> pd . DataFrame : \"\"\" Convert a qbed dataframe to a pyranges dataframe. :param df: The qbed dataframe. :type df: pd.DataFrame :return: The pyranges dataframe. :rtype: pd.DataFrame :Example: >>> import pandas as pd >>> df = pd.DataFrame({'chr': ['chr1', 'chr1'], ... 'start': [1, 2], ... 'end': [2, 3], ... 'strand': ['+', '-'], ... 'depth': [1, 2]}) >>> pyranges_df = qbed_df_to_pyranges(df) >>> list(pyranges_df.columns) == ['Chromosome', 'Start', 'End', 'Strand', 'depth'] True \"\"\" # check input if not isinstance ( df , pd . DataFrame ): raise ValueError ( \"df must be a DataFrame\" ) if not all ( col in df . columns for col in [ \"chr\" , \"start\" , \"end\" , \"strand\" , \"depth\" ]): raise ValueError ( \"df must have columns `chr`, `start`, `end`, `strand`, \" \"and `depth`\" ) return pr . PyRanges ( df . rename ( { \"chr\" : \"Chromosome\" , \"start\" : \"Start\" , \"end\" : \"End\" , \"strand\" : \"Strand\" }, axis = 1 , ) )","title":"qbed_df_to_pyranges"},{"location":"API/PeakCalling/yeast/read_in_data/read_in_background_data/","text":"Read in background (hops) data from a qbed file. The qbed file may be plain text or gzipped and may or may not have column headers. If the column headers are present, they must be in the following order: chr , start , end , strand , depth . If the column headers are not present, the columns must be in same order and number. Datatypes are checked but will not be coerced \u2013 errors are raised if they do not match the expected datatypes. the chr column is relabeled from the curr_chr_name_convention to the new_chr_name_convention using the chrmap_df . NOTE: unlike the experiment df, there is no option to deduplicate as the background file is expected to be the combination of multiple experiments at this point. Additional keyword arguments genomic_only (bool): Whether to return only records with type == \u201cgenomic\u201d. See relabel_chr_column for more information. Defaults to True. :param background_data_path: Path to the background data qbed file, plain text or gzipped, with or without column headers :type background_data_path: str :param curr_chr_name_convention: The current chromosome name convention :type curr_chr_name_convention: str :param new_chr_name_convention: The new chromosome name convention :type new_chr_name_convention: str :param chrmap_df: The chrmap dataframe :type chrmap_df: pd.DataFrame :return: The background data. :rtype: pd.DataFrame :raises ValueError: If the background_data_path does not exist or is not a file; if the column headers exist but do not match expectation or if the datatypes do not match expectation. :Example: import pandas as pd import os import tempfile tmp_qbed = tempfile.NamedTemporaryFile(suffix=\u2019.qbed\u2019).name with open(tmp_qbed, \u2018w\u2019) as f: \u2026 _ = f.write(\u2018chr\\tstart\\tend\\tstrand\\tdepth\\n\u2019) \u2026 _ = f.write(\u2018chr1\\t1\\t2\\t+\\t1\\n\u2019) create a temporary chrmap file \u00b6 chrmap_df = pd.DataFrame({\u2018curr_chr_name_convention\u2019: \u2026 [\u2018chr1\u2019, \u2018chr2\u2019, \u2018chr3\u2019], \u2026 \u2018new_chr_name_convention\u2019: \u2026 [\u2018chrI\u2019, \u2018chrII\u2019, \u2018chrIII\u2019]}) read in the data \u00b6 background_df, background_total_hops = read_in_background_data( \u2026 tmp_qbed, \u2026 \u2018curr_chr_name_convention\u2019, \u2026 \u2018new_chr_name_convention\u2019, \u2026 chrmap_df) list(background_df.columns) == [\u2018chr\u2019, \u2018start\u2019, \u2018end\u2019, \u2018depth\u2019, \u2026 \u2018strand\u2019] True background_total_hops 1 Source code in callingcardstools/PeakCalling/yeast/read_in_data.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 def read_in_background_data ( background_data_path : str , curr_chr_name_convention : pd . DataFrame , new_chr_name_convention : pd . DataFrame , chrmap_df : str , ** kwargs , ) -> pd . DataFrame : \"\"\" Read in background (hops) data from a qbed file. The qbed file may be plain text or gzipped and may or may not have column headers. If the column headers are present, they must be in the following order: `chr`, `start`, `end`, `strand`, `depth`. If the column headers are not present, the columns must be in same order and number. Datatypes are checked but will not be coerced -- errors are raised if they do not match the expected datatypes. the `chr` column is relabeled from the `curr_chr_name_convention` to the `new_chr_name_convention` using the `chrmap_df`. NOTE: unlike the experiment df, there is no option to deduplicate as the background file is expected to be the combination of multiple experiments at this point. Additional keyword arguments: - genomic_only (bool): Whether to return only records with type == \"genomic\". See `relabel_chr_column` for more information. Defaults to True. :param background_data_path: Path to the background data qbed file, plain text or gzipped, with or without column headers :type background_data_path: str :param curr_chr_name_convention: The current chromosome name convention :type curr_chr_name_convention: str :param new_chr_name_convention: The new chromosome name convention :type new_chr_name_convention: str :param chrmap_df: The chrmap dataframe :type chrmap_df: pd.DataFrame :return: The background data. :rtype: pd.DataFrame :raises ValueError: If the `background_data_path` does not exist or is not a file; if the column headers exist but do not match expectation or if the datatypes do not match expectation. :Example: >>> import pandas as pd >>> import os >>> import tempfile >>> tmp_qbed = tempfile.NamedTemporaryFile(suffix='.qbed').name >>> with open(tmp_qbed, 'w') as f: ... _ = f.write('chr\\\\tstart\\\\tend\\\\tstrand\\\\tdepth\\\\n') ... _ = f.write('chr1\\\\t1\\\\t2\\\\t+\\\\t1\\\\n') >>> # create a temporary chrmap file >>> chrmap_df = pd.DataFrame({'curr_chr_name_convention': ... ['chr1', 'chr2', 'chr3'], ... 'new_chr_name_convention': ... ['chrI', 'chrII', 'chrIII']}) >>> # read in the data >>> background_df, background_total_hops = read_in_background_data( ... tmp_qbed, ... 'curr_chr_name_convention', ... 'new_chr_name_convention', ... chrmap_df) >>> list(background_df.columns) == ['chr', 'start', 'end', 'depth', ... 'strand'] True >>> background_total_hops 1 \"\"\" # check input if not os . path . exists ( background_data_path ): raise ValueError ( \"background_data_path must exist\" ) if not os . path . isfile ( background_data_path ): raise ValueError ( \"background_data_path must be a file\" ) # check if data is gzipped gzipped = str ( background_data_path ) . endswith ( \".gz\" ) # check if data has column headers header = pd . read_csv ( background_data_path , sep = \" \\t \" , nrows = 0 ) if header . columns . tolist () != [ \"chr\" , \"start\" , \"end\" , \"depth\" , \"strand\" ]: header = None else : header = 0 # read in data try : background_df = pd . read_csv ( background_data_path , sep = \" \\t \" , header = header , names = [ \"chr\" , \"start\" , \"end\" , \"depth\" , \"strand\" ], dtype = { \"chr\" : str , \"start\" : int , \"end\" : int , \"depth\" : \"int64\" , \"strand\" : str , }, compression = \"gzip\" if gzipped else None , ) except ValueError as e : raise ValueError ( \"background_data_path must be a qbed file \" \"with columns `chr`, `start`, `end`, `depth`, \" \"and `strand`\" ) from e # if the file is empty, raise an error. Background data should never be empty if background_df . shape [ 0 ] == 0 : raise ValueError ( \"The background file is empty -- no data to process\" ) # relabel chr column background_df = relabel_chr_column ( background_df , chrmap_df , curr_chr_name_convention , new_chr_name_convention , kwargs . get ( \"genomic_only\" , True ), ) return qbed_df_to_pyranges ( background_df ), len ( background_df )","title":"read_in_background_data"},{"location":"API/PeakCalling/yeast/read_in_data/read_in_background_data/#callingcardstools.PeakCalling.yeast.read_in_data.read_in_background_data--create-a-temporary-chrmap-file","text":"chrmap_df = pd.DataFrame({\u2018curr_chr_name_convention\u2019: \u2026 [\u2018chr1\u2019, \u2018chr2\u2019, \u2018chr3\u2019], \u2026 \u2018new_chr_name_convention\u2019: \u2026 [\u2018chrI\u2019, \u2018chrII\u2019, \u2018chrIII\u2019]})","title":"create a temporary chrmap file"},{"location":"API/PeakCalling/yeast/read_in_data/read_in_background_data/#callingcardstools.PeakCalling.yeast.read_in_data.read_in_background_data--read-in-the-data","text":"background_df, background_total_hops = read_in_background_data( \u2026 tmp_qbed, \u2026 \u2018curr_chr_name_convention\u2019, \u2026 \u2018new_chr_name_convention\u2019, \u2026 chrmap_df) list(background_df.columns) == [\u2018chr\u2019, \u2018start\u2019, \u2018end\u2019, \u2018depth\u2019, \u2026 \u2018strand\u2019] True background_total_hops 1 Source code in callingcardstools/PeakCalling/yeast/read_in_data.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 def read_in_background_data ( background_data_path : str , curr_chr_name_convention : pd . DataFrame , new_chr_name_convention : pd . DataFrame , chrmap_df : str , ** kwargs , ) -> pd . DataFrame : \"\"\" Read in background (hops) data from a qbed file. The qbed file may be plain text or gzipped and may or may not have column headers. If the column headers are present, they must be in the following order: `chr`, `start`, `end`, `strand`, `depth`. If the column headers are not present, the columns must be in same order and number. Datatypes are checked but will not be coerced -- errors are raised if they do not match the expected datatypes. the `chr` column is relabeled from the `curr_chr_name_convention` to the `new_chr_name_convention` using the `chrmap_df`. NOTE: unlike the experiment df, there is no option to deduplicate as the background file is expected to be the combination of multiple experiments at this point. Additional keyword arguments: - genomic_only (bool): Whether to return only records with type == \"genomic\". See `relabel_chr_column` for more information. Defaults to True. :param background_data_path: Path to the background data qbed file, plain text or gzipped, with or without column headers :type background_data_path: str :param curr_chr_name_convention: The current chromosome name convention :type curr_chr_name_convention: str :param new_chr_name_convention: The new chromosome name convention :type new_chr_name_convention: str :param chrmap_df: The chrmap dataframe :type chrmap_df: pd.DataFrame :return: The background data. :rtype: pd.DataFrame :raises ValueError: If the `background_data_path` does not exist or is not a file; if the column headers exist but do not match expectation or if the datatypes do not match expectation. :Example: >>> import pandas as pd >>> import os >>> import tempfile >>> tmp_qbed = tempfile.NamedTemporaryFile(suffix='.qbed').name >>> with open(tmp_qbed, 'w') as f: ... _ = f.write('chr\\\\tstart\\\\tend\\\\tstrand\\\\tdepth\\\\n') ... _ = f.write('chr1\\\\t1\\\\t2\\\\t+\\\\t1\\\\n') >>> # create a temporary chrmap file >>> chrmap_df = pd.DataFrame({'curr_chr_name_convention': ... ['chr1', 'chr2', 'chr3'], ... 'new_chr_name_convention': ... ['chrI', 'chrII', 'chrIII']}) >>> # read in the data >>> background_df, background_total_hops = read_in_background_data( ... tmp_qbed, ... 'curr_chr_name_convention', ... 'new_chr_name_convention', ... chrmap_df) >>> list(background_df.columns) == ['chr', 'start', 'end', 'depth', ... 'strand'] True >>> background_total_hops 1 \"\"\" # check input if not os . path . exists ( background_data_path ): raise ValueError ( \"background_data_path must exist\" ) if not os . path . isfile ( background_data_path ): raise ValueError ( \"background_data_path must be a file\" ) # check if data is gzipped gzipped = str ( background_data_path ) . endswith ( \".gz\" ) # check if data has column headers header = pd . read_csv ( background_data_path , sep = \" \\t \" , nrows = 0 ) if header . columns . tolist () != [ \"chr\" , \"start\" , \"end\" , \"depth\" , \"strand\" ]: header = None else : header = 0 # read in data try : background_df = pd . read_csv ( background_data_path , sep = \" \\t \" , header = header , names = [ \"chr\" , \"start\" , \"end\" , \"depth\" , \"strand\" ], dtype = { \"chr\" : str , \"start\" : int , \"end\" : int , \"depth\" : \"int64\" , \"strand\" : str , }, compression = \"gzip\" if gzipped else None , ) except ValueError as e : raise ValueError ( \"background_data_path must be a qbed file \" \"with columns `chr`, `start`, `end`, `depth`, \" \"and `strand`\" ) from e # if the file is empty, raise an error. Background data should never be empty if background_df . shape [ 0 ] == 0 : raise ValueError ( \"The background file is empty -- no data to process\" ) # relabel chr column background_df = relabel_chr_column ( background_df , chrmap_df , curr_chr_name_convention , new_chr_name_convention , kwargs . get ( \"genomic_only\" , True ), ) return qbed_df_to_pyranges ( background_df ), len ( background_df )","title":"read in the data"},{"location":"API/PeakCalling/yeast/read_in_data/read_in_chrmap/","text":"Read in the chrmap data from the chrmap_data_path and check that the required_col_set is a subset of the columns in the chrmap dataframe. :param chrmap_data_path: Path to the chrmap file. :type chrmap_data_path: str :param required_col_set: The required columns in the chrmap dataframe. :type required_col_set: set :return: The chrmap dataframe. :rtype: pd.DataFrame :raises ValueError: If the chrmap_data_path does not exist or is not a file; if the required_col_set is not a subset of the columns in the chrmap dataframe. :Example: import pandas as pd import os import tempfile tmp_chrmap = tempfile.NamedTemporaryFile(suffix=\u2019.csv\u2019).name with open(tmp_chrmap, \u2018w\u2019) as f: \u2026 _ = f.write(\u2018experiment_orig_chr_convention,\u2019 \u2026 \u2018promoter_orig_chr_convention,\u2019 \u2026 \u2018background_orig_chr_convention,\u2019 \u2026 \u2018unified_chr_convention\\n\u2019) \u2026 _ = f.write(\u2018chr1,chr1,chr1,chrI\\n\u2019) chrmap_df = read_in_chrmap(tmp_chrmap, \u2026 {\u2018experiment_orig_chr_convention\u2019, \u2026 \u2018promoter_orig_chr_convention\u2019, \u2026 \u2018background_orig_chr_convention\u2019, \u2026 \u2018unified_chr_convention\u2019}) list(chrmap_df.columns) == [\u2018experiment_orig_chr_convention\u2019, \u2026 \u2018promoter_orig_chr_convention\u2019, \u2026 \u2018background_orig_chr_convention\u2019, \u2026 \u2018unified_chr_convention\u2019] True Source code in callingcardstools/PeakCalling/yeast/read_in_data.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def read_in_chrmap ( chrmap_data_path : str , required_col_set : set ) -> pd . DataFrame : \"\"\" Read in the chrmap data from the `chrmap_data_path` and check that the `required_col_set` is a subset of the columns in the chrmap dataframe. :param chrmap_data_path: Path to the chrmap file. :type chrmap_data_path: str :param required_col_set: The required columns in the chrmap dataframe. :type required_col_set: set :return: The chrmap dataframe. :rtype: pd.DataFrame :raises ValueError: If the `chrmap_data_path` does not exist or is not a file; if the `required_col_set` is not a subset of the columns in the chrmap dataframe. :Example: >>> import pandas as pd >>> import os >>> import tempfile >>> tmp_chrmap = tempfile.NamedTemporaryFile(suffix='.csv').name >>> with open(tmp_chrmap, 'w') as f: ... _ = f.write('experiment_orig_chr_convention,' ... 'promoter_orig_chr_convention,' ... 'background_orig_chr_convention,' ... 'unified_chr_convention\\\\n') ... _ = f.write('chr1,chr1,chr1,chrI\\\\n') >>> chrmap_df = read_in_chrmap(tmp_chrmap, ... {'experiment_orig_chr_convention', ... 'promoter_orig_chr_convention', ... 'background_orig_chr_convention', ... 'unified_chr_convention'}) >>> list(chrmap_df.columns) == ['experiment_orig_chr_convention', ... 'promoter_orig_chr_convention', ... 'background_orig_chr_convention', ... 'unified_chr_convention'] True \"\"\" if not os . path . exists ( chrmap_data_path ): raise ValueError ( f \"chrmap_data_path does \" f \"not exist: { chrmap_data_path } \" ) required_col_set . add ( \"type\" ) # read in the chr map chrmap_df = pd . read_csv ( chrmap_data_path ) # raise an error if the nrow of the chrmap is 0 if chrmap_df . shape [ 0 ] == 0 : raise ValueError ( \"The chrmap file is empty\" ) # raise an error if the ncol of the chrmap is less than 2 if chrmap_df . shape [ 1 ] < 1 : raise ValueError ( \"The chrmap file must have at least two column, \" , \"one for the original chromosome name and one for \" , \"the new chromosome name\" , ) # raise an error if the experiment_orig_chr_convention, # promoter_orig_chr_convention, # background_orig_chr_convention and unified_chr_convention are not in # the chrmap_df columns missing_chr_cols = required_col_set . difference ( chrmap_df . columns ) if len ( missing_chr_cols ) > 0 : raise ValueError ( f \"The following chromosome columns are missing \" f \"from the chrmap file: { missing_chr_cols } \" ) # cast all columns to str chrmap_df = chrmap_df . astype ( \"str\" ) return chrmap_df handler: python","title":"read_in_chrmap"},{"location":"API/PeakCalling/yeast/read_in_data/read_in_experiment_data/","text":"Read in experiment (hops) data from a qbed file. The qbed file may be plain text or gzipped and may or may not have column headers. If the column headers are present, they must be in the following order: chr , start , end , strand , depth . If the column headers are not present, the columns must be in same order and number. Datatypes are checked but will not be coerced \u2013 errors are raised if they do not match the expected datatypes. the chr column is relabeled from the curr_chr_name_convention to the new_chr_name_convention using the chrmap_df . Additional keyword arguments genomic_only (bool): Whether to return only records with type == \u201cgenomic\u201d. See relabel_chr_column for more information. Defaults to True. :param experiment_data_path: Path to the qbed file, plain text or gzipped, with or without column headers :type experiment_data_path: str :param curr_chr_name_convention: The current chromosome name convention. :type curr_chr_name_convention: str :param new_chr_name_convention: The new chromosome name convention. :type new_chr_name_convention: str :param chrmap_df: The chrmap dataframe. :type chrmap_df: pd.DataFrame :param deduplicate: Whether to deduplicate the experiment data based on chr , start , end such that if an insertion occurs at the same coordinate but on opposite strands, only one record is retained. :type deduplicate: bool :return: The experiment data as a dataframe with the chr column refactored to the new_chr_name_convention :rtype: pd.DataFrame :raises ValueError: If the experiment_data_path does not exist or is not a file; if the column headers exist but do not match expectation or if the datatypes do not match expectation. :Example: import pandas as pd import os import tempfile tmp_qbed = tempfile.NamedTemporaryFile(suffix=\u2019.qbed\u2019).name with open(tmp_qbed, \u2018w\u2019) as f: \u2026 _ = f.write(\u2018chr\\tstart\\tend\\tstrand\\tdepth\\n\u2019) \u2026 _ = f.write(\u2018chr1\\t1\\t2\\t+\\t1\\n\u2019) create a temporary chrmap file \u00b6 tmp_chrmap = tempfile.NamedTemporaryFile(suffix=\u2019.csv\u2019).name chrmap_df = pd.DataFrame({\u2018curr_chr_name_convention\u2019: \u2026 [\u2018chr1\u2019, \u2018chr2\u2019, \u2018chr3\u2019], \u2026 \u2018new_chr_name_convention\u2019: \u2026 [\u2018chrI\u2019, \u2018chrII\u2019, \u2018chrIII\u2019]}) read in the data \u00b6 experiment_df, experiment_total_hops = read_in_experiment_data( \u2026 tmp_qbed, \u2026 \u2018curr_chr_name_convention\u2019, \u2026 \u2018new_chr_name_convention\u2019, \u2026 chrmap_df) list(experiment_df.columns) == [\u2018chr\u2019, \u2018start\u2019, \u2018end\u2019, \u2018depth\u2019, \u2026 \u2018strand\u2019] True experiment_total_hops 1 Source code in callingcardstools/PeakCalling/yeast/read_in_data.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 def read_in_experiment_data ( experiment_data_path : str , curr_chr_name_convention : pd . DataFrame , new_chr_name_convention : pd . DataFrame , chrmap_df : str , deduplicate : bool = True , ** kwargs , ) -> pd . DataFrame : \"\"\" Read in experiment (hops) data from a qbed file. The qbed file may be plain text or gzipped and may or may not have column headers. If the column headers are present, they must be in the following order: `chr`, `start`, `end`, `strand`, `depth`. If the column headers are not present, the columns must be in same order and number. Datatypes are checked but will not be coerced -- errors are raised if they do not match the expected datatypes. the `chr` column is relabeled from the `curr_chr_name_convention` to the `new_chr_name_convention` using the `chrmap_df`. Additional keyword arguments: - genomic_only (bool): Whether to return only records with type == \"genomic\". See `relabel_chr_column` for more information. Defaults to True. :param experiment_data_path: Path to the qbed file, plain text or gzipped, with or without column headers :type experiment_data_path: str :param curr_chr_name_convention: The current chromosome name convention. :type curr_chr_name_convention: str :param new_chr_name_convention: The new chromosome name convention. :type new_chr_name_convention: str :param chrmap_df: The chrmap dataframe. :type chrmap_df: pd.DataFrame :param deduplicate: Whether to deduplicate the experiment data based on `chr`, `start`, `end` such that if an insertion occurs at the same coordinate but on opposite strands, only one record is retained. :type deduplicate: bool :return: The experiment data as a dataframe with the `chr` column refactored to the `new_chr_name_convention` :rtype: pd.DataFrame :raises ValueError: If the `experiment_data_path` does not exist or is not a file; if the column headers exist but do not match expectation or if the datatypes do not match expectation. :Example: >>> import pandas as pd >>> import os >>> import tempfile >>> tmp_qbed = tempfile.NamedTemporaryFile(suffix='.qbed').name >>> with open(tmp_qbed, 'w') as f: ... _ = f.write('chr\\\\tstart\\\\tend\\\\tstrand\\\\tdepth\\\\n') ... _ = f.write('chr1\\\\t1\\\\t2\\\\t+\\\\t1\\\\n') >>> # create a temporary chrmap file >>> tmp_chrmap = tempfile.NamedTemporaryFile(suffix='.csv').name >>> chrmap_df = pd.DataFrame({'curr_chr_name_convention': ... ['chr1', 'chr2', 'chr3'], ... 'new_chr_name_convention': ... ['chrI', 'chrII', 'chrIII']}) >>> # read in the data >>> experiment_df, experiment_total_hops = read_in_experiment_data( ... tmp_qbed, ... 'curr_chr_name_convention', ... 'new_chr_name_convention', ... chrmap_df) >>> list(experiment_df.columns) == ['chr', 'start', 'end', 'depth', ... 'strand'] True >>> experiment_total_hops 1 \"\"\" # check input if not os . path . exists ( experiment_data_path ): raise ValueError ( \"experiment_data_path must exist\" ) if not os . path . isfile ( experiment_data_path ): raise ValueError ( \"experiment_data_path must be a file\" ) # check if data is gzipped gzipped = str ( experiment_data_path ) . endswith ( \".gz\" ) # check if data has column headers header = pd . read_csv ( experiment_data_path , sep = \" \\t \" , compression = \"gzip\" if gzipped else None , nrows = 0 ) if header . columns . tolist () != [ \"chr\" , \"start\" , \"end\" , \"depth\" , \"strand\" ]: header = None else : header = 0 # read in data try : experiment_df = pd . read_csv ( experiment_data_path , sep = \" \\t \" , header = header , names = [ \"chr\" , \"start\" , \"end\" , \"depth\" , \"strand\" ], dtype = { \"chr\" : str , \"start\" : int , \"end\" : int , \"depth\" : int , \"strand\" : str }, compression = \"gzip\" if gzipped else None , ) except ValueError as e : raise ValueError ( \"experiment_data_path must be a qbed file \" \"with columns `chr`, `start`, `end`, `strand`, \" \"and `depth`\" ) from e # if the file is empty, raise an error. This needs to be handled in an outer # function in order to exit gracefully if there is no actual data to do # calculations on if experiment_df . shape [ 0 ] == 0 : raise ValueError ( \"The experiment file is empty -- no data to process\" ) # relabel chr column experiment_df = relabel_chr_column ( experiment_df , chrmap_df , curr_chr_name_convention , new_chr_name_convention , kwargs . get ( \"genomic_only\" , True ), ) if deduplicate : # deduplicate based on `chr`, `start`, `end` such that if an insertion occurs # at the same coordinate but on opposite strands, only one record is retained experiment_df . drop_duplicates ( subset = [ \"chr\" , \"start\" , \"end\" ], inplace = True ) # set strand to '*' experiment_df [ \"strand\" ] = \"*\" return qbed_df_to_pyranges ( experiment_df ), len ( experiment_df )","title":"read_in_experiment_data"},{"location":"API/PeakCalling/yeast/read_in_data/read_in_experiment_data/#callingcardstools.PeakCalling.yeast.read_in_data.read_in_experiment_data--create-a-temporary-chrmap-file","text":"tmp_chrmap = tempfile.NamedTemporaryFile(suffix=\u2019.csv\u2019).name chrmap_df = pd.DataFrame({\u2018curr_chr_name_convention\u2019: \u2026 [\u2018chr1\u2019, \u2018chr2\u2019, \u2018chr3\u2019], \u2026 \u2018new_chr_name_convention\u2019: \u2026 [\u2018chrI\u2019, \u2018chrII\u2019, \u2018chrIII\u2019]})","title":"create a temporary chrmap file"},{"location":"API/PeakCalling/yeast/read_in_data/read_in_experiment_data/#callingcardstools.PeakCalling.yeast.read_in_data.read_in_experiment_data--read-in-the-data","text":"experiment_df, experiment_total_hops = read_in_experiment_data( \u2026 tmp_qbed, \u2026 \u2018curr_chr_name_convention\u2019, \u2026 \u2018new_chr_name_convention\u2019, \u2026 chrmap_df) list(experiment_df.columns) == [\u2018chr\u2019, \u2018start\u2019, \u2018end\u2019, \u2018depth\u2019, \u2026 \u2018strand\u2019] True experiment_total_hops 1 Source code in callingcardstools/PeakCalling/yeast/read_in_data.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 def read_in_experiment_data ( experiment_data_path : str , curr_chr_name_convention : pd . DataFrame , new_chr_name_convention : pd . DataFrame , chrmap_df : str , deduplicate : bool = True , ** kwargs , ) -> pd . DataFrame : \"\"\" Read in experiment (hops) data from a qbed file. The qbed file may be plain text or gzipped and may or may not have column headers. If the column headers are present, they must be in the following order: `chr`, `start`, `end`, `strand`, `depth`. If the column headers are not present, the columns must be in same order and number. Datatypes are checked but will not be coerced -- errors are raised if they do not match the expected datatypes. the `chr` column is relabeled from the `curr_chr_name_convention` to the `new_chr_name_convention` using the `chrmap_df`. Additional keyword arguments: - genomic_only (bool): Whether to return only records with type == \"genomic\". See `relabel_chr_column` for more information. Defaults to True. :param experiment_data_path: Path to the qbed file, plain text or gzipped, with or without column headers :type experiment_data_path: str :param curr_chr_name_convention: The current chromosome name convention. :type curr_chr_name_convention: str :param new_chr_name_convention: The new chromosome name convention. :type new_chr_name_convention: str :param chrmap_df: The chrmap dataframe. :type chrmap_df: pd.DataFrame :param deduplicate: Whether to deduplicate the experiment data based on `chr`, `start`, `end` such that if an insertion occurs at the same coordinate but on opposite strands, only one record is retained. :type deduplicate: bool :return: The experiment data as a dataframe with the `chr` column refactored to the `new_chr_name_convention` :rtype: pd.DataFrame :raises ValueError: If the `experiment_data_path` does not exist or is not a file; if the column headers exist but do not match expectation or if the datatypes do not match expectation. :Example: >>> import pandas as pd >>> import os >>> import tempfile >>> tmp_qbed = tempfile.NamedTemporaryFile(suffix='.qbed').name >>> with open(tmp_qbed, 'w') as f: ... _ = f.write('chr\\\\tstart\\\\tend\\\\tstrand\\\\tdepth\\\\n') ... _ = f.write('chr1\\\\t1\\\\t2\\\\t+\\\\t1\\\\n') >>> # create a temporary chrmap file >>> tmp_chrmap = tempfile.NamedTemporaryFile(suffix='.csv').name >>> chrmap_df = pd.DataFrame({'curr_chr_name_convention': ... ['chr1', 'chr2', 'chr3'], ... 'new_chr_name_convention': ... ['chrI', 'chrII', 'chrIII']}) >>> # read in the data >>> experiment_df, experiment_total_hops = read_in_experiment_data( ... tmp_qbed, ... 'curr_chr_name_convention', ... 'new_chr_name_convention', ... chrmap_df) >>> list(experiment_df.columns) == ['chr', 'start', 'end', 'depth', ... 'strand'] True >>> experiment_total_hops 1 \"\"\" # check input if not os . path . exists ( experiment_data_path ): raise ValueError ( \"experiment_data_path must exist\" ) if not os . path . isfile ( experiment_data_path ): raise ValueError ( \"experiment_data_path must be a file\" ) # check if data is gzipped gzipped = str ( experiment_data_path ) . endswith ( \".gz\" ) # check if data has column headers header = pd . read_csv ( experiment_data_path , sep = \" \\t \" , compression = \"gzip\" if gzipped else None , nrows = 0 ) if header . columns . tolist () != [ \"chr\" , \"start\" , \"end\" , \"depth\" , \"strand\" ]: header = None else : header = 0 # read in data try : experiment_df = pd . read_csv ( experiment_data_path , sep = \" \\t \" , header = header , names = [ \"chr\" , \"start\" , \"end\" , \"depth\" , \"strand\" ], dtype = { \"chr\" : str , \"start\" : int , \"end\" : int , \"depth\" : int , \"strand\" : str }, compression = \"gzip\" if gzipped else None , ) except ValueError as e : raise ValueError ( \"experiment_data_path must be a qbed file \" \"with columns `chr`, `start`, `end`, `strand`, \" \"and `depth`\" ) from e # if the file is empty, raise an error. This needs to be handled in an outer # function in order to exit gracefully if there is no actual data to do # calculations on if experiment_df . shape [ 0 ] == 0 : raise ValueError ( \"The experiment file is empty -- no data to process\" ) # relabel chr column experiment_df = relabel_chr_column ( experiment_df , chrmap_df , curr_chr_name_convention , new_chr_name_convention , kwargs . get ( \"genomic_only\" , True ), ) if deduplicate : # deduplicate based on `chr`, `start`, `end` such that if an insertion occurs # at the same coordinate but on opposite strands, only one record is retained experiment_df . drop_duplicates ( subset = [ \"chr\" , \"start\" , \"end\" ], inplace = True ) # set strand to '*' experiment_df [ \"strand\" ] = \"*\" return qbed_df_to_pyranges ( experiment_df ), len ( experiment_df )","title":"read in the data"},{"location":"API/PeakCalling/yeast/read_in_data/read_in_promoter_data/","text":"Read in the promoter data. The promoter data should be a tsv with extension .bed or .bed.gz and should have the following columns: chr start end name score strand . The chr column is refactored from the curr_chr_name_convention to the new_chr_name_convention using the chrmap_df . :param promoter_data_path: Path to the promoter bed file (plus colnames) :type promoter_data_path: str :param curr_chr_name_convention: The current chromosome name convention. :type curr_chr_name_convention: str :param new_chr_name_convention: The new chromosome name convention. :type new_chr_name_convention: str :param chrmap_df: The chrmap dataframe. :type chrmap_df: pd.DataFrame :return: The promoter data as a dataframe with the chr column refactored to the new_chr_name_convention :rtype: pd.DataFrame :raises ValueError: If the promoter_data_path does not exist or is not a file; if the column headers exist but do not match expectation or if the datatypes do not match expectation. :Example: import pandas as pd import os import tempfile tmp_bed = tempfile.NamedTemporaryFile(suffix=\u2019.bed\u2019).name with open(tmp_bed, \u2018w\u2019) as f: \u2026 _ = f.write(\u2018chr\\tstart\\tend\\tname\\tscore\\tstrand\\n\u2019) \u2026 _ = f.write(\u2018chr1\\t1\\t2\\ttest\\t1\\t+\\n\u2019) chrmap_df = pd.DataFrame({\u2018curr_chr_name_convention\u2019: \u2026 [\u2018chr1\u2019, \u2018chr2\u2019, \u2018chr3\u2019], \u2026 \u2018new_chr_name_convention\u2019: \u2026 [\u2018chrI\u2019, \u2018chrII\u2019, \u2018chrIII\u2019]}) promoter_df = read_in_promoter_data( \u2026 tmp_bed, \u2026 \u2018curr_chr_name_convention\u2019, \u2026 \u2018new_chr_name_convention\u2019, \u2026 chrmap_df) list(promoter_df.columns) == [\u2018chr\u2019, \u2018start\u2019, \u2018end\u2019, \u2018name\u2019, \u2026 \u2018score\u2019, \u2018strand\u2019] True len(promoter_df) == 1 True Source code in callingcardstools/PeakCalling/yeast/read_in_data.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 def read_in_promoter_data ( promoter_data_path : str , curr_chr_name_convention : pd . DataFrame , new_chr_name_convention : pd . DataFrame , chrmap_df : str , ) -> pd . DataFrame : \"\"\" Read in the promoter data. The promoter data should be a tsv with extension `.bed` or `.bed.gz` and should have the following columns: `chr` `start` `end` `name` `score` `strand`. The `chr` column is refactored from the `curr_chr_name_convention` to the `new_chr_name_convention` using the `chrmap_df`. :param promoter_data_path: Path to the promoter bed file (plus colnames) :type promoter_data_path: str :param curr_chr_name_convention: The current chromosome name convention. :type curr_chr_name_convention: str :param new_chr_name_convention: The new chromosome name convention. :type new_chr_name_convention: str :param chrmap_df: The chrmap dataframe. :type chrmap_df: pd.DataFrame :return: The promoter data as a dataframe with the `chr` column refactored to the `new_chr_name_convention` :rtype: pd.DataFrame :raises ValueError: If the `promoter_data_path` does not exist or is not a file; if the column headers exist but do not match expectation or if the datatypes do not match expectation. :Example: >>> import pandas as pd >>> import os >>> import tempfile >>> tmp_bed = tempfile.NamedTemporaryFile(suffix='.bed').name >>> with open(tmp_bed, 'w') as f: ... _ = f.write('chr\\\\tstart\\\\tend\\\\tname\\\\tscore\\\\tstrand\\\\n') ... _ = f.write('chr1\\\\t1\\\\t2\\\\ttest\\\\t1\\\\t+\\\\n') >>> chrmap_df = pd.DataFrame({'curr_chr_name_convention': ... ['chr1', 'chr2', 'chr3'], ... 'new_chr_name_convention': ... ['chrI', 'chrII', 'chrIII']}) >>> promoter_df = read_in_promoter_data( ... tmp_bed, ... 'curr_chr_name_convention', ... 'new_chr_name_convention', ... chrmap_df) >>> list(promoter_df.columns) == ['chr', 'start', 'end', 'name', ... 'score', 'strand'] True >>> len(promoter_df) == 1 True \"\"\" # check input if not os . path . exists ( promoter_data_path ): raise ValueError ( \"promoter_data_path must exist\" ) if not os . path . isfile ( promoter_data_path ): raise ValueError ( \"promoter_data_path must be a file\" ) # check if data is gzipped gzipped = str ( promoter_data_path ) . endswith ( \".gz\" ) # check if data has column headers header = pd . read_csv ( promoter_data_path , sep = \" \\t \" , compression = \"gzip\" if gzipped else None , nrows = 0 ) if header . columns . tolist () != [ \"chr\" , \"start\" , \"end\" , \"name\" , \"score\" , \"strand\" ]: header = None else : header = 0 # read in data try : promoter_df = pd . read_csv ( promoter_data_path , sep = \" \\t \" , header = header , names = [ \"chr\" , \"start\" , \"end\" , \"name\" , \"score\" , \"strand\" ], dtype = { \"chr\" : str , \"start\" : int , \"end\" : int , \"name\" : str , \"score\" : float , \"strand\" : str , }, compression = \"gzip\" if gzipped else None , ) except ValueError as e : raise ValueError ( \"promoter_data_path must be a bed file \" \"with columns `chr`, `start`, `end`, `name`, \" \"`score`, and `strand`\" ) from e # if the file is empty, raise an error. if promoter_df . shape [ 0 ] == 0 : raise ValueError ( \"The promoter file is empty -- no data to process\" ) # relabel chr column chr_relabeled_promoter_df = relabel_chr_column ( promoter_df , chrmap_df , curr_chr_name_convention , new_chr_name_convention ) return chr_relabeled_promoter_df","title":"read_in_promoter_data"},{"location":"API/PeakCalling/yeast/read_in_data/relabel_chr_column/","text":"Given a data_df with column chr , a curr_chr_name_convention and a new_chr_name_convention , that are both columns of chrmap_df , join the chrmap to the data_df based on the curr_chr_name_convention and swap the values in the chr column to the new_chr_name_convention . relabel the new_chr_name_convention to chr and return the dataframe with columns in the same order as the input dataframe. If geonmic_only is set to True , only those chromosomes with type == \u2018genomic\u2019 are returned. :param df: The dataframe to relabel. :type df: pd.DataFrame :param curr_chr_name_convention: The current chromosome name convention. :type curr_chr_name_convention: str :param new_chr_name_convention: The new chromosome name convention. :type new_chr_name_convention: str :param genomic_only: Whether to return only records with type == \u201cgenomic\u201d. :type genomic_only: bool :return: The relabeled dataframe. :rtype: pd.DataFrame :raises ValueError: If the curr_chr_name_convention or new_chr_name_convention are not columns in chrmap_df . :Example: import pandas as pd data_df = pd.DataFrame({\u2018chr\u2019: [\u2018chr1\u2019, \u2018chr2\u2019, \u2018chr3\u2019], \u2026 \u2018start\u2019: [1, 2, 3],}) chrmap_df = pd.DataFrame({\u2018curr_chr_name_convention\u2019: \u2026 [\u2018chr1\u2019, \u2018chr2\u2019, \u2018chr3\u2019], \u2026 \u2018new_chr_name_convention\u2019: \u2026 [\u2018chrI\u2019, \u2018chrII\u2019, \u2018chrIII\u2019]}) relabeled_df = relabel_chr_column(data_df, chrmap_df, \u2026 \u2018curr_chr_name_convention\u2019, \u2026 \u2018new_chr_name_convention\u2019) list(relabeled_df.columns) == [\u2018chr\u2019, \u2018start\u2019] True list(relabeled_df[\u2018chr\u2019]) == [\u2018chrI\u2019, \u2018chrII\u2019, \u2018chrIII\u2019] True Source code in callingcardstools/PeakCalling/yeast/read_in_data.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def relabel_chr_column ( data_df : pd . DataFrame , chrmap_df : pd . DataFrame , curr_chr_name_convention : str , new_chr_name_convention : str , genomic_only : bool = True , ) -> pd . DataFrame : \"\"\" Given a `data_df` with column `chr`, a `curr_chr_name_convention` and a `new_chr_name_convention`, that are both columns of `chrmap_df`, join the `chrmap` to the `data_df` based on the `curr_chr_name_convention` and swap the values in the `chr` column to the `new_chr_name_convention`. relabel the `new_chr_name_convention` to `chr` and return the dataframe with columns in the same order as the input dataframe. If `geonmic_only` is set to `True`, only those chromosomes with type == 'genomic' are returned. :param df: The dataframe to relabel. :type df: pd.DataFrame :param curr_chr_name_convention: The current chromosome name convention. :type curr_chr_name_convention: str :param new_chr_name_convention: The new chromosome name convention. :type new_chr_name_convention: str :param genomic_only: Whether to return only records with type == \"genomic\". :type genomic_only: bool :return: The relabeled dataframe. :rtype: pd.DataFrame :raises ValueError: If the `curr_chr_name_convention` or `new_chr_name_convention` are not columns in `chrmap_df`. :Example: >>> import pandas as pd >>> data_df = pd.DataFrame({'chr': ['chr1', 'chr2', 'chr3'], ... 'start': [1, 2, 3],}) >>> chrmap_df = pd.DataFrame({'curr_chr_name_convention': ... ['chr1', 'chr2', 'chr3'], ... 'new_chr_name_convention': ... ['chrI', 'chrII', 'chrIII']}) >>> relabeled_df = relabel_chr_column(data_df, chrmap_df, ... 'curr_chr_name_convention', ... 'new_chr_name_convention') >>> list(relabeled_df.columns) == ['chr', 'start'] True >>> list(relabeled_df['chr']) == ['chrI', 'chrII', 'chrIII'] True \"\"\" # check input if \"chr_curr\" in chrmap_df . columns : raise ValueError ( \"chr_curr cannot be a column in chrmap_df for the \" \"purposes of relabelling. rename that column in \" \"chrmap_df and resubmit\" ) if curr_chr_name_convention not in chrmap_df . columns : raise ValueError ( \"curr_chr_name_convention \" \"must be a column in chrmap_df\" ) if new_chr_name_convention not in chrmap_df . columns : raise ValueError ( \"new_chr_name_convention \" \"must be a column in chrmap_df\" ) # rename the current chr column to chr_curr to avoid any errors in # joining, if the old/new format is called 'chr' data_df = data_df . rename ( columns = { \"chr\" : \"chr_curr\" }) if curr_chr_name_convention != new_chr_name_convention : # join a subset of chrmap_df -- only the columns we need -- to data_df data_df = data_df . merge ( chrmap_df [[ curr_chr_name_convention , new_chr_name_convention , \"type\" ]], left_on = \"chr_curr\" , right_on = curr_chr_name_convention , ) # swap values in chr column data_df [ \"chr_curr\" ] = data_df [ new_chr_name_convention ] else : data_df = data_df . merge ( chrmap_df [[ curr_chr_name_convention , \"type\" ]], left_on = \"chr_curr\" , right_on = curr_chr_name_convention , ) if genomic_only : data_df = data_df . query ( \"type=='genomic'\" ) return data_df . drop ( columns = [ new_chr_name_convention , curr_chr_name_convention , \"type\" ] ) . rename ( columns = { \"chr_curr\" : \"chr\" })","title":"relabel_chr_column"},{"location":"API/QC/StatusFlags/","text":"Bases: IntFlag A class used to represent different status flags for read alignment. Each status flag corresponds to a different power of 2, allowing combinations of flags to be represented as a sum of these powers. Attributes: Name Type Description BARCODE int Corresponds to a barcode failure. MAPQ int Corresponds to a MAPQ failure. INSERT_SEQ int Corresponds to an insert sequence failure. FIVE_PRIME_CLIP int Corresponds to a failure due to 5\u2019 end clipping in the read. UNMAPPED int Corresponds to the read being unmapped. NOT_PRIMARY int Corresponds to the read not being primary. ALIGNER_QC_FAIL int Corresponds to the read failing aligner QC. RESTRICTION_ENZYME int Corresponds to a failure due to restriction enzyme. Source code in callingcardstools/QC/StatusFlags.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 class StatusFlags ( IntFlag ): \"\"\" A class used to represent different status flags for read alignment. Each status flag corresponds to a different power of 2, allowing combinations of flags to be represented as a sum of these powers. Attributes: BARCODE (int): Corresponds to a barcode failure. MAPQ (int): Corresponds to a MAPQ failure. INSERT_SEQ (int): Corresponds to an insert sequence failure. FIVE_PRIME_CLIP (int): Corresponds to a failure due to 5' end clipping in the read. UNMAPPED (int): Corresponds to the read being unmapped. NOT_PRIMARY (int): Corresponds to the read not being primary. ALIGNER_QC_FAIL (int): Corresponds to the read failing aligner QC. RESTRICTION_ENZYME (int): Corresponds to a failure due to restriction enzyme. \"\"\" BARCODE = 0x0 MAPQ = 0x1 INSERT_SEQ = 0x2 FIVE_PRIME_CLIP = 0x3 UNMAPPED = 0x4 NOT_PRIMARY = 0x5 ALIGNER_QC_FAIL = 0x6 RESTRICTION_ENZYME = 0x7 def flag ( self ) -> int : \"\"\" Returns the power of 2 corresponding to the flag's value. Returns: int: The power of 2 corresponding to the flag's value. \"\"\" return 2 ** self . value @staticmethod def decompose ( nums : Union [ int , List [ int ], ndarray , Series ], as_str : bool = True ) -> List [ int ]: \"\"\" Decomposes a number, list, ndarray, or pandas Series of numbers representing the sum of the powers of two into a list of those powers of two. Optionally, return the string representation of the powers of two according to the StatusFlags object. Args: nums (Union[int, List[int], ndarray, pd.Series]): The input number, list, ndarray, or pandas Series to decompose. as_str (bool, optional): Whether to return the string representation of the powers of two according to the StatusFlags object. Defaults to True. Returns: List: A list representing the sum of the powers of two if `as_str` is False, e.g., 10 decomposes into [2, 8]. If `as_str` is true, then the result would be ['MAPQ', 'RESTRICTION_ENZYME']. Raises: TypeError: If the input type is neither int, list, numpy array, nor pandas Series. ValueError: If the input is a negative integer. \"\"\" def decompose_single ( num : int , as_str : bool = True ) -> list : \"\"\" Helper function to decompose a single integer into powers of 2. Args: num (int): The integer to decompose. as_str (bool): Whether to return the string representation of the powers of two according to the StatusFlags object. Defaults to True. Returns: list: List of powers of two composing the input number. If `as_str` is True, the powers of two are represented by their corresponding flag names. Raises: ValueError: If the input integer is negative. \"\"\" # check input if num < 0 : raise ValueError ( \"Invalid input, expected positive int\" ) # if num is 0 and as_str is true, return NO_STATUS. otherwise, the # decomposed list will be empty if num == 0 : if as_str : return [ 'NO_STATUS' ] # Use list comprehension to find the powers of two that # compose the input number powers = [ int ( log2 ( 1 << i )) for i in range ( num . bit_length ()) if num & ( 1 << i )] if as_str : # Convert the powers of two to their string representation powers = [ StatusFlags ( int ( x )) . name for x in powers ] return powers if isinstance ( nums , ( list , ndarray , Series )): return [ decompose_single ( num , as_str ) for num in nums ] elif isinstance ( nums , int ): return decompose_single ( nums , as_str ) else : raise TypeError ( \"Invalid input type, expected int, list, or numpy array\" ) decompose ( nums , as_str = True ) staticmethod \u00b6 Decomposes a number, list, ndarray, or pandas Series of numbers representing the sum of the powers of two into a list of those powers of two. Optionally, return the string representation of the powers of two according to the StatusFlags object. Parameters: Name Type Description Default nums Union [ int , List [ int ], ndarray , Series ] The input number, list, ndarray, or pandas Series to decompose. required as_str bool Whether to return the string representation of the powers of two according to the StatusFlags object. Defaults to True. True Returns: Name Type Description List List [ int ] A list representing the sum of the powers of two if as_str is False, e.g., 10 decomposes into [2, 8]. If as_str is true, then the result would be [\u2018MAPQ\u2019, \u2018RESTRICTION_ENZYME\u2019]. Raises: Type Description TypeError If the input type is neither int, list, numpy array, nor pandas Series. ValueError If the input is a negative integer. Source code in callingcardstools/QC/StatusFlags.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 @staticmethod def decompose ( nums : Union [ int , List [ int ], ndarray , Series ], as_str : bool = True ) -> List [ int ]: \"\"\" Decomposes a number, list, ndarray, or pandas Series of numbers representing the sum of the powers of two into a list of those powers of two. Optionally, return the string representation of the powers of two according to the StatusFlags object. Args: nums (Union[int, List[int], ndarray, pd.Series]): The input number, list, ndarray, or pandas Series to decompose. as_str (bool, optional): Whether to return the string representation of the powers of two according to the StatusFlags object. Defaults to True. Returns: List: A list representing the sum of the powers of two if `as_str` is False, e.g., 10 decomposes into [2, 8]. If `as_str` is true, then the result would be ['MAPQ', 'RESTRICTION_ENZYME']. Raises: TypeError: If the input type is neither int, list, numpy array, nor pandas Series. ValueError: If the input is a negative integer. \"\"\" def decompose_single ( num : int , as_str : bool = True ) -> list : \"\"\" Helper function to decompose a single integer into powers of 2. Args: num (int): The integer to decompose. as_str (bool): Whether to return the string representation of the powers of two according to the StatusFlags object. Defaults to True. Returns: list: List of powers of two composing the input number. If `as_str` is True, the powers of two are represented by their corresponding flag names. Raises: ValueError: If the input integer is negative. \"\"\" # check input if num < 0 : raise ValueError ( \"Invalid input, expected positive int\" ) # if num is 0 and as_str is true, return NO_STATUS. otherwise, the # decomposed list will be empty if num == 0 : if as_str : return [ 'NO_STATUS' ] # Use list comprehension to find the powers of two that # compose the input number powers = [ int ( log2 ( 1 << i )) for i in range ( num . bit_length ()) if num & ( 1 << i )] if as_str : # Convert the powers of two to their string representation powers = [ StatusFlags ( int ( x )) . name for x in powers ] return powers if isinstance ( nums , ( list , ndarray , Series )): return [ decompose_single ( num , as_str ) for num in nums ] elif isinstance ( nums , int ): return decompose_single ( nums , as_str ) else : raise TypeError ( \"Invalid input type, expected int, list, or numpy array\" ) flag () \u00b6 Returns the power of 2 corresponding to the flag\u2019s value. Returns: Name Type Description int int The power of 2 corresponding to the flag\u2019s value. Source code in callingcardstools/QC/StatusFlags.py 39 40 41 42 43 44 45 46 def flag ( self ) -> int : \"\"\" Returns the power of 2 corresponding to the flag's value. Returns: int: The power of 2 corresponding to the flag's value. \"\"\" return 2 ** self . value","title":"StatusFlags"},{"location":"API/QC/StatusFlags/#callingcardstools.QC.StatusFlags.decompose","text":"Decomposes a number, list, ndarray, or pandas Series of numbers representing the sum of the powers of two into a list of those powers of two. Optionally, return the string representation of the powers of two according to the StatusFlags object. Parameters: Name Type Description Default nums Union [ int , List [ int ], ndarray , Series ] The input number, list, ndarray, or pandas Series to decompose. required as_str bool Whether to return the string representation of the powers of two according to the StatusFlags object. Defaults to True. True Returns: Name Type Description List List [ int ] A list representing the sum of the powers of two if as_str is False, e.g., 10 decomposes into [2, 8]. If as_str is true, then the result would be [\u2018MAPQ\u2019, \u2018RESTRICTION_ENZYME\u2019]. Raises: Type Description TypeError If the input type is neither int, list, numpy array, nor pandas Series. ValueError If the input is a negative integer. Source code in callingcardstools/QC/StatusFlags.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 @staticmethod def decompose ( nums : Union [ int , List [ int ], ndarray , Series ], as_str : bool = True ) -> List [ int ]: \"\"\" Decomposes a number, list, ndarray, or pandas Series of numbers representing the sum of the powers of two into a list of those powers of two. Optionally, return the string representation of the powers of two according to the StatusFlags object. Args: nums (Union[int, List[int], ndarray, pd.Series]): The input number, list, ndarray, or pandas Series to decompose. as_str (bool, optional): Whether to return the string representation of the powers of two according to the StatusFlags object. Defaults to True. Returns: List: A list representing the sum of the powers of two if `as_str` is False, e.g., 10 decomposes into [2, 8]. If `as_str` is true, then the result would be ['MAPQ', 'RESTRICTION_ENZYME']. Raises: TypeError: If the input type is neither int, list, numpy array, nor pandas Series. ValueError: If the input is a negative integer. \"\"\" def decompose_single ( num : int , as_str : bool = True ) -> list : \"\"\" Helper function to decompose a single integer into powers of 2. Args: num (int): The integer to decompose. as_str (bool): Whether to return the string representation of the powers of two according to the StatusFlags object. Defaults to True. Returns: list: List of powers of two composing the input number. If `as_str` is True, the powers of two are represented by their corresponding flag names. Raises: ValueError: If the input integer is negative. \"\"\" # check input if num < 0 : raise ValueError ( \"Invalid input, expected positive int\" ) # if num is 0 and as_str is true, return NO_STATUS. otherwise, the # decomposed list will be empty if num == 0 : if as_str : return [ 'NO_STATUS' ] # Use list comprehension to find the powers of two that # compose the input number powers = [ int ( log2 ( 1 << i )) for i in range ( num . bit_length ()) if num & ( 1 << i )] if as_str : # Convert the powers of two to their string representation powers = [ StatusFlags ( int ( x )) . name for x in powers ] return powers if isinstance ( nums , ( list , ndarray , Series )): return [ decompose_single ( num , as_str ) for num in nums ] elif isinstance ( nums , int ): return decompose_single ( nums , as_str ) else : raise TypeError ( \"Invalid input type, expected int, list, or numpy array\" )","title":"decompose()"},{"location":"API/QC/StatusFlags/#callingcardstools.QC.StatusFlags.flag","text":"Returns the power of 2 corresponding to the flag\u2019s value. Returns: Name Type Description int int The power of 2 corresponding to the flag\u2019s value. Source code in callingcardstools/QC/StatusFlags.py 39 40 41 42 43 44 45 46 def flag ( self ) -> int : \"\"\" Returns the power of 2 corresponding to the flag's value. Returns: int: The power of 2 corresponding to the flag's value. \"\"\" return 2 ** self . value","title":"flag()"},{"location":"API/QC/create_status_coder/","text":"A factory function which returns a function capable of determining the status code of a read tagged by an AlignmentTagger object. Parameters: Name Type Description Default insert_seqs list A list of acceptable insert sequences. Defaults to [\u2018*\u2019], which will skip the insert seq check altogether. ['*'] mapq_threshold int A mapq_threshold. Reads with map quality less than this value will be marked as failing the mapq threshold test. Default is 10. 10 check_5_prime_clip bool Whether to check for 5\u2019 end clipping in the read. Defaults to False. False check_passing bool Whether to check the passing key in the barcode_details dict. Defaults to True. True Returns: Type Description Callable [[ AlignedSegment ], int ] Callable[[AlignedSegment], int]: A function which given a tagged Callable [[ AlignedSegment ], int ] pysam AlignedSegment will return the status code for the read. Source code in callingcardstools/QC/create_status_coder.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def create_status_coder ( insert_seqs : list = [ '*' ], mapq_threshold : int = 10 , check_5_prime_clip : bool = False , check_passing : bool = True ) -> Callable [[ AlignedSegment ], int ]: \"\"\" A factory function which returns a function capable of determining the status code of a read tagged by an AlignmentTagger object. Args: insert_seqs (list): A list of acceptable insert sequences. Defaults to ['*'], which will skip the insert seq check altogether. mapq_threshold (int): A mapq_threshold. Reads with map quality less than this value will be marked as failing the mapq threshold test. Default is 10. check_5_prime_clip (bool): Whether to check for 5' end clipping in the read. Defaults to False. check_passing (bool, optional): Whether to check the passing key in the barcode_details dict. Defaults to True. Returns: Callable[[AlignedSegment], int]: A function which given a tagged pysam AlignedSegment will return the status code for the read. \"\"\" def coder ( read_details : AlignedSegment , status_code : int = 0 ) -> int : \"\"\" Returns the status code for a given read after checking for various flags. Args: read_details (AlignedSegment): A pysam AlignedSegment object. status_code (int, optional): Initial status code. Defaults to 0. Raises: ValueError: If read_details is not a dictionary or does not contain expected keys. KeyError: If required keys are not present in read_details. ValueError: If the types of values in read_details do not match the expected types. Returns: int: The status code for a given read. \"\"\" if not isinstance ( read_details , dict ): raise ValueError ( 'read_details must be a dictionary with ' 'keys \"read\" which is a pysam.AlignedSegment and ' '\"barcode_details\" which is a dict' ) if not { 'read' , 'barcode_details' } - read_details . keys () == set (): raise KeyError ( '\"read\" and \"barcode_details\" must be keys in' 'read_details' ) if not isinstance ( read_details . get ( 'read' ), AlignedSegment ): raise ValueError ( 'read_details[\"read\"] must be a ' 'pysam.AlignedSegment object' ) if not isinstance ( read_details . get ( 'barcode_details' ), dict ): raise ValueError ( 'read_details[\"barcode_details\"] must be a ' 'dict' ) if check_passing : if not isinstance ( read_details . get ( 'barcode_details' ) . get ( 'passing' , None ), bool ): raise KeyError ( 'passing must be a key in ' 'read_details[\"barcode_details\"]' ) # if check passing is set to false, then the passing key may not # exist. In this event, assume the read is passing if not read_details . get ( 'barcode_details' ) . get ( 'passing' , True ): status_code += StatusFlags . BARCODE . flag () # if the read is unmapped, add the flag, but don't check # other alignment metrics if read_details . get ( 'read' ) . is_unmapped : status_code += StatusFlags . UNMAPPED . flag () else : if read_details . get ( 'read' ) . is_qcfail : status_code += StatusFlags . ALIGNER_QC_FAIL . flag () if read_details . get ( 'read' ) . is_secondary or \\ read_details . get ( 'read' ) . is_supplementary : status_code += StatusFlags . NOT_PRIMARY . flag () if read_details . get ( 'read' ) . mapping_quality < mapq_threshold : status_code += StatusFlags . MAPQ . flag () # note: for mammals, this isn't necessary as the insert seq can # be checked if check_5_prime_clip : # if the read is clipped on the 5' end, flag if ( read_details . get ( 'read' ) . is_forward and read_details . get ( 'read' ) . query_alignment_start != 0 ) or \\ ( read_details . get ( 'read' ) . is_reverse and read_details . get ( 'read' ) . query_alignment_end != read_details . get ( 'read' ) . infer_query_length ()): status_code += StatusFlags . FIVE_PRIME_CLIP . flag () # check the insert sequence try : if insert_seqs != [ \"*\" ]: if read_details . get ( 'read' ) . get_tag ( \"XZ\" ) not in insert_seqs : status_code += StatusFlags . INSERT_SEQ . flag () except AttributeError as exc : logger . debug ( f \"insert sequence not found in Barcode Parser. { exc } \" ) return status_code return coder","title":"create_status_coder"},{"location":"API/Reads/ReadParser/","text":"Bases: BarcodeParser Given either single or paired end reads, use the provided barcode details json to examine expected read components Depending on the entries in the barcode details json, this class will parse the read(s), return the assembled components (this could include both what could be construed as a barcode as well as any other components), and the reads trimmed for the components labelled for trimming. Attributes: Name Type Description r1_path str File path to Read 1. r2_path str File path to Read 2. r1_handle TextIOWrapper File handle for Read 1. r2_handle TextIOWrapper File handle for Read 2. cur_r1 SeqRecord Current read record for Read 1. cur_r2 SeqRecord Current read record for Read 2. Example: >>> rb = ReadParser('/path/to/barcode_details.json') >>> r1 = next(SeqIO.parse('/path/to/r1.fq', format=\"fastq\")) >>> r2 = next(SeqIO.parse('/path/to/r2.fq', format=\"fastq\")) >>> read_dict = rb.process_read(r1,r2) Source code in callingcardstools/Reads/ReadParser.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 class ReadParser ( BarcodeParser ): \"\"\"Given either single or paired end reads, use the provided barcode details json to examine expected read components Depending on the entries in the barcode details json, this class will parse the read(s), return the assembled components (this could include both what could be construed as a barcode as well as any other components), and the reads trimmed for the components labelled for trimming. Attributes: r1_path (str): File path to Read 1. r2_path (str): File path to Read 2. r1_handle (_io.TextIOWrapper): File handle for Read 1. r2_handle (_io.TextIOWrapper): File handle for Read 2. cur_r1 (SeqRecord): Current read record for Read 1. cur_r2 (SeqRecord): Current read record for Read 2. Example: ``` >>> rb = ReadParser('/path/to/barcode_details.json') >>> r1 = next(SeqIO.parse('/path/to/r1.fq', format=\"fastq\")) >>> r2 = next(SeqIO.parse('/path/to/r2.fq', format=\"fastq\")) >>> read_dict = rb.process_read(r1,r2) ``` \"\"\" _r1_path = \"\" _r2_path = \"\" _r1_handle = \"\" _r2_handle = \"\" _cur_r1 = \"\" _cur_r2 = \"\" def __init__ ( self , barcode_details_json : str = \"\" , r1 : str = \"\" , r2 : str = \"\" ) -> None : # noqa \"\"\"Initializes the ReadParser with barcode details and optional read files. Args: barcode_details_json (str): The JSON file path containing barcode details. r1 (str): The path to the Read 1 file. r2 (str): The path to the Read 2 file. \"\"\" if barcode_details_json : super () . __init__ ( barcode_details_json ) if r1 : self . r1_path = r1 if r2 : self . r2_path = r2 def __del__ ( self ): self . close () @property def r1_path ( self ): \"\"\"filepath to read 1\"\"\" return self . _r1_path @r1_path . setter def r1_path ( self , r1_path ): try : self . fastq_path_parser ( r1_path ) except FileNotFoundError : raise except IOError : # pylint:disable=W0706 raise self . _r1_path = r1_path @property def r2_path ( self ): \"\"\"filepath to read 2\"\"\" return self . _r2_path @r2_path . setter def r2_path ( self , r2_path ): try : self . fastq_path_parser ( r2_path ) except FileNotFoundError : raise except IOError : # pylint:disable=W0706 raise self . _r2_path = r2_path @property def r1_handle ( self ): \"\"\"open SeqIO file handle to read 1\"\"\" return self . _r1_handle @r1_handle . setter def r1_handle ( self , r1_handle ): self . _r1_handle = r1_handle @property def r2_handle ( self ): \"\"\"open SeqIO file handle to read 2\"\"\" return self . _r2_handle @r2_handle . setter def r2_handle ( self , r2_handle ): self . _r2_handle = r2_handle @property def cur_r1 ( self ): \"\"\"Current read SeqRecord for read 1\"\"\" return self . _cur_r1 @cur_r1 . setter def cur_r1 ( self , r1_seqrecord ): self . _cur_r1 = r1_seqrecord @property def cur_r2 ( self ): \"\"\"Current read SeqRecord for read 2\"\"\" return self . _cur_r2 @cur_r2 . setter def cur_r2 ( self , r2_seqrecord ): self . _cur_r2 = r2_seqrecord def fastq_path_parser ( self , fq_path : str ) -> bool : \"\"\"Checks if the FastQ file path is valid. Args: fq_path (str): The path to the FastQ file. Raises: FileNotFoundError: If the FastQ file does not exist. IOError: If the FastQ file extension is not .fastq, .fq, or .gz. Returns: bool: True if the file path is valid, otherwise False. \"\"\" error_msg = 'fastq extension must be either .fastq or .fq. ' + \\ 'it may be gzipped, eg .fq.gz' fq_extensions = { '.fastq' , '.fq' } if not os . path . exists ( fq_path ): raise FileNotFoundError ( f ' { fq_path } Does Not Exist!' ) elif os . path . splitext ( fq_path )[ 1 ] not in fq_extensions : if os . path . splitext ( fq_path )[ 1 ] == \".gz\" : if not os . path . splitext ( os . path . splitext ( fq_path )[ 0 ])[ 1 ] in fq_extensions : # noqa raise IOError ( error_msg ) else : raise IOError ( error_msg ) def open ( self ) -> None : \"\"\"Opens the read file(s). If the file is gzipped, it is opened with gzip, otherwise it's opened normally. In case of paired-end reads, both files are opened. Raises: AttributeError: If the Read 1 file path is not set. \"\"\" if not self . r1_path : raise AttributeError ( 'R1 Not Set' ) elif os . path . splitext ( self . r1_path )[ 1 ] == \".gz\" : self . r1_handle = SeqIO . parse ( gzip . open ( self . r1_path , \"rt\" ), format = 'fastq' ) else : self . r1_handle = SeqIO . parse ( self . r1_path , format = 'fastq' ) if not self . r2_path : print ( 'Only R1 set -- single end mode' ) else : print ( 'Opening R1 and R2 in paired end mode' ) if os . path . splitext ( self . r2_path )[ 1 ] == \".gz\" : self . r2_handle = SeqIO . parse ( gzip . open ( self . r2_path , \"rt\" ), format = 'fastq' ) else : self . r2_handle = SeqIO . parse ( self . r2_path , format = 'fastq' ) def close ( self ) -> None : \"\"\"close file objects, if they are set\"\"\" if self . r1_handle : self . r1_handle = \"\" if self . r2_handle : self . r2_handle = \"\" def next ( self ) -> None : \"\"\"Iterate the reads and set cur_r1 and cur_r2 (if paired end) to the next read in hte file Raises: AttributeError: If R1 is not open StopIteration: If the end of file is reached in either R1 or R2 (if it is set) ValueError: If after advancing the read ids for R1 and r2 (if paired end) do not match \"\"\" r1_stop = False r2_stop = False if not self . r1_handle : raise AttributeError ( 'R1 is not open -- cannot advance' ) elif not self . r2_handle : self . cur_r1 = next ( self . r1_handle ) else : try : self . cur_r1 = next ( self . r1_handle ) except StopIteration : r1_stop = True try : self . cur_r2 = next ( self . r2_handle ) except StopIteration : r2_stop = True if r1_stop != r2_stop : error_msg = 'The length of R1 and R2 is not the same' logger . warning ( error_msg ) # pylint:disable=E1102 raise IOError ( error_msg ) elif r1_stop or r2_stop : raise StopIteration if self . cur_r1 . id != self . cur_r2 . id : error_msg = f \"r1 ID: { self . cur_r1 . id } does not match r2 \" \\ f \"fID: { self . cur_r2 . id } \" logger . critical ( error_msg ) # pylint:disable=E1102 raise ValueError ( error_msg ) def parse ( self , set_name_to_empty : bool = True ) -> dict : # noqa \"\"\"Using the barcode details, process a given read Args: set_name_to_empty (bool, optional): Set the name attribute to empty. SeqIO sets to ID if a name DNE. Defaults to True. Returns: dict: A dictionary with the r1 and r2 SeqRecord objects, the barcode and the unadulterated read ID \"\"\" # extract this in case it is augmented with the barcode later on read_id = self . cur_r1 . id # create the beginnigns out of the output dictionary read_dict = { 'r1' : self . cur_r1 , 'r2' : self . cur_r2 } # instantiate a dict to hold the sequences corresponding to # barcode components components_dict = {} # a dictionary to assist with trimming the reads offset_dict = { 'r1' : 0 , 'r2' : 0 } for end , read in read_dict . items (): if read : for k , v in self . barcode_dict [ end ] . items (): components_dict [ \"_\" . join ([ end , k ])] = str ( read . seq [ v [ 'index' ][ 0 ]: v [ 'index' ][ 1 ]]) # adjust offset for trimming if v [ 'trim' ]: left_index = v [ 'index' ][ 0 ] - ( offset_dict [ end ]) right_index = v [ 'index' ][ 1 ] - ( offset_dict [ end ]) read_dict [ end ] = \\ read_dict [ end ][: left_index ] + \\ read_dict [ end ][ right_index :] # adjust offset offset_dict [ end ] += v [ 'index' ][ 1 ] # set name to empty string if flag set if set_name_to_empty : read_dict [ 'r1' ] . name = \"\" try : read_dict [ 'r2' ] . name = \"\" except NameError : pass # add barcode to read_dict read_dict [ 'components' ] = components_dict read_dict [ 'status' ] = self . component_check ( components_dict ) # add parse-able bam_tag line to the read id append_to_read_id = [] for k , v in read_dict [ 'status' ][ 'details' ] . items (): if v . get ( 'bam_tag' , None ): tag_value = v [ 'name' ] if v [ 'dist' ] == 0 \\ else \"/\" . join ([ v [ 'name' ], str ( v [ 'dist' ])]) append_to_read_id . append ( \"-\" . join ([ v [ 'bam_tag' ], tag_value ])) append_to_read_id = \";\" . join ( append_to_read_id ) if append_to_read_id : augmented_read_id = \"_\" . join ([ read_id , append_to_read_id ]) read_dict [ 'r1' ] . id = augmented_read_id read_dict [ 'r2' ] . id = augmented_read_id read_dict [ 'id' ] = read_id return read_dict cur_r1 property writable \u00b6 Current read SeqRecord for read 1 cur_r2 property writable \u00b6 Current read SeqRecord for read 2 r1_handle property writable \u00b6 open SeqIO file handle to read 1 r1_path property writable \u00b6 filepath to read 1 r2_handle property writable \u00b6 open SeqIO file handle to read 2 r2_path property writable \u00b6 filepath to read 2 __init__ ( barcode_details_json = '' , r1 = '' , r2 = '' ) \u00b6 Initializes the ReadParser with barcode details and optional read files. Parameters: Name Type Description Default barcode_details_json str The JSON file path containing barcode details. '' r1 str The path to the Read 1 file. '' r2 str The path to the Read 2 file. '' Source code in callingcardstools/Reads/ReadParser.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def __init__ ( self , barcode_details_json : str = \"\" , r1 : str = \"\" , r2 : str = \"\" ) -> None : # noqa \"\"\"Initializes the ReadParser with barcode details and optional read files. Args: barcode_details_json (str): The JSON file path containing barcode details. r1 (str): The path to the Read 1 file. r2 (str): The path to the Read 2 file. \"\"\" if barcode_details_json : super () . __init__ ( barcode_details_json ) if r1 : self . r1_path = r1 if r2 : self . r2_path = r2 close () \u00b6 close file objects, if they are set Source code in callingcardstools/Reads/ReadParser.py 192 193 194 195 196 197 def close ( self ) -> None : \"\"\"close file objects, if they are set\"\"\" if self . r1_handle : self . r1_handle = \"\" if self . r2_handle : self . r2_handle = \"\" fastq_path_parser ( fq_path ) \u00b6 Checks if the FastQ file path is valid. Parameters: Name Type Description Default fq_path str The path to the FastQ file. required Raises: Type Description FileNotFoundError If the FastQ file does not exist. IOError If the FastQ file extension is not .fastq, .fq, or .gz. Returns: Name Type Description bool bool True if the file path is valid, otherwise False. Source code in callingcardstools/Reads/ReadParser.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def fastq_path_parser ( self , fq_path : str ) -> bool : \"\"\"Checks if the FastQ file path is valid. Args: fq_path (str): The path to the FastQ file. Raises: FileNotFoundError: If the FastQ file does not exist. IOError: If the FastQ file extension is not .fastq, .fq, or .gz. Returns: bool: True if the file path is valid, otherwise False. \"\"\" error_msg = 'fastq extension must be either .fastq or .fq. ' + \\ 'it may be gzipped, eg .fq.gz' fq_extensions = { '.fastq' , '.fq' } if not os . path . exists ( fq_path ): raise FileNotFoundError ( f ' { fq_path } Does Not Exist!' ) elif os . path . splitext ( fq_path )[ 1 ] not in fq_extensions : if os . path . splitext ( fq_path )[ 1 ] == \".gz\" : if not os . path . splitext ( os . path . splitext ( fq_path )[ 0 ])[ 1 ] in fq_extensions : # noqa raise IOError ( error_msg ) else : raise IOError ( error_msg ) next () \u00b6 Iterate the reads and set cur_r1 and cur_r2 (if paired end) to the next read in hte file Raises: Type Description AttributeError If R1 is not open StopIteration If the end of file is reached in either R1 or R2 (if it is set) ValueError If after advancing the read ids for R1 and r2 (if paired end) do not match Source code in callingcardstools/Reads/ReadParser.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def next ( self ) -> None : \"\"\"Iterate the reads and set cur_r1 and cur_r2 (if paired end) to the next read in hte file Raises: AttributeError: If R1 is not open StopIteration: If the end of file is reached in either R1 or R2 (if it is set) ValueError: If after advancing the read ids for R1 and r2 (if paired end) do not match \"\"\" r1_stop = False r2_stop = False if not self . r1_handle : raise AttributeError ( 'R1 is not open -- cannot advance' ) elif not self . r2_handle : self . cur_r1 = next ( self . r1_handle ) else : try : self . cur_r1 = next ( self . r1_handle ) except StopIteration : r1_stop = True try : self . cur_r2 = next ( self . r2_handle ) except StopIteration : r2_stop = True if r1_stop != r2_stop : error_msg = 'The length of R1 and R2 is not the same' logger . warning ( error_msg ) # pylint:disable=E1102 raise IOError ( error_msg ) elif r1_stop or r2_stop : raise StopIteration if self . cur_r1 . id != self . cur_r2 . id : error_msg = f \"r1 ID: { self . cur_r1 . id } does not match r2 \" \\ f \"fID: { self . cur_r2 . id } \" logger . critical ( error_msg ) # pylint:disable=E1102 raise ValueError ( error_msg ) open () \u00b6 Opens the read file(s). If the file is gzipped, it is opened with gzip, otherwise it\u2019s opened normally. In case of paired-end reads, both files are opened. Raises: Type Description AttributeError If the Read 1 file path is not set. Source code in callingcardstools/Reads/ReadParser.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def open ( self ) -> None : \"\"\"Opens the read file(s). If the file is gzipped, it is opened with gzip, otherwise it's opened normally. In case of paired-end reads, both files are opened. Raises: AttributeError: If the Read 1 file path is not set. \"\"\" if not self . r1_path : raise AttributeError ( 'R1 Not Set' ) elif os . path . splitext ( self . r1_path )[ 1 ] == \".gz\" : self . r1_handle = SeqIO . parse ( gzip . open ( self . r1_path , \"rt\" ), format = 'fastq' ) else : self . r1_handle = SeqIO . parse ( self . r1_path , format = 'fastq' ) if not self . r2_path : print ( 'Only R1 set -- single end mode' ) else : print ( 'Opening R1 and R2 in paired end mode' ) if os . path . splitext ( self . r2_path )[ 1 ] == \".gz\" : self . r2_handle = SeqIO . parse ( gzip . open ( self . r2_path , \"rt\" ), format = 'fastq' ) else : self . r2_handle = SeqIO . parse ( self . r2_path , format = 'fastq' ) parse ( set_name_to_empty = True ) \u00b6 Using the barcode details, process a given read Parameters: Name Type Description Default set_name_to_empty bool Set the name attribute to empty. SeqIO sets to ID if a name DNE. Defaults to True. True Returns: Name Type Description dict dict A dictionary with the r1 and r2 SeqRecord objects, the barcode and the unadulterated read ID Source code in callingcardstools/Reads/ReadParser.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 def parse ( self , set_name_to_empty : bool = True ) -> dict : # noqa \"\"\"Using the barcode details, process a given read Args: set_name_to_empty (bool, optional): Set the name attribute to empty. SeqIO sets to ID if a name DNE. Defaults to True. Returns: dict: A dictionary with the r1 and r2 SeqRecord objects, the barcode and the unadulterated read ID \"\"\" # extract this in case it is augmented with the barcode later on read_id = self . cur_r1 . id # create the beginnigns out of the output dictionary read_dict = { 'r1' : self . cur_r1 , 'r2' : self . cur_r2 } # instantiate a dict to hold the sequences corresponding to # barcode components components_dict = {} # a dictionary to assist with trimming the reads offset_dict = { 'r1' : 0 , 'r2' : 0 } for end , read in read_dict . items (): if read : for k , v in self . barcode_dict [ end ] . items (): components_dict [ \"_\" . join ([ end , k ])] = str ( read . seq [ v [ 'index' ][ 0 ]: v [ 'index' ][ 1 ]]) # adjust offset for trimming if v [ 'trim' ]: left_index = v [ 'index' ][ 0 ] - ( offset_dict [ end ]) right_index = v [ 'index' ][ 1 ] - ( offset_dict [ end ]) read_dict [ end ] = \\ read_dict [ end ][: left_index ] + \\ read_dict [ end ][ right_index :] # adjust offset offset_dict [ end ] += v [ 'index' ][ 1 ] # set name to empty string if flag set if set_name_to_empty : read_dict [ 'r1' ] . name = \"\" try : read_dict [ 'r2' ] . name = \"\" except NameError : pass # add barcode to read_dict read_dict [ 'components' ] = components_dict read_dict [ 'status' ] = self . component_check ( components_dict ) # add parse-able bam_tag line to the read id append_to_read_id = [] for k , v in read_dict [ 'status' ][ 'details' ] . items (): if v . get ( 'bam_tag' , None ): tag_value = v [ 'name' ] if v [ 'dist' ] == 0 \\ else \"/\" . join ([ v [ 'name' ], str ( v [ 'dist' ])]) append_to_read_id . append ( \"-\" . join ([ v [ 'bam_tag' ], tag_value ])) append_to_read_id = \";\" . join ( append_to_read_id ) if append_to_read_id : augmented_read_id = \"_\" . join ([ read_id , append_to_read_id ]) read_dict [ 'r1' ] . id = augmented_read_id read_dict [ 'r2' ] . id = augmented_read_id read_dict [ 'id' ] = read_id return read_dict","title":"ReadParser"},{"location":"API/Reads/ReadParser/#callingcardstools.Reads.ReadParser.ReadParser.cur_r1","text":"Current read SeqRecord for read 1","title":"cur_r1"},{"location":"API/Reads/ReadParser/#callingcardstools.Reads.ReadParser.ReadParser.cur_r2","text":"Current read SeqRecord for read 2","title":"cur_r2"},{"location":"API/Reads/ReadParser/#callingcardstools.Reads.ReadParser.ReadParser.r1_handle","text":"open SeqIO file handle to read 1","title":"r1_handle"},{"location":"API/Reads/ReadParser/#callingcardstools.Reads.ReadParser.ReadParser.r1_path","text":"filepath to read 1","title":"r1_path"},{"location":"API/Reads/ReadParser/#callingcardstools.Reads.ReadParser.ReadParser.r2_handle","text":"open SeqIO file handle to read 2","title":"r2_handle"},{"location":"API/Reads/ReadParser/#callingcardstools.Reads.ReadParser.ReadParser.r2_path","text":"filepath to read 2","title":"r2_path"},{"location":"API/Reads/ReadParser/#callingcardstools.Reads.ReadParser.ReadParser.__init__","text":"Initializes the ReadParser with barcode details and optional read files. Parameters: Name Type Description Default barcode_details_json str The JSON file path containing barcode details. '' r1 str The path to the Read 1 file. '' r2 str The path to the Read 2 file. '' Source code in callingcardstools/Reads/ReadParser.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def __init__ ( self , barcode_details_json : str = \"\" , r1 : str = \"\" , r2 : str = \"\" ) -> None : # noqa \"\"\"Initializes the ReadParser with barcode details and optional read files. Args: barcode_details_json (str): The JSON file path containing barcode details. r1 (str): The path to the Read 1 file. r2 (str): The path to the Read 2 file. \"\"\" if barcode_details_json : super () . __init__ ( barcode_details_json ) if r1 : self . r1_path = r1 if r2 : self . r2_path = r2","title":"__init__()"},{"location":"API/Reads/ReadParser/#callingcardstools.Reads.ReadParser.ReadParser.close","text":"close file objects, if they are set Source code in callingcardstools/Reads/ReadParser.py 192 193 194 195 196 197 def close ( self ) -> None : \"\"\"close file objects, if they are set\"\"\" if self . r1_handle : self . r1_handle = \"\" if self . r2_handle : self . r2_handle = \"\"","title":"close()"},{"location":"API/Reads/ReadParser/#callingcardstools.Reads.ReadParser.ReadParser.fastq_path_parser","text":"Checks if the FastQ file path is valid. Parameters: Name Type Description Default fq_path str The path to the FastQ file. required Raises: Type Description FileNotFoundError If the FastQ file does not exist. IOError If the FastQ file extension is not .fastq, .fq, or .gz. Returns: Name Type Description bool bool True if the file path is valid, otherwise False. Source code in callingcardstools/Reads/ReadParser.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def fastq_path_parser ( self , fq_path : str ) -> bool : \"\"\"Checks if the FastQ file path is valid. Args: fq_path (str): The path to the FastQ file. Raises: FileNotFoundError: If the FastQ file does not exist. IOError: If the FastQ file extension is not .fastq, .fq, or .gz. Returns: bool: True if the file path is valid, otherwise False. \"\"\" error_msg = 'fastq extension must be either .fastq or .fq. ' + \\ 'it may be gzipped, eg .fq.gz' fq_extensions = { '.fastq' , '.fq' } if not os . path . exists ( fq_path ): raise FileNotFoundError ( f ' { fq_path } Does Not Exist!' ) elif os . path . splitext ( fq_path )[ 1 ] not in fq_extensions : if os . path . splitext ( fq_path )[ 1 ] == \".gz\" : if not os . path . splitext ( os . path . splitext ( fq_path )[ 0 ])[ 1 ] in fq_extensions : # noqa raise IOError ( error_msg ) else : raise IOError ( error_msg )","title":"fastq_path_parser()"},{"location":"API/Reads/ReadParser/#callingcardstools.Reads.ReadParser.ReadParser.next","text":"Iterate the reads and set cur_r1 and cur_r2 (if paired end) to the next read in hte file Raises: Type Description AttributeError If R1 is not open StopIteration If the end of file is reached in either R1 or R2 (if it is set) ValueError If after advancing the read ids for R1 and r2 (if paired end) do not match Source code in callingcardstools/Reads/ReadParser.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 def next ( self ) -> None : \"\"\"Iterate the reads and set cur_r1 and cur_r2 (if paired end) to the next read in hte file Raises: AttributeError: If R1 is not open StopIteration: If the end of file is reached in either R1 or R2 (if it is set) ValueError: If after advancing the read ids for R1 and r2 (if paired end) do not match \"\"\" r1_stop = False r2_stop = False if not self . r1_handle : raise AttributeError ( 'R1 is not open -- cannot advance' ) elif not self . r2_handle : self . cur_r1 = next ( self . r1_handle ) else : try : self . cur_r1 = next ( self . r1_handle ) except StopIteration : r1_stop = True try : self . cur_r2 = next ( self . r2_handle ) except StopIteration : r2_stop = True if r1_stop != r2_stop : error_msg = 'The length of R1 and R2 is not the same' logger . warning ( error_msg ) # pylint:disable=E1102 raise IOError ( error_msg ) elif r1_stop or r2_stop : raise StopIteration if self . cur_r1 . id != self . cur_r2 . id : error_msg = f \"r1 ID: { self . cur_r1 . id } does not match r2 \" \\ f \"fID: { self . cur_r2 . id } \" logger . critical ( error_msg ) # pylint:disable=E1102 raise ValueError ( error_msg )","title":"next()"},{"location":"API/Reads/ReadParser/#callingcardstools.Reads.ReadParser.ReadParser.open","text":"Opens the read file(s). If the file is gzipped, it is opened with gzip, otherwise it\u2019s opened normally. In case of paired-end reads, both files are opened. Raises: Type Description AttributeError If the Read 1 file path is not set. Source code in callingcardstools/Reads/ReadParser.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def open ( self ) -> None : \"\"\"Opens the read file(s). If the file is gzipped, it is opened with gzip, otherwise it's opened normally. In case of paired-end reads, both files are opened. Raises: AttributeError: If the Read 1 file path is not set. \"\"\" if not self . r1_path : raise AttributeError ( 'R1 Not Set' ) elif os . path . splitext ( self . r1_path )[ 1 ] == \".gz\" : self . r1_handle = SeqIO . parse ( gzip . open ( self . r1_path , \"rt\" ), format = 'fastq' ) else : self . r1_handle = SeqIO . parse ( self . r1_path , format = 'fastq' ) if not self . r2_path : print ( 'Only R1 set -- single end mode' ) else : print ( 'Opening R1 and R2 in paired end mode' ) if os . path . splitext ( self . r2_path )[ 1 ] == \".gz\" : self . r2_handle = SeqIO . parse ( gzip . open ( self . r2_path , \"rt\" ), format = 'fastq' ) else : self . r2_handle = SeqIO . parse ( self . r2_path , format = 'fastq' )","title":"open()"},{"location":"API/Reads/ReadParser/#callingcardstools.Reads.ReadParser.ReadParser.parse","text":"Using the barcode details, process a given read Parameters: Name Type Description Default set_name_to_empty bool Set the name attribute to empty. SeqIO sets to ID if a name DNE. Defaults to True. True Returns: Name Type Description dict dict A dictionary with the r1 and r2 SeqRecord objects, the barcode and the unadulterated read ID Source code in callingcardstools/Reads/ReadParser.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 def parse ( self , set_name_to_empty : bool = True ) -> dict : # noqa \"\"\"Using the barcode details, process a given read Args: set_name_to_empty (bool, optional): Set the name attribute to empty. SeqIO sets to ID if a name DNE. Defaults to True. Returns: dict: A dictionary with the r1 and r2 SeqRecord objects, the barcode and the unadulterated read ID \"\"\" # extract this in case it is augmented with the barcode later on read_id = self . cur_r1 . id # create the beginnigns out of the output dictionary read_dict = { 'r1' : self . cur_r1 , 'r2' : self . cur_r2 } # instantiate a dict to hold the sequences corresponding to # barcode components components_dict = {} # a dictionary to assist with trimming the reads offset_dict = { 'r1' : 0 , 'r2' : 0 } for end , read in read_dict . items (): if read : for k , v in self . barcode_dict [ end ] . items (): components_dict [ \"_\" . join ([ end , k ])] = str ( read . seq [ v [ 'index' ][ 0 ]: v [ 'index' ][ 1 ]]) # adjust offset for trimming if v [ 'trim' ]: left_index = v [ 'index' ][ 0 ] - ( offset_dict [ end ]) right_index = v [ 'index' ][ 1 ] - ( offset_dict [ end ]) read_dict [ end ] = \\ read_dict [ end ][: left_index ] + \\ read_dict [ end ][ right_index :] # adjust offset offset_dict [ end ] += v [ 'index' ][ 1 ] # set name to empty string if flag set if set_name_to_empty : read_dict [ 'r1' ] . name = \"\" try : read_dict [ 'r2' ] . name = \"\" except NameError : pass # add barcode to read_dict read_dict [ 'components' ] = components_dict read_dict [ 'status' ] = self . component_check ( components_dict ) # add parse-able bam_tag line to the read id append_to_read_id = [] for k , v in read_dict [ 'status' ][ 'details' ] . items (): if v . get ( 'bam_tag' , None ): tag_value = v [ 'name' ] if v [ 'dist' ] == 0 \\ else \"/\" . join ([ v [ 'name' ], str ( v [ 'dist' ])]) append_to_read_id . append ( \"-\" . join ([ v [ 'bam_tag' ], tag_value ])) append_to_read_id = \";\" . join ( append_to_read_id ) if append_to_read_id : augmented_read_id = \"_\" . join ([ read_id , append_to_read_id ]) read_dict [ 'r1' ] . id = augmented_read_id read_dict [ 'r2' ] . id = augmented_read_id read_dict [ 'id' ] = read_id return read_dict","title":"parse()"},{"location":"API/Resources/Resources/","text":"An object to provide access to package resources to the user Source code in callingcardstools/Resources/Resources.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Resources : \"\"\"An object to provide access to package resources to the user\"\"\" def __init__ ( self ) -> None : self . _configured_organisms = [ 'yeast' , 'mammals' ] self . _yeast = { 'barcode_details' : pkg_resources . read_text ( yeast , \"barcode_details.json\" ) } self . _mammals = { 'barcode_details' : pkg_resources . read_text ( mammals , \"barcode_details.json\" ), } @property def configured_organisms ( self ): \"\"\"list of organisms for which there are resources\"\"\" return self . _configured_organisms @property def yeast ( self ): \"\"\"dict of paths to resources for yeast\"\"\" return self . _yeast @property def mammals ( self ): \"\"\"dict of paths to resources for mammals\"\"\" return self . _mammals configured_organisms property \u00b6 list of organisms for which there are resources mammals property \u00b6 dict of paths to resources for mammals yeast property \u00b6 dict of paths to resources for yeast","title":"Resources"},{"location":"API/Resources/Resources/#callingcardstools.Resources.Resources.Resources.configured_organisms","text":"list of organisms for which there are resources","title":"configured_organisms"},{"location":"API/Resources/Resources/#callingcardstools.Resources.Resources.Resources.mammals","text":"dict of paths to resources for mammals","title":"mammals"},{"location":"API/Resources/Resources/#callingcardstools.Resources.Resources.Resources.yeast","text":"dict of paths to resources for yeast","title":"yeast"},{"location":"file_format_specs/barcode_details/","text":"Barcode Details Json \u00b6 The Barcode Details Json provides callingCardsTools with information about what and where to expect non-genomic sequence which are included in the raw reads and serve to both demultiplex, in the case of yeast data, and confirm that a given read represents a transposon insertion. The mammals data barcode details is generally static \u2013 you can use the same barcode details for all data. However, the yeast barcode details must be adjusted for each specific library. Yeast Barcode Details Format \u00b6 It is most convenient to store the TF barcode sequences as a tsv and then use the cmd line tool callingcardstools barcode_table_to_json to convert the table to the appropriate json template. Save a tsv in the following format: Do not include a header MTH1 GTCCC CAGAGGGG SKN7 TCAAG ATCAGACC HAP3 AATGA GGGGGTAG The output of callingcardstools barcode_table_to_json -t run_6354_bc_table.tsv -r run_6354 will be a json in the following format: { \"r1\": { \"primer\": { \"trim\": true, \"index\": [ 0, 5 ] }, \"transposon\": { \"trim\": true, \"index\": [ 5, 22 ] } }, \"r2\": { \"transposon\": { \"trim\": true, \"index\": [ 0, 8 ] }, \"restriction\": { \"trim\": true, \"index\": [ 8, 20 ] } }, \"components\": { \"r1_transposon\": { \"map\": [ \"AATTCACTACGTCAACA\" ], \"bam_tag\": \"RT\" }, \"r2_restriction\": { \"map\": { \"TCGAGCGCCCGG\": \"Hpall\", \"TCGAGCGC\": \"HinP1I\", \"TCGA\": \"TaqAI\" }, \"match_type\": \"greedy\", \"require\": false, \"bam_tag\": \"RS\" }, \"tf\": { \"components\": [ \"r1_primer\", \"r2_transposon\" ], \"map\": { \"GTCCCCAGAGGGG\": \"MTH1\", \"TCAAGATCAGACC\": \"SKN7\", \"AATGAGGGGGTAG\": \"HAP4\" }, \"bam_tag\": \"TF\" } }, \"match_allowance\": { \"r1_transposon\": 0 }, \"batch\": \"run_6354\" } Mammals Barcode Details Format \u00b6 Since this will generally be the same for all mammals Calling Cards data, you can likely simply copy and paste this onto your system and use it directly: { \"batch\": \"\", \"tf\": \"\", \"r1\": { \"pb\": {\"trim\": true, \"index\": [0,3]}, \"ltr1\": {\"trim\": true, \"index\": [3,28]}, \"srt\": {\"trim\": true, \"index\":[28,32]}, \"ltr2\": {\"trim\": true, \"index\": [32,38]} }, \"r2\":{}, \"components\": { \"r1_pb\": {\"map\":[\"TAG\"], \"match_allowance\": 0, \"bam_tag\": \"PB\"}, \"r1_ltr1\": {\"map\": [\"CGTCAATTTTACGCAGACTATCTTT\"], \"match_type\": \"edit_distance\", \"match_allowance\": 0, \"require\": true, \"bam_tag\": \"L1\"}, \"r1_srt\": {\"map\": [\"CTAG\", \"CAAC\", \"CTGA\", \"GCAT\", \"GTAC\", \"CACA\", \"TGAC\", \"GTCA\", \"CGAT\", \"CTCT\", \"GAAG\", \"TCGA\", \"CATG\", \"GTTG\", \"CTTC\", \"GCTA\", \"GAGA\", \"GTGT\", \"CGTA\", \"TGGT\", \"GGAA\", \"ACAC\", \"TCAG\", \"TTGG\", \"CAGT\", \"TTTT\"], \"match_type\": \"edit_distance\", \"match_allowance\": 0, \"require\": true, \"bam_tag\": \"ST\", \"annotation\": true}, \"r1_ltr2\": {\"map\": [\"GGTTAA\"], \"match_type\": \"edit_distance\", \"match_allowance\": 0, \"require\": true, \"bam_tag\": \"L2\"} }, \"insert_seq\": [\"TTAA\"], \"max_mismatch\": 0 }","title":"barcode_details"},{"location":"file_format_specs/barcode_details/#barcode-details-json","text":"The Barcode Details Json provides callingCardsTools with information about what and where to expect non-genomic sequence which are included in the raw reads and serve to both demultiplex, in the case of yeast data, and confirm that a given read represents a transposon insertion. The mammals data barcode details is generally static \u2013 you can use the same barcode details for all data. However, the yeast barcode details must be adjusted for each specific library.","title":"Barcode Details Json"},{"location":"file_format_specs/barcode_details/#yeast-barcode-details-format","text":"It is most convenient to store the TF barcode sequences as a tsv and then use the cmd line tool callingcardstools barcode_table_to_json to convert the table to the appropriate json template. Save a tsv in the following format: Do not include a header MTH1 GTCCC CAGAGGGG SKN7 TCAAG ATCAGACC HAP3 AATGA GGGGGTAG The output of callingcardstools barcode_table_to_json -t run_6354_bc_table.tsv -r run_6354 will be a json in the following format: { \"r1\": { \"primer\": { \"trim\": true, \"index\": [ 0, 5 ] }, \"transposon\": { \"trim\": true, \"index\": [ 5, 22 ] } }, \"r2\": { \"transposon\": { \"trim\": true, \"index\": [ 0, 8 ] }, \"restriction\": { \"trim\": true, \"index\": [ 8, 20 ] } }, \"components\": { \"r1_transposon\": { \"map\": [ \"AATTCACTACGTCAACA\" ], \"bam_tag\": \"RT\" }, \"r2_restriction\": { \"map\": { \"TCGAGCGCCCGG\": \"Hpall\", \"TCGAGCGC\": \"HinP1I\", \"TCGA\": \"TaqAI\" }, \"match_type\": \"greedy\", \"require\": false, \"bam_tag\": \"RS\" }, \"tf\": { \"components\": [ \"r1_primer\", \"r2_transposon\" ], \"map\": { \"GTCCCCAGAGGGG\": \"MTH1\", \"TCAAGATCAGACC\": \"SKN7\", \"AATGAGGGGGTAG\": \"HAP4\" }, \"bam_tag\": \"TF\" } }, \"match_allowance\": { \"r1_transposon\": 0 }, \"batch\": \"run_6354\" }","title":"Yeast Barcode Details Format"},{"location":"file_format_specs/barcode_details/#mammals-barcode-details-format","text":"Since this will generally be the same for all mammals Calling Cards data, you can likely simply copy and paste this onto your system and use it directly: { \"batch\": \"\", \"tf\": \"\", \"r1\": { \"pb\": {\"trim\": true, \"index\": [0,3]}, \"ltr1\": {\"trim\": true, \"index\": [3,28]}, \"srt\": {\"trim\": true, \"index\":[28,32]}, \"ltr2\": {\"trim\": true, \"index\": [32,38]} }, \"r2\":{}, \"components\": { \"r1_pb\": {\"map\":[\"TAG\"], \"match_allowance\": 0, \"bam_tag\": \"PB\"}, \"r1_ltr1\": {\"map\": [\"CGTCAATTTTACGCAGACTATCTTT\"], \"match_type\": \"edit_distance\", \"match_allowance\": 0, \"require\": true, \"bam_tag\": \"L1\"}, \"r1_srt\": {\"map\": [\"CTAG\", \"CAAC\", \"CTGA\", \"GCAT\", \"GTAC\", \"CACA\", \"TGAC\", \"GTCA\", \"CGAT\", \"CTCT\", \"GAAG\", \"TCGA\", \"CATG\", \"GTTG\", \"CTTC\", \"GCTA\", \"GAGA\", \"GTGT\", \"CGTA\", \"TGGT\", \"GGAA\", \"ACAC\", \"TCAG\", \"TTGG\", \"CAGT\", \"TTTT\"], \"match_type\": \"edit_distance\", \"match_allowance\": 0, \"require\": true, \"bam_tag\": \"ST\", \"annotation\": true}, \"r1_ltr2\": {\"map\": [\"GGTTAA\"], \"match_type\": \"edit_distance\", \"match_allowance\": 0, \"require\": true, \"bam_tag\": \"L2\"} }, \"insert_seq\": [\"TTAA\"], \"max_mismatch\": 0 }","title":"Mammals Barcode Details Format"},{"location":"file_format_specs/qbed/","text":"qBed \u00b6 qBed files are 0 indexed, half open with the following fields: \u2018chr\u2019, \u2018start\u2019, \u2018end\u2019, \u2018depth\u2019, \u2018strand\u2019, \u2018annotation\u2019 Where the first three fields are required. See the following paper for a fuller definition, along with examples: qBed Format","title":"qBed"},{"location":"file_format_specs/qbed/#qbed","text":"qBed files are 0 indexed, half open with the following fields: \u2018chr\u2019, \u2018start\u2019, \u2018end\u2019, \u2018depth\u2019, \u2018strand\u2019, \u2018annotation\u2019 Where the first three fields are required. See the following paper for a fuller definition, along with examples: qBed Format","title":"qBed"},{"location":"file_format_specs/yeast_rank_response/","text":"Yeast Rank Response Configuration \u00b6 The yeast_rank_response tool requires a json configuration file as input. The configuration file specifies the location of the input files and various other settings. Below, you will find first an example and next a table defining each key. In the definitions table, if a key/value pair is not marked as [REQUIRED], then there is a default which is set in the tool. [REQUIRED] key/value pairs must be present. Example \u00b6 { \"binding_data_path\": \"path/to/binding/data\", \"binding_source\": \"eg, 'chipexo_1234' or cc_1234'\", \"binding_identifier_col\": \"identifier_column_name\", \"binding_effect_col\": \"effect_column_name_or_none\", \"binding_pvalue_col\": \"pvalue_column_name_or_none\", \"rank_by_binding_effect\": false, \"expression_data_path\": \"path/to/expression/data\", \"binding_source\": \"eg, 'mcisaac_1234' or kemmeren_1234'\", \"expression_identifier_col\": \"identifier_column_name\", \"expression_effect_col\": \"effect_column_name_or_none\", \"expression_effect_thres\": 0.0, \"expression_pvalue_col\": \"pvalue_column_name_or_none\", \"expression_pvalue_thres\": 0.05, \"rank_bin_size\": 5, \"normalize\": false, \"output_file\": \"rank_response.csv\", \"compress\": false } Definitions \u00b6 Key Description binding_data_path [REQUIRED] Path to the binding data file. The binding_effect_col , binding_pval_col , and \u2018gene_id\u2019 are required. binding_source [REQUIRED] A description of where the binding data comes from. This might just be the data source, eg \u2018chipexo\u2019, or it might be a identifier, eg callingcards_17 or cc_17 binding_identifier_col [REQUIRED] Name of the feature identifier column in the binding data. binding_effect_col [REQUIRED] Name of the effect column in the binding data. Set to none if an effect column does not exist. binding_pvalue_col [REQUIRED] Name of the pvalue column in the binding data. Set to none if a pvalue column does not exist. rank_by_binding_effect true or false . Defaults to false if not provided. Set to true to rank by the binding effect column. expression_data_path [REQUIRED] Path to the expression data file. expression_source [REQUIRED] A description of where the expressin data comes from. This might just be the data source, eg \u2018mcisaac\u2019, or it might be a identifier, eg mcisaac_17 expression_identifier_col [REQUIRED] Name of the feature identifier column in the expression data. expression_effect_col [REQUIRED] Name of the effect column in the gene expression data. Set to none if an effect column does not exist. expression_effect_thres [REQUIRED] Threshold for effect expression. Set to none if an effect column does not exist expression_pvalue_col [REQUIRED] Name of the pvalue column in the gene expression data. Set to none if a pvalue column does not exist. expression_pvalue_thres [REQUIRED] Threshold for pvalue of effect expression. Set to none if a pvalue column does not exist rank_bin_size Defaults to 5 if not provided. Bin size for rank response. normalize This is not currently implemented \u2013 it is a placeholder for future development when list input of binding/effect data is supported. true or false . output_file Path to the output file. Defaults to rank_response.csv . compress Set this flag to gzip the output file. Defaults to false","title":"yeast_rank_response"},{"location":"file_format_specs/yeast_rank_response/#yeast-rank-response-configuration","text":"The yeast_rank_response tool requires a json configuration file as input. The configuration file specifies the location of the input files and various other settings. Below, you will find first an example and next a table defining each key. In the definitions table, if a key/value pair is not marked as [REQUIRED], then there is a default which is set in the tool. [REQUIRED] key/value pairs must be present.","title":"Yeast Rank Response Configuration"},{"location":"file_format_specs/yeast_rank_response/#example","text":"{ \"binding_data_path\": \"path/to/binding/data\", \"binding_source\": \"eg, 'chipexo_1234' or cc_1234'\", \"binding_identifier_col\": \"identifier_column_name\", \"binding_effect_col\": \"effect_column_name_or_none\", \"binding_pvalue_col\": \"pvalue_column_name_or_none\", \"rank_by_binding_effect\": false, \"expression_data_path\": \"path/to/expression/data\", \"binding_source\": \"eg, 'mcisaac_1234' or kemmeren_1234'\", \"expression_identifier_col\": \"identifier_column_name\", \"expression_effect_col\": \"effect_column_name_or_none\", \"expression_effect_thres\": 0.0, \"expression_pvalue_col\": \"pvalue_column_name_or_none\", \"expression_pvalue_thres\": 0.05, \"rank_bin_size\": 5, \"normalize\": false, \"output_file\": \"rank_response.csv\", \"compress\": false }","title":"Example"},{"location":"file_format_specs/yeast_rank_response/#definitions","text":"Key Description binding_data_path [REQUIRED] Path to the binding data file. The binding_effect_col , binding_pval_col , and \u2018gene_id\u2019 are required. binding_source [REQUIRED] A description of where the binding data comes from. This might just be the data source, eg \u2018chipexo\u2019, or it might be a identifier, eg callingcards_17 or cc_17 binding_identifier_col [REQUIRED] Name of the feature identifier column in the binding data. binding_effect_col [REQUIRED] Name of the effect column in the binding data. Set to none if an effect column does not exist. binding_pvalue_col [REQUIRED] Name of the pvalue column in the binding data. Set to none if a pvalue column does not exist. rank_by_binding_effect true or false . Defaults to false if not provided. Set to true to rank by the binding effect column. expression_data_path [REQUIRED] Path to the expression data file. expression_source [REQUIRED] A description of where the expressin data comes from. This might just be the data source, eg \u2018mcisaac\u2019, or it might be a identifier, eg mcisaac_17 expression_identifier_col [REQUIRED] Name of the feature identifier column in the expression data. expression_effect_col [REQUIRED] Name of the effect column in the gene expression data. Set to none if an effect column does not exist. expression_effect_thres [REQUIRED] Threshold for effect expression. Set to none if an effect column does not exist expression_pvalue_col [REQUIRED] Name of the pvalue column in the gene expression data. Set to none if a pvalue column does not exist. expression_pvalue_thres [REQUIRED] Threshold for pvalue of effect expression. Set to none if a pvalue column does not exist rank_bin_size Defaults to 5 if not provided. Bin size for rank response. normalize This is not currently implemented \u2013 it is a placeholder for future development when list input of binding/effect data is supported. true or false . output_file Path to the output file. Defaults to rank_response.csv . compress Set this flag to gzip the output file. Defaults to false","title":"Definitions"},{"location":"home/","text":"","title":"Index"},{"location":"home/changelog/","text":"Change Log \u00b6 version 1.8.1 \u00b6 Changes \u00b6 Fixing an inaccuracy in the poisson pvalue contribution. See issue #16 version 1.6.0 \u00b6 Changes \u00b6 enrichment_vectorized has been re-written such that the pseudocount is only applied to the background in a given promoter region. poisson_pvalue has been re-written to remove the pseudocount from the experiment_hops in the promoter regions adding a significant amount of error handling and input/output validity checks to the metric calculations in yeast Peak Calling chrmap nrow must be > 0 and ncol > 1 background data (hops) must have at least one hop. Will error if not. experiment data (hops) must have at least one hop. Will error if not. promoter set files must have at least 1 promoter region enrichment input and output is checked for type expectations Old un-vectorized enrichment, poisson and hypergeometric code files removed #Version 1.5.2 \u00b6 Changes \u00b6 added kwargs arguments to PeakCalling.yeast.call_peaks to allow user to pass in validation method on pyranges join, background_total_hops and experiment_total_hops. moved promoters_df to promoters_pr conversion in PealCalling.yeast.call_peaks from call_peaks to external function. Also corrected the slack in the join method where the overlaps are counted. Now in the conversion method, the End is incremented by 1 to allow hops on the right endpoint, whatever that is, to be counted. Version 1.5.1 \u00b6 Changes \u00b6 Needed to keep name in the output of PeakCalling.yeast.call_peaks adding Analysis and PeakCalling modules to the documentation API section Version 1.5.0 \u00b6 Changes \u00b6 overhaul of the PeakCalling/yeast module to address memory usage. adding pyranges as a depedency as a result. removed consider_strand and added a argument to deduplicate the experiment qbeds based on chr , start , end Version 1.4.1 \u00b6 Changes \u00b6 chipexo_promoter_sig now checks the columns and expects the original yeastepigenome.org allevents coord column to be split into start and end where end is simply coord + 1. This is to be consistent with the other bed-type files. Version 1.4.0 \u00b6 Additions \u00b6 For yeast, changing the yeast_call_peaks consider_strand functionality to collapse read counts at the same coordinate on the forward/reverse strand in addition to ignoring the strand with regards to the promoter. Version 1.3.0 \u00b6 Additions \u00b6 For yeast, adding peak calling and analysis functionality. This includes the following to the cmd line: - yeast_call_peaks - call peaks on a yeast qBED file - yeast_chipexo_promoter_sig - calculate the significance of the promoter signal over a given set of promoter regions - yeast_rank_response - Given a promoter set and expression data, compare the binding and expression sets Changes \u00b6 the yeast barcode qc summary outputs the actual r1/r2 sequences as opposed to just the edit distance equivalent classes Version 1.2.0 \u00b6 Bug fixes \u00b6 Removing the deprecated (and removed in pandas 2.0) DataFrame.append function calls from Qbed.py Version 1.1.0 \u00b6 Bug fixes \u00b6 Adding SRT annotation to mammals qBED output Features/not bugs \u00b6 Remove header from mammals qBed Fix typo in mammals barcode details \u2013 the component identified with lrt should be (and now is) ltr Version 1.0.0 \u00b6 Initial release","title":"Changelog"},{"location":"home/changelog/#change-log","text":"","title":"Change Log"},{"location":"home/changelog/#version-181","text":"","title":"version 1.8.1"},{"location":"home/changelog/#changes","text":"Fixing an inaccuracy in the poisson pvalue contribution. See issue #16","title":"Changes"},{"location":"home/changelog/#version-160","text":"","title":"version 1.6.0"},{"location":"home/changelog/#changes_1","text":"enrichment_vectorized has been re-written such that the pseudocount is only applied to the background in a given promoter region. poisson_pvalue has been re-written to remove the pseudocount from the experiment_hops in the promoter regions adding a significant amount of error handling and input/output validity checks to the metric calculations in yeast Peak Calling chrmap nrow must be > 0 and ncol > 1 background data (hops) must have at least one hop. Will error if not. experiment data (hops) must have at least one hop. Will error if not. promoter set files must have at least 1 promoter region enrichment input and output is checked for type expectations Old un-vectorized enrichment, poisson and hypergeometric code files removed","title":"Changes"},{"location":"home/changelog/#version-152","text":"","title":"#Version 1.5.2"},{"location":"home/changelog/#changes_2","text":"added kwargs arguments to PeakCalling.yeast.call_peaks to allow user to pass in validation method on pyranges join, background_total_hops and experiment_total_hops. moved promoters_df to promoters_pr conversion in PealCalling.yeast.call_peaks from call_peaks to external function. Also corrected the slack in the join method where the overlaps are counted. Now in the conversion method, the End is incremented by 1 to allow hops on the right endpoint, whatever that is, to be counted.","title":"Changes"},{"location":"home/changelog/#version-151","text":"","title":"Version 1.5.1"},{"location":"home/changelog/#changes_3","text":"Needed to keep name in the output of PeakCalling.yeast.call_peaks adding Analysis and PeakCalling modules to the documentation API section","title":"Changes"},{"location":"home/changelog/#version-150","text":"","title":"Version 1.5.0"},{"location":"home/changelog/#changes_4","text":"overhaul of the PeakCalling/yeast module to address memory usage. adding pyranges as a depedency as a result. removed consider_strand and added a argument to deduplicate the experiment qbeds based on chr , start , end","title":"Changes"},{"location":"home/changelog/#version-141","text":"","title":"Version 1.4.1"},{"location":"home/changelog/#changes_5","text":"chipexo_promoter_sig now checks the columns and expects the original yeastepigenome.org allevents coord column to be split into start and end where end is simply coord + 1. This is to be consistent with the other bed-type files.","title":"Changes"},{"location":"home/changelog/#version-140","text":"","title":"Version 1.4.0"},{"location":"home/changelog/#additions","text":"For yeast, changing the yeast_call_peaks consider_strand functionality to collapse read counts at the same coordinate on the forward/reverse strand in addition to ignoring the strand with regards to the promoter.","title":"Additions"},{"location":"home/changelog/#version-130","text":"","title":"Version 1.3.0"},{"location":"home/changelog/#additions_1","text":"For yeast, adding peak calling and analysis functionality. This includes the following to the cmd line: - yeast_call_peaks - call peaks on a yeast qBED file - yeast_chipexo_promoter_sig - calculate the significance of the promoter signal over a given set of promoter regions - yeast_rank_response - Given a promoter set and expression data, compare the binding and expression sets","title":"Additions"},{"location":"home/changelog/#changes_6","text":"the yeast barcode qc summary outputs the actual r1/r2 sequences as opposed to just the edit distance equivalent classes","title":"Changes"},{"location":"home/changelog/#version-120","text":"","title":"Version 1.2.0"},{"location":"home/changelog/#bug-fixes","text":"Removing the deprecated (and removed in pandas 2.0) DataFrame.append function calls from Qbed.py","title":"Bug fixes"},{"location":"home/changelog/#version-110","text":"","title":"Version 1.1.0"},{"location":"home/changelog/#bug-fixes_1","text":"Adding SRT annotation to mammals qBED output","title":"Bug fixes"},{"location":"home/changelog/#featuresnot-bugs","text":"Remove header from mammals qBed Fix typo in mammals barcode details \u2013 the component identified with lrt should be (and now is) ltr","title":"Features/not bugs"},{"location":"home/changelog/#version-100","text":"Initial release","title":"Version 1.0.0"},{"location":"home/contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. Fork the repo \u00b6 Fork the repo into your personal profile. This will allow you to make changes and push them to your forked repo. When you\u2019re ready, issue a pull request to the main repo. Issues \u00b6 Ideally, create an issue first. This will allow us to discuss the feature or bug fix you want to implement. Pull Requests \u00b6 Issue pull requests against the dev branch. Please make sure that your code is covered by a test (see below). Environment setup \u00b6 install poetry I prefer to set the default location of the virtual environment to the project directory. You can set that as a global configuration for your poetry installation like so: poetry config virtualenvs.in-project true git clone the repo cd into the repo and issue the command poetry install shell into the virtual environment with poetry shell you can pip install -e . to install the package in editable mode. This is useful if you want to test the cmd line interface as you make changes to the source code. Tests are reproducible debugging tools! \u00b6 There is a test suite provided with this package which is intended to give any future Calling Cards developers who might use this an easier way into the code. All code should be accessible from the current tests, and the current tests can be used to give some assurance that a new change doesn\u2019t absolutely break any functionality. However , the purpose of the tests is not to prove correctness. Rather, it is a record of the debugging that I have done. It makes the debugging reproducible! The real benefit to me in the future, or you, if you have inherited this, is that you can use VScode to set breakpoints in the code. This means you don\u2019t have to read my documentation or guess at what I was trying to do. Here is an example: What you\u2019re seeing here is a a dummy test on the right. Notice that this test does nothing! it just says that 2 == 2. However, it is importing a fixture . A fixture is a test data object \u2013 in this case, it is a SummaryParser object that is loaded from test data that is provided in the package repository. Notice that I have set a breakpoint on the SummaryParser() constructor. This isn\u2019t necessarily the most direct way or setting up this test, but it provides an example of both a fixture, and proves that tests don\u2019t need to actually test anything at all to be useful. Why is this useful? Because now you can \u201cstep into\u201d the SummaryParser constructor. When I wrote the constructor, I needed to make sure that it actually constructs. We do that by trying it out. Because I have provided this \u201ctest\u201d, you can re-run that debugging process as many times as you wan to. You don\u2019t need to read the documentation on what the SummaryParser() is supposed to do \u2013 just set a breakpoint and run the test \u2013 the execution will stop at the breakpoint and you\u2019ll be in an interactive coding environment. By doing this, as a developer, you can interatively improve the tests over time. As you debug, upon first writing or at any point in the future when you are debugging, just keep a record (that is the iteratively improved test). For example, maybe you write a more robust test of the constructor: As you write the rest of the class, you\u2019ll want to make sure that you\u2019re including tests. At the very very least, provide the names of the tests: If you\u2019re good, then you\u2019ll write this tests firsts, and then write the function. But most of us are mere mortals, and will write the functional code and the tests concurrently. You do this already, most likely \u2013 write and test in an interactive environment, and then copy/paste the working function into your production environment \u2013 the only difference between that, and using the testing framework, is that by using the testing framework, you\u2019re providing a resource to yourself-in-the-future, and possibly other developers.","title":"Contributing"},{"location":"home/contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.","title":"Contributing"},{"location":"home/contributing/#fork-the-repo","text":"Fork the repo into your personal profile. This will allow you to make changes and push them to your forked repo. When you\u2019re ready, issue a pull request to the main repo.","title":"Fork the repo"},{"location":"home/contributing/#issues","text":"Ideally, create an issue first. This will allow us to discuss the feature or bug fix you want to implement.","title":"Issues"},{"location":"home/contributing/#pull-requests","text":"Issue pull requests against the dev branch. Please make sure that your code is covered by a test (see below).","title":"Pull Requests"},{"location":"home/contributing/#environment-setup","text":"install poetry I prefer to set the default location of the virtual environment to the project directory. You can set that as a global configuration for your poetry installation like so: poetry config virtualenvs.in-project true git clone the repo cd into the repo and issue the command poetry install shell into the virtual environment with poetry shell you can pip install -e . to install the package in editable mode. This is useful if you want to test the cmd line interface as you make changes to the source code.","title":"Environment setup"},{"location":"home/contributing/#tests-are-reproducible-debugging-tools","text":"There is a test suite provided with this package which is intended to give any future Calling Cards developers who might use this an easier way into the code. All code should be accessible from the current tests, and the current tests can be used to give some assurance that a new change doesn\u2019t absolutely break any functionality. However , the purpose of the tests is not to prove correctness. Rather, it is a record of the debugging that I have done. It makes the debugging reproducible! The real benefit to me in the future, or you, if you have inherited this, is that you can use VScode to set breakpoints in the code. This means you don\u2019t have to read my documentation or guess at what I was trying to do. Here is an example: What you\u2019re seeing here is a a dummy test on the right. Notice that this test does nothing! it just says that 2 == 2. However, it is importing a fixture . A fixture is a test data object \u2013 in this case, it is a SummaryParser object that is loaded from test data that is provided in the package repository. Notice that I have set a breakpoint on the SummaryParser() constructor. This isn\u2019t necessarily the most direct way or setting up this test, but it provides an example of both a fixture, and proves that tests don\u2019t need to actually test anything at all to be useful. Why is this useful? Because now you can \u201cstep into\u201d the SummaryParser constructor. When I wrote the constructor, I needed to make sure that it actually constructs. We do that by trying it out. Because I have provided this \u201ctest\u201d, you can re-run that debugging process as many times as you wan to. You don\u2019t need to read the documentation on what the SummaryParser() is supposed to do \u2013 just set a breakpoint and run the test \u2013 the execution will stop at the breakpoint and you\u2019ll be in an interactive coding environment. By doing this, as a developer, you can interatively improve the tests over time. As you debug, upon first writing or at any point in the future when you are debugging, just keep a record (that is the iteratively improved test). For example, maybe you write a more robust test of the constructor: As you write the rest of the class, you\u2019ll want to make sure that you\u2019re including tests. At the very very least, provide the names of the tests: If you\u2019re good, then you\u2019ll write this tests firsts, and then write the function. But most of us are mere mortals, and will write the functional code and the tests concurrently. You do this already, most likely \u2013 write and test in an interactive environment, and then copy/paste the working function into your production environment \u2013 the only difference between that, and using the testing framework, is that by using the testing framework, you\u2019re providing a resource to yourself-in-the-future, and possibly other developers.","title":"Tests are reproducible debugging tools!"},{"location":"home/license/","text":"FreeBSD License \u00b6 Copyright (c) 2022, Chase Mateusiak Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \u201cAS IS\u201d AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"License"},{"location":"home/license/#freebsd-license","text":"Copyright (c) 2022, Chase Mateusiak Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \u201cAS IS\u201d AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"FreeBSD License"}]}