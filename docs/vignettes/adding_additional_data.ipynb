{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking table column names...\n",
      "Current database tables are valid\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "# the peak callers all inherit from DatabaseApi, a class which offers an interface \n",
    "# to a sqlite database to store Calling Cards data\n",
    "from callingcardstools.database_managers.yeast import HopsDb\n",
    "from callingcardstools.PackageResources import Resources\n",
    "import pandas as pd\n",
    "\n",
    "# This object allows retrieval of package resources\n",
    "cc_resources = Resources()\n",
    "\n",
    "# create a database either in memory or at a specified location\n",
    "#yeast_db = hopsdb(\"/home/oguzkhan/Desktop/cc_metadata/hops_db.sqlite\")\n",
    "yeast_db = HopsDb(\"/home/oguzkhan/projects/rank_response_shiny/data/qc_db_v2.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from callingcardstools.BarcodeParser import BarcodeParser\n",
    "# d = {\n",
    "# \t'run_5301_5088': ['/mnt/scratch/calling_cards/sequence/run_5301_5088/run_5301_5088_barcode_details.json','/mnt/scratch/calling_cards/sequence/run_5301_5088/cctools_split/id_bc_map.tsv'],\n",
    "# \t'run_5690': ['/mnt/scratch/calling_cards/sequence/run_5690/run_5690_barcode_details.json', '/mnt/scratch/calling_cards/sequence/run_5690/cctools_split/id_bc_map.tsv'],\n",
    "# \t'run_6100': ['/mnt/scratch/calling_cards/sequence/run_6100/run_6100_barcode_details.json','/mnt/scratch/calling_cards/sequence/run_6100/cctools_split/id_bc_map.tsv'],\n",
    "# \t'run_6106': ['/mnt/scratch/calling_cards/sequence/run_6106/run_6106_barcode_details.json','/mnt/scratch/calling_cards/sequence/run_6106/cctools_split/id_bc_map.tsv'],\n",
    "# }\n",
    "d = {\n",
    "\t'run_6390': ['/mnt/scratch/cc/mitra_pipeline/run_6390/run_6390_barcode_details.json',\n",
    "\t      '/mnt/scratch/cc/mitra_pipeline/run_6390/cctools_split/id_bc_map.tsv'],\n",
    "}\n",
    "\n",
    "for k,v in d.items():\n",
    "\tbp = BarcodeParser(v[0])\n",
    "\tyeast_db.add_batch_qc(bp,v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccf_basepath='/mnt/scratch/cc/mitra_pipeline/run_6390/ccf/'\n",
    "\n",
    "tf_list = ['SKO1', 'MSN2', 'SKN7', \n",
    "            'USV1', 'DAL80', 'GZF3',\n",
    "            'INO2', 'MSN2']\n",
    "\n",
    "replicate_list = ['none', '1', 'none', \n",
    "                   'none', 'none', 'none',\n",
    "                   'none', '2']\n",
    "\n",
    "ccf_list = [os.path.join(ccf_basepath,x+'_with_annote.ccf') for x in tf_list]\n",
    "\n",
    "ccf_list[1] = ccf_list[1].replace('MSN2','MSN2_1')\n",
    "\n",
    "ccf_list[7] = ccf_list[5].replace('MSN2','MSN2_2')\n",
    "\n",
    "ccf_df = pd.DataFrame.from_dict(\n",
    "\t{'batch': ['run_6390'] * 8,\n",
    "     'tf': tf_list,\n",
    "     'replicate': replicate_list,\n",
    "\t 'ccf':ccf_list}\n",
    ")\n",
    "\n",
    "#ccf_df = pd.read_csv(\"/home/oguzkhan/projects/rank_response_shiny/data/run_6177_ccf_lookup.csv\")\n",
    "ccf_df['batch'] = ccf_df['batch'].str.lower()\n",
    "batch_tbl = pd.read_sql_query('Select * from batch', yeast_db.con)\n",
    "ccf_df_with_batch_id = pd.merge(ccf_df,batch_tbl,how='left', on=['batch','tf', 'replicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_ccf_to_db(row: pd.Series) -> None:\n",
    "\t\"\"\"read in the ccf, augment and add to the yeast_db\n",
    "\n",
    "\tArgs:\n",
    "\t\trow (pd.Series): a row from the ccf_df\n",
    "\t\"\"\"\n",
    "\t# note that only the first 6 rows are used for the names. sample, the 6th, \n",
    "\t# is added in this function\n",
    "\tdf = pd.read_csv(row['ccf'], \n",
    "\t                 sep = '\\t', \n",
    "\t\t\t\t\t names = ['chr','start','end','depth','strand','annotation'])\n",
    "\tdf['batch_id'] = row['id']\n",
    "\tprint(df.head(2))\n",
    "\tyeast_db.add_frame(df,'qbed',table_type='experiment',tablename_suffix = row['tf'], fk_tablelist=['batch'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ccf_df.apply(lambda row: extract_descriptors(row), axis=1, result_type='expand')\n",
    "ccf_df_with_batch_id.apply(lambda row: add_ccf_to_db(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#background_and_expr_tbls = [x for x in yeast_db.list_tables(yeast_db.con) if re.search(r\"^background|^experiment\",x)]\n",
    "experiment_list = ['experiment_'+x for x in tf_list]\n",
    "# in this case -- remove the last item b/c it is a second replicate of a given TF\n",
    "# in the run\n",
    "experiment_list = experiment_list[0:7]\n",
    "\n",
    "# NOTE! This doesn't need to be re-done if the view already exists, meaning \n",
    "# the experiment table already exists. It doesn't hurt to re-run\n",
    "for regions_tbl in ['regions_yiming', 'regions_not_orf']:\n",
    "    for qbed in experiment_list:\n",
    "        yeast_db.create_aggregate_view(qbed,regions_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region: regions_yiming\n",
      "background: background_adh1\n",
      "experiment: experiment_GZF3\n",
      "<bound method HopsDb.peak_caller of <callingcardstools.database_managers.yeast.HopsDb.HopsDb object at 0x7f3d001fd2b0>>\n",
      "    def peak_caller(self,replicate_handling:Literal['separate','sum']='separate',poisson_pseudocount:float = 0.2, if_exists:str = 'fail', *args, **kwargs) -> None:\n",
      "        \"\"\"Call Peaks and add the result to the database.\n",
      "\n",
      "        Args:\n",
      "            replicate_handling (str, ['separate','sum'], optional): How to handle replicates. 'sum' \n",
      "            will add hops by region for all replicates. 'separate' will call peaks for each \n",
      "            replicate separately. Defaults to separate.\n",
      "            *args (list): additional positional arguments -- currently unused\n",
      "            poisson_pseudocount (float, optional): peudocount to add to poisson pvalue calculation. Defaults to 0.2.\n",
      "            **kwargs (dict): Use the following combination of keyword arguments \n",
      "            to direct the peak_caller method:\n",
      "                regions='regions_tbl',background='background_tbl',experiment='experiment_tbl' -- Call peaks with background. \n",
      "\n",
      "        Raises:\n",
      "            AttributeError: _description_\n",
      "        \"\"\"\n",
      "        # note that the idea of using the kwargs argument is to simulate \n",
      "        # overloading, so that a different set of kwargs could be used to call \n",
      "        # a different peak caller. If another peak caller is added at some point, \n",
      "        # this will need to turn into an if, elif, ... statement with this \n",
      "        # error handling ahppening in the final 'else' \n",
      "        if {'regions','background','experiment'} != set(kwargs):\n",
      "            raise KeyError(f'The combination of tables {kwargs} '\\\n",
      "                'does not match any expected combinations -- cannot find an '\n",
      "                'appropriate peak calling method')\n",
      "\n",
      "        # TODO handle undetermined case better! \n",
      "        if kwargs.get('experiment') != 'undetermined':\n",
      "            \n",
      "            # create the tablename \n",
      "            join_sql = self._regions_background_expr_sql(\n",
      "                kwargs.get('regions'), \n",
      "                kwargs.get('background'), \n",
      "                kwargs.get('experiment'))\n",
      "\n",
      "            quant_df = pd.read_sql_query(join_sql, self.con)\n",
      "\n",
      "            total_hops_dict = \\\n",
      "                {'background': self.get_total_hops(kwargs.get('background'))}\n",
      "            \n",
      "            # group by batch_id (replicates)\n",
      "            batch_grouped_df = quant_df.groupby('batch_id')\n",
      "            # add total hops for each replicate to the total_hop_dict\n",
      "            for group,df in batch_grouped_df:\n",
      "                total_hops_dict[group] = \\\n",
      "                    self.get_total_hops(kwargs.get('experiment'),group)\n",
      "            \n",
      "            if replicate_handling == 'separate': \n",
      "                # call peaks\n",
      "                output_df = \\\n",
      "                    call_peaks_with_background(\n",
      "                        batch_grouped_df, \n",
      "                        total_hops_dict,poisson_pseudocount)\n",
      "                # send the table to the database\n",
      "\n",
      "            elif replicate_handling == 'sum':\n",
      "                passing_batch_ids = \\\n",
      "                    self._get_passing_batch_ids(\n",
      "                        quant_df.loc[0,'batch_id'],\n",
      "                        kwargs.get('include_unreviewed', False))\n",
      "                summed_passing_replicate_df = \\\n",
      "                    quant_df[quant_df.batch_id in passing_batch_ids]\n",
      "                summed_passing_replicate_df['group'] = \\\n",
      "                    ['all'] * len(summed_passing_replicate_df)\n",
      "                summed_passing_replicate_df = \\\n",
      "                    summed_passing_replicate_df\\\n",
      "                        .groupby('group')\n",
      "                        # SUM OVER POSITIONS!\n",
      "                \n",
      "                # extract the max hops for a given passing replicate\n",
      "                remove_keys = [k for k in total_hops_dict if k in passing_batch_ids]\n",
      "                max_expr_hops = max([total_hops_dict.pop(k) for k in remove_keys])\n",
      "                total_hops_dict['all'] = max_expr_hops\n",
      "\n",
      "                # call peaks\n",
      "                output_df = call_peaks_with_background(\n",
      "                    summed_passing_replicate_df, \n",
      "                    total_hops_dict, \n",
      "                    poisson_pseudocount)\n",
      "            else:\n",
      "                raise IOError(f'replicate handling method '\\\n",
      "                    f'{replicate_handling} not recognized.')\n",
      "            \n",
      "            sig_tablename = kwargs.get('regions') + '_' + kwargs.get('background')+ \\\n",
      "                        \"_\" + kwargs.get('experiment') + '_' + replicate_handling + \"_sig\"\n",
      "\n",
      "            output_df.to_sql(sig_tablename,\n",
      "                con=self.con,\n",
      "                if_exists=if_exists,\n",
      "                index=False)\n",
      "\n",
      "            \n",
      "            # index the table note that the index is create only if one with the \n",
      "            # same name doesn't already exist\n",
      "            index_col_string = self.index_col_string_dict['qbed']+',\"batch_id\"'\n",
      "            self.index_table(sig_tablename, index_col_string)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# experiment list is being created above now -- see previous block\n",
    "\n",
    "# experiment_list = [x for x in yeast_db.list_tables(yeast_db.con) if re.match('^experiment_', x)]\n",
    "\n",
    "# experiment_list = ['experiment_RDS2', \n",
    "#                    'experiment_MET31',\n",
    "#                    'experiment_INO2',\n",
    "#                    'experiment_CAD1',\n",
    "#                    'experiment_SIP4',\n",
    "#                    'experiment_GZF3']\n",
    "\n",
    "#regions_list = ['regions_yiming', 'regions_not_orf']\n",
    "regions_list = ['regions_yiming']\n",
    "#background_list = ['background_adh1', 'background_dSir4']\n",
    "background_list = ['background_adh1']\n",
    "experiment_list = ['experiment_GZF3']\n",
    "\n",
    "for region_tbl in regions_list:\n",
    "\tprint(f\"region: {region_tbl}\")\n",
    "\tfor background_tbl in background_list:\n",
    "\t\tprint(f\"background: {background_tbl}\")\n",
    "\t\tfor experiment_tbl in experiment_list:\n",
    "\t\t\tprint(f\"experiment: {experiment_tbl}\")\n",
    "\t\t\tyeast_db.peak_caller(regions = region_tbl,\n",
    "\t\t\t                     background = background_tbl, \n",
    "\t\t\t\t\t\t\t\t experiment = experiment_tbl,\n",
    "\t\t\t\t\t\t\t\t if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70180e53ac4002ada796cb5a849661852af895640b68d8d95e5f4d9de4173714"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
